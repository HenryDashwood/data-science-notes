[
  {
    "objectID": "Plotting/matplotlib.html",
    "href": "Plotting/matplotlib.html",
    "title": "Matplotlib",
    "section": "",
    "text": "from pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nPROJECT_ROOT = Path().cwd().parent.parent\n\n\n# Load some sample data\nmedals = pd.read_csv(f\"{PROJECT_ROOT}/data/medals_by_country_2016.csv\", index_col=[0])\nclimate_change = pd.read_csv(f\"{PROJECT_ROOT}/data/climate_change.csv\", parse_dates=[\"date\"])\n\n\nmedals\n\n\n\n\n\n\n\n\nBronze\nGold\nSilver\n\n\n\n\nUnited States\n67\n137\n52\n\n\nGermany\n67\n47\n43\n\n\nGreat Britain\n26\n64\n55\n\n\nRussia\n35\n50\n28\n\n\nChina\n35\n44\n30\n\n\nFrance\n21\n20\n55\n\n\nAustralia\n25\n23\n34\n\n\nItaly\n24\n8\n38\n\n\nCanada\n61\n4\n4\n\n\nJapan\n34\n17\n13\n\n\n\n\n\n\n\n\nclimate_change\n\n\n\n\n\n\n\n\ndate\nco2\nrelative_temp\n\n\n\n\n0\n1958-03-06\n315.71\n0.10\n\n\n1\n1958-04-06\n317.45\n0.01\n\n\n2\n1958-05-06\n317.50\n0.08\n\n\n3\n1958-06-06\nNaN\n-0.05\n\n\n4\n1958-07-06\n315.86\n0.06\n\n\n...\n...\n...\n...\n\n\n701\n2016-08-06\n402.27\n0.98\n\n\n702\n2016-09-06\n401.05\n0.87\n\n\n703\n2016-10-06\n401.59\n0.89\n\n\n704\n2016-11-06\n403.55\n0.93\n\n\n705\n2016-12-06\n404.45\n0.81\n\n\n\n\n706 rows × 3 columns\n\n\n\n\nfig, ax = plt.subplots()\n\n\n\n\nWe created two variables, fig and ax. These are arbitrary names but a convention.\nThese two variables hold the core objects used for all types of plotting operations. First object fig, short for figure, imagine it as the frame of your plot. You can resize, reshape the frame but you cannot draw on it. On a single notebook or a script, you can have multiple figures. Each figure can have multiple subplots. Here, subplot is synonymous with axes. The second object, ax, short for axes, is the canvas you draw on. Or rephrasing, it is the blank sheet you can plot and hold your data. An axes object can only belong to one figure.\n\nAxes methods vs. pyplot, understanding further\nPyplot is a more beginner-friendly method to interact with Matplotlib. Compared to axes methods, pyplot offers a quicker and more concise method of plotting. It will have fewer local variables and syntax. But most people prefer the object-oriented way?\nLet’s see the concise use of pyplot:\n\nplt.bar(medals.index, medals[\"Gold\"])\nplt.xticks(rotation=90)\nplt.ylabel(\"# of gold medals\")\n\nText(0, 0.5, '# of gold medals')\n\n\n\n\n\nThis was easy. It only took 3 lines. Now let’s what happens if we try to plot the (completely unrelated) climate change data next to it:\n\n# plt.bar(medals.index, medals[\"Gold\"])\n# plt.xticks(rotation=90)\n# plt.ylabel(\"# of gold medals\")\n# plt.plot(climate_change[\"date\"], climate_change[\"co2\"])\n\nIn this case, we get a TypeError. pyplot, on its own, cannot create new axes or a new figure and intelligently plot the new data. As we get to more complex plotting like this one, we are going to need more a flexible approach.\nMatplotlib has this concept called current figure. By default, pyplot itself creates a current figure axes and plots on it. If for example, we want to focus on that current figure and plot extra data on it, like we tried in the last example, pyplot moves the current figure to a new one immediately after a new plotting command is given.\nTo avoid this, let’s see the approach where we are in full control of each figure and axes:\n\nfig, ax = plt.subplots()\nax.plot(climate_change[\"date\"], climate_change[\"co2\"])\nax.set(title=\"Amount of CO2 (ppm) in each year\", xlabel=\"Year\", ylabel=\"Amount of CO2 (ppm)\")\n\n[Text(0.5, 1.0, 'Amount of CO2 (ppm) in each year'),\n Text(0.5, 0, 'Year'),\n Text(0, 0.5, 'Amount of CO2 (ppm)')]\n\n\n\n\n\nWe specifically point out that we are working on this fig object. It means that any plotting command we write will be applied to the axes (ax) object that belongs to fig. Unless, we define a new figure with plt.subplots() command, the current figure will be the variable fig. This way is very nice since now we can create as many axes or subplots in a single figure and work with them.\nFrom now on, I will be using subplot and axes terms interchangeably as they are synonyms\n\n\nplt.subplots() grid system\nWe saw an example of creating one subplot. Let’s see how can create more in a single figure:\n\nfig, ax = plt.subplots(nrows=1, ncols=2)\nprint(ax)\nprint(type(ax))\n\n[&lt;AxesSubplot: &gt; &lt;AxesSubplot: &gt;]\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\nAmong other parameters, .subplots() have two parameters to specify the grid size. nrows and ncols are used to point out the number of rows and columns we need respectively. If you paid attention, now our second variable contains not one but two axes. And it is now given as a numpy.ndarray. So, we have to unpack or index this array to use our plotting commands:\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nfig.tight_layout(pad=3)\nax1.plot(climate_change[\"date\"], climate_change[\"co2\"])\nax2.plot(climate_change[\"date\"], climate_change[\"relative_temp\"])\n\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nfig.tight_layout(pad=3)\nax[0].plot(climate_change[\"date\"], climate_change[\"co2\"])\nax[1].plot(climate_change[\"date\"], climate_change[\"relative_temp\"])\n\nplt.show()\n\n\n\n\nThe two methods are completely similar and up to you to choose one. Let’s see one more example but slightly more difficult:\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 5))\n\n\n\n\n\nfig, ax = plt.subplots()\nax.bar(medals.index, medals[\"Gold\"])\nax.set(title=\"# gold medals by country\", ylabel=\"# of medals\", xlabel=\"Country\")\nax.set_xticklabels(medals.index, rotation=90)\nplt.show()\n\n/var/folders/qz/3pl78vgn6t7cltx35ttht03r0000gn/T/ipykernel_48226/3128316306.py:4: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(medals.index, rotation=90)\n\n\n\n\n\nAll the methods that are available in pyplot API has an equivalent through ax.set_. If you use a general, ax.set(), you will avoid repetition when you have multiple subplots. However, if you need to specify additional parameters to specific parts of your plot, use ax.set_:\n\nfig, ax = plt.subplots()\nax.bar(medals.index, medals[\"Gold\"])\nax.set_xticklabels(medals.index, rotation=90)\nax.set_xlabel(\"Country\", fontsize=15)\nax.set_title(\"# gold medals by country\", fontsize=20)\nplt.show()\n\n/var/folders/qz/3pl78vgn6t7cltx35ttht03r0000gn/T/ipykernel_48226/3042962666.py:3: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(medals.index, rotation=90)\n\n\n\n\n\n\n\nDoubling axis\nSometimes, we want to have a single subplot to have more than one x axis or y axis. While it is not possible with plain pyplot interface, it is very easy with top-level figure object-oriented API. Let’s say we want to plot the relative_temp and co2 columns of climate_change in a single plot. We want them to share an x axis since the data is for the same time period:\n\n# Create a figure and an axis\nfig, ax = plt.subplots()\n# Plot CO2 emissions with a blue line\nax.plot(climate_change[\"date\"], climate_change[\"co2\"], color=\"blue\")\n\n# Specify that we will be using a twin x axis\nax2 = ax.twinx()\n\nax2.plot(climate_change[\"date\"], climate_change[\"relative_temp\"], color=\"red\")\n\n# Change the color of ticks\nax.tick_params(\"y\", colors=\"blue\")  # 'y' because we want to change the y axis\nax2.tick_params(\"y\", colors=\"red\")\n\nax.set(\n    title=\"CO2 emmissions and relative temperature by year\", xlabel=\"Year\"\n)  # Does not matter which one you pick, ax or ax2\nplt.show()\n\n\n\n\nWe wanted to have a common x axis, which was date column, so we created another axis using ax.twinx(). If in some cases you want a common y axis, the function is ax.twiny().\n\n\nSharing a commong axis between subplots\nLet’s say we wanted to compare the CO2 emissions of the eighties with nineties. Ideally, we would want to plot the eighties on side and nineties to the other. So, let’s subset our data for these two time periods:\n\n# Set the date column as index for easy subsetting\nclimate_change = climate_change.set_index(\"date\")\n\n# Subset data for two time periods\neighties = climate_change[\"1980-01-01\":\"1989-12-31\"]\nnineties = climate_change[\"1990-01-01\":\"1999-12-31\"]\n\nPro Tip: Set the date column as an index for a dataframe if you are working with time-series data. Use .set_index() method or use index_col parameter in pd.read_csv() function. It will make subsetting for time periods much easier.\n\n# Create axes and  a figure\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nfig.tight_layout()\n# Plot eighties\nax[0].plot(eighties.index, eighties[\"co2\"])\n# Plot nineties\nax[1].plot(nineties.index, nineties[\"co2\"])\n\nplt.show()\n\n\n\n\nGreat, we have the two plots side by side, but if we look closer, our plots are misleading. It looks like there was not much difference in CO2 emmissions throughout two time periods. The reason for this is that the two plots have different YAxis ranges. Let’s set it right for better insight:\n\n# Create axes and  a figure\nfig, ax = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\nfig.tight_layout()\n# Plot eighties\nax[0].plot(eighties.index, eighties[\"co2\"])\n# Plot nineties\nax[1].plot(nineties.index, nineties[\"co2\"])\n\nplt.show()\n\n\n\n\nNow, it is clear that CO2 emissions continued increasing through time (it is much higher that this right now). We use sharey=True to specify that we want the same YAxis for all the subplots.\n\n\nWorking with figure object\nI think you noticed that once you create a figure object using .subplots() command or other methods, pretty much everything happens with axes objects. One common method of figure object is savefig() method, which is used often. So, let’s get exploring. We will get back to our double-axed plot of CO2. Let’s save it to local memory:\n\n# Create a figure and an axis\nfig, ax = plt.subplots()\n# Plot CO2 emissions with a blue line\nax.plot(climate_change.index, climate_change[\"co2\"], color=\"blue\")\n\n# Specify that we will be using a twin x axis\nax2 = ax.twinx()\n\nax2.plot(climate_change.index, climate_change[\"relative_temp\"], color=\"red\")\n\n# Change the color of ticks\nax.tick_params(\"y\", colors=\"blue\")  # 'y' because we want to change the y axis\nax2.tick_params(\"y\", colors=\"red\")\n\nax.set(\n    title=\"CO2 emmissions and relative temperature by year\", xlabel=\"Year\"\n)  # Does not matter which one you pick, ax or ax2\n\n# fig.savefig(\"co2_relative_temp.png\")\n\n[Text(0.5, 1.0, 'CO2 emmissions and relative temperature by year'),\n Text(0.5, 0, 'Year')]"
  },
  {
    "objectID": "00_Statistics/linear_regression.html",
    "href": "00_Statistics/linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nPROJECT_ROOT = f\"{Path.cwd().parent}\"\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0"
  },
  {
    "objectID": "00_Statistics/linear_regression.html#linear-regression-with-a-single-independent-variable",
    "href": "00_Statistics/linear_regression.html#linear-regression-with-a-single-independent-variable",
    "title": "Linear Regression",
    "section": "Linear regression with a single independent variable",
    "text": "Linear regression with a single independent variable\nCorrelation tells us the strength of a linear relationship between two variables. But what if we want to predict the value of one variable given the value of another? For example, suppose we want to predict the price of a house given its size. We can do this using linear regression.\nWe begin by hypothesising the existence a linear model:\n\\[\ny_i = \\beta x_i + \\alpha\n\\]\nwhere \\(y_i\\) is the price of the house \\(i\\), \\(x_i\\) is the size of the house \\(i\\), \\(\\beta\\) is the slope of the line, \\(\\alpha\\) is the intercept. Which parameters would result in the best fit line? We can use the least squares method to find the best fit line. The least squares method minimises the sum of the squared errors (or residuals). This is also known as our cost function, \\(S\\):\n\\[\nS = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\n\\]\nwhere \\(\\hat{y_i}\\) is the predicted value of \\(y_i\\).\nWe can subsitute our model for the predicted value of \\(y_i\\):\n\\[\n\\begin{align}\nS &= \\sum_{i=1}^n (y_i - (\\beta x_i + \\alpha))^2 \\\\\n&= \\sum_{i=1}^n (y_i - \\beta x_i - \\alpha)^2\n\\end{align}\n\\]\nTo minimize our cost function, S, we must find where the first derivative of \\(S\\) is equal to 0 with respect to \\(\\alpha\\) and \\(\\beta\\). The closer \\(\\alpha\\) and \\(\\beta\\) are to 0, the less the total error for each point is. Let’s start with the partial derivative of \\(\\alpha\\) first.\n\nFinding \\(\\alpha\\)\n\\[\n\\frac{\\partial S}{\\partial \\alpha}[\\sum_{i=1}^n (y_i - \\beta x_i - \\alpha)^2]\n\\]\nWe can use the chain rule to find the partial derivative with respect to \\(\\alpha\\).\nThe outer term goes from \\(u^2\\) to 2u (where \\(u =y_i - \\alpha - \\beta x_i\\)).\nWithin the perenthesese we treat the non-\\(\\alpha\\) terms as constants so we go from \\(y_i - \\alpha - \\beta x_i\\) to \\(-1\\).\nSo in the end we have:\n\\[\n0 = \\sum_{i=1}^n -2(y_i - \\alpha - \\beta x_i)\n\\]\nWe can divide both sides by \\(-2\\) to get:\n\\[\n0 = \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)\n\\]\nWe can then break this summation in 3 parts and pull the constant \\(\\beta\\) out:\n\\[\n0 = \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\alpha - \\beta \\sum_{i=1}^n x_i\n\\]\nSo the summamation of \\(\\alpha\\) to \\(n\\) is\n\\[\n\\sum_{i=1}^n \\alpha = n\\alpha\n\\]\nWe can subsitute this back in to get\n\\[\n0 = \\sum_{i=1}^n y_i - n\\alpha - \\beta \\sum_{i=1}^n x_i\n\\]\nWe are trying to solve for \\(\\alpha\\) so we add \\(n\\alpha\\) to both sides and divide by \\(n\\).\n\\[\n\\alpha = \\frac{\\sum_{i=1}^n y_i - \\beta \\sum_{i=1}^n x_i}{n}\n\\]\nIn the above equation we are calculating the sum of \\(y\\) and $x& and then dividing by the number of of points. In other words we are using the mean of \\(y\\) and \\(x\\). So we can rewrite the equation as:\n\\[\n\\alpha = \\bar{y} - \\beta \\bar{x} \\tag{1}\n\\]\nwhere \\(\\bar{y}\\) is the mean of \\(y\\) and \\(\\bar{x}\\) is the mean of \\(x\\).\n\n\nFinding \\(\\beta\\)\nHaving minimised the cost function of \\(S\\) with respect to \\(\\alpha\\). Let’s find the last part which is \\(S\\) with respect to \\(\\beta\\).\n\\[\n\\frac{\\partial S}{\\partial \\beta}[\\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2]\n\\]\nWe can use the chain rule to find the partial derivative with respect to \\(\\beta\\).\nThe outer term goes from \\(u^2\\) to 2u (where \\(u =y_i - \\alpha - \\beta x_i\\)).\nWithin the perenthesese we treat the non-\\(\\beta\\) terms as constants so we go from \\(y_i - \\alpha - \\beta x_i\\) to \\(-x_i\\).\nSo in the end we have:\n\\[\n0 = \\sum_{i=1}^n -2x_i(y_i - \\alpha - \\beta x_i)\n\\]\nAgain we can divide both sides by \\(-2\\) to get:\n\\[\n0 = \\sum_{i=1}^n x_i(y_i - \\alpha - \\beta x_i)\n\\]\nMultiplying each term by \\(x_i\\) we get:\n\\[\n0 = \\sum_{i=1}^n (y_ix_i - \\alpha x_i - \\beta x_i^2)\n\\]\nLet’s substitute \\(a\\) (formula \\((1)\\)) into the partial derivative of \\(S\\) with respect to \\(\\beta\\) so we have a function of \\(\\alpha\\) and \\(\\beta\\) in terms of only \\(x\\) and \\(y\\).\n\\[\n0 = \\sum_{i=1}^n(x_iy_i - (\\bar{y} - \\beta\\bar{x})x_i - \\beta x_i^2)\n\\]\nMultiplying out the brackets we get:\n\\[\n0 = \\sum_{i=1}^n(x_iy_i - \\bar{y}x_i + \\beta\\bar{x}x_i - \\beta x_i^2)\n\\]\nWe can split this into 2 sums\n\\[\n0 = \\sum_{i=1}^n(x_iy_i - \\bar{y}x_i) + \\sum_{i=1}^n(\\beta\\bar{x}x_i - \\beta x_i^2))\n\\]\nand factor out \\(-\\beta\\) (note the minus!)\n\\[\n0 = \\sum_{i=1}^n(x_iy_i - \\bar{y}x_i) - \\beta \\sum_{i=1}^n(x_i^2 - \\bar{x}x_i))\n\\]\nAdding \\(\\beta \\sum_{i=1}^n(x_i^2 - \\bar{x}x_i))\\) to both sides and then dividing both sides by the same gives us:\n\\[\n\\beta = \\frac{\\sum_{i=1}^n(x_iy_i - \\bar{y}x_i)}{\\sum_{i=1}^n(x_i^2 - \\bar{x}x_i))} \\tag{2}\n\\]\n\n\nConclusion\nSo the tldr is that if you have a dataset with 1 independent variable, you find the line of best fit by:\n\nCalculating \\(\\beta\\) using equation \\((2)\\)\nSubstituting \\(\\beta\\) into equation \\((1)\\) to find \\(\\alpha\\)\nSubstituting \\(\\beta\\) and \\(\\alpha\\) into the equation for the line of best fit:\n\n\\[\n\\hat{y} = \\beta x_i + \\alpha\n\\]"
  },
  {
    "objectID": "00_Statistics/linear_regression.html#examples",
    "href": "00_Statistics/linear_regression.html#examples",
    "title": "Linear Regression",
    "section": "Examples",
    "text": "Examples\n\nWith NumPy and random data\n\ndef get_beta(x: np.ndarray, y: np.ndarray) -&gt; float:\n    numerator = np.sum((x * y) - (y.mean() * x))\n    denominator = np.sum((x**2) - (x.mean() * x))\n    return numerator / denominator\n\n\ndef get_alpha(x: np.ndarray, y: np.ndarray) -&gt; float:\n    return np.mean(y) - get_beta(x, y) * np.mean(x)\n\n\ndef predict(x_pred: np.ndarray, x_known: np.ndarray, y_known: np.ndarray) -&gt; np.ndarray:\n    y_pred = get_alpha(x_known, y_known) + get_beta(x_known, y_known) * x_pred\n    return y_pred\n\n\nxs = np.random.rand(10) * 10\nys = np.random.rand(10) * 10\nxs = np.sort(xs)\nys = np.sort(ys)\n\n\nbeta = get_beta(xs, ys)\nalpha = get_alpha(xs, ys)\nprint(alpha, beta)\n\n1.177260540374534 0.9373577460801178\n\n\n\ndef plot_results(x: np.ndarray, y: np.ndarray, alpha: float, beta: float) -&gt; None:\n    fig, ax = plt.subplots(figsize=(6, 5))\n\n    x_low, x_high = np.floor(x.min()), np.ceil(x.max())\n    y_low, y_high = np.floor(y.min()), np.ceil(y.max())\n\n    ax.scatter(x, y)\n\n    x_vals = np.linspace(x_low, x_high)\n    y_vals = alpha + beta * x_vals\n    ax.plot(x_vals, y_vals, \"-\")\n\n    ax.set_xlim([0, x_high + 1])\n    ax.set_ylim([0, y_high + 1])\n\n    ax.set_xlabel(\"x values\")\n    ax.set_ylabel(\"y values\")\n    ax.set_title(f\"Linear Regression Example\\nbeta:{beta:.2f} alpha:{alpha:.2f}\")\n\n    ax.set_xmargin(0)\n    ax.set_ymargin(0)\n\n    ax.spines[\"right\"].set_color(\"none\")\n    ax.spines[\"top\"].set_color(\"none\")\n\n    ax.grid(True)\n\n    plt.show()\n\n\nplot_results(xs, ys, alpha, beta)\n\n\n\n\n\n\nWith Sklearn and random data\n\nmodel = LinearRegression()\nmodel.fit(xs.reshape(-1, 1), ys)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nbeta = model.coef_.item()\nalpha = model.intercept_\nprint(alpha, beta)\n\n1.1772605403745358 0.9373577460801175\n\n\n\nplot_results(xs, ys, alpha, beta)\n\n\n\n\n\n\nUsing the automoblie dataset from the UCI Machine Learning Repository\n\ndf = pd.read_csv(f\"{PROJECT_ROOT}/data/automobiles/imports-85.data\", header=None, usecols=[21, 25], na_values=[\"?\"])\ndf.columns = [\"horsepower\", \"price\"]\ndf = df.dropna()\n\nxs = df[\"horsepower\"].values\nys = df[\"price\"].values\n\n\ndf\n\n\n\n\n\n\n\n\nhorsepower\nprice\n\n\n\n\n0\n111.0\n13495.0\n\n\n1\n111.0\n16500.0\n\n\n2\n154.0\n16500.0\n\n\n3\n102.0\n13950.0\n\n\n4\n115.0\n17450.0\n\n\n...\n...\n...\n\n\n200\n114.0\n16845.0\n\n\n201\n160.0\n19045.0\n\n\n202\n134.0\n21485.0\n\n\n203\n106.0\n22470.0\n\n\n204\n114.0\n22625.0\n\n\n\n\n199 rows × 2 columns\n\n\n\n\nmodel = LinearRegression()\nmodel.fit(xs.reshape(-1, 1), ys)\n\nbeta = model.coef_.item()\nalpha = model.intercept_\n\n\nplot_results(xs, ys, alpha, beta)"
  },
  {
    "objectID": "00_Statistics/linear_regression.html#multiple-regression",
    "href": "00_Statistics/linear_regression.html#multiple-regression",
    "title": "Linear Regression",
    "section": "Multiple regression",
    "text": "Multiple regression\nConsider a model with multiple independent variables. e.g. minutes spent on a social media site for data scientists:\n\\[\n\\text{minutes} = \\beta_1 \\text{friends} + \\beta_2 \\text{work hours} + \\beta_3 \\text{has phd} + \\alpha\n\\]\nEssentially we are expanding the model with one independent variable:\n\\[\ny_i = \\beta x_i + \\alpha\n\\]\nwhere \\(x_i\\) is a single number to a model of the form:\n\\[\ny_i = \\beta_1 x_{i1} + ... + \\beta_k x_{ik} + \\alpha\n\\]\nwhere \\(x_i\\) is a vector of numbers \\(x_{i1}, ..., x_{ik}\\). In multiple regression the vector of parameters is usually called \\(\\beta\\)\nWe are making the assumption that the columns of \\(x\\) are linearly independent, that there is no way to write one as the weighted sum of the others."
  },
  {
    "objectID": "00_Statistics/linear_regression.html#references",
    "href": "00_Statistics/linear_regression.html#references",
    "title": "Linear Regression",
    "section": "References",
    "text": "References\nLinear Regression - Ridley Leisy"
  },
  {
    "objectID": "imbalanced_data.html",
    "href": "imbalanced_data.html",
    "title": "Imbalanced Data",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nfrom imblearn.datasets import make_imbalance\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    roc_auc_score,\n    confusion_matrix,\n    classification_report,\n    ConfusionMatrixDisplay,\n    RocCurveDisplay,\n)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0"
  },
  {
    "objectID": "imbalanced_data.html#example-dataset",
    "href": "imbalanced_data.html#example-dataset",
    "title": "Imbalanced Data",
    "section": "Example Dataset",
    "text": "Example Dataset\nWe are going to use a sample of the adult census dataset. For simplicity we will only use numberic features. We will also artificially make it very imbalanced (only ~1% of rows will be positive). The prediction task is to determine whether a person makes over 50K a year.\n\nxs, y = fetch_openml(data_id=1590, as_frame=True, return_X_y=True, parser=\"auto\")\nxs = xs.select_dtypes(include=\"number\")\ny = y.cat.codes\n\n\nxs, y = make_imbalance(xs, y, sampling_strategy={1: 400}, random_state=1)\n\n\ny.value_counts(normalize=True)\n\n0    0.989349\n1    0.010651\nName: proportion, dtype: float64\n\n\n\nxs\n\n\n\n\n\n\n\n\nage\nfnlwgt\neducation-num\ncapital-gain\ncapital-loss\nhours-per-week\n\n\n\n\n0\n25\n226802\n7\n0\n0\n40\n\n\n1\n38\n89814\n9\n0\n0\n50\n\n\n2\n18\n103497\n10\n0\n0\n30\n\n\n3\n34\n198693\n6\n0\n0\n30\n\n\n4\n29\n227026\n9\n0\n0\n40\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n37550\n40\n153372\n10\n0\n0\n50\n\n\n37551\n40\n173651\n10\n0\n0\n40\n\n\n37552\n39\n192251\n9\n0\n0\n40\n\n\n37553\n34\n240252\n14\n0\n0\n40\n\n\n37554\n34\n196791\n12\n0\n0\n25\n\n\n\n\n37555 rows × 6 columns\n\n\n\n\nxs_train, xs_test, y_train, y_test = train_test_split(xs, y, stratify=y, random_state=0)\n\n\ny_test.value_counts()\n\n0    9289\n1     100\nName: count, dtype: int64"
  },
  {
    "objectID": "imbalanced_data.html#baseline-model",
    "href": "imbalanced_data.html#baseline-model",
    "title": "Imbalanced Data",
    "section": "Baseline Model",
    "text": "Baseline Model\n\nrf_classifier = RandomForestClassifier(random_state=42)\nrf_classifier.fit(xs_train, y_train)\n\ny_test_preds = rf_classifier.predict_proba(xs_test)[:, 1]\n\nprint(classification_report(y_test, (y_test_preds &gt; 0.5).astype(int)))\n\nprint(f\"ROC AUC Score: {roc_auc_score(y_test, y_test_preds)}\")\nprint(f\"Gini Score: {2 * roc_auc_score(y_test, y_test_preds) - 1}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nConfusionMatrixDisplay(confusion_matrix(y_test, (y_test_preds &gt; 0.5).astype(int))).plot(ax=axes[0])\nRocCurveDisplay.from_estimator(rf_classifier, xs_test, y_test, ax=axes[1])\naxes[1].set_xlim((0, 1))\naxes[1].set_ylim((0, 1))\naxes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"red\")\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      1.00      9289\n           1       0.71      0.15      0.25       100\n\n    accuracy                           0.99      9389\n   macro avg       0.85      0.57      0.62      9389\nweighted avg       0.99      0.99      0.99      9389\n\nROC AUC Score: 0.692423296372053\nGini Score: 0.38484659274410604"
  },
  {
    "objectID": "imbalanced_data.html#ways-to-deal-with-imbalanced-data",
    "href": "imbalanced_data.html#ways-to-deal-with-imbalanced-data",
    "title": "Imbalanced Data",
    "section": "Ways to deal with imbalanced data",
    "text": "Ways to deal with imbalanced data\n\nOversampling with SMOTE\nSMOTE (Synthetic Minority Over-sampling Technique) is an algorithm designed to address class imbalance in classification problems by generating synthetic samples for the minority class. The main idea behind SMOTE is to create new samples by interpolating between existing minority class samples rather than simply duplicating them. This approach helps improve the classifier’s ability to learn the decision boundary without leading to overfitting, as it introduces more diversity in the minority class.\nHere’s a step-by-step overview of how SMOTE works:\n\nFor each minority class sample, select k nearest neighbors from the minority class. k is a user-defined parameter.\nChoose one of the k nearest neighbors randomly.\nGenerate a synthetic sample by interpolating between the selected sample and its chosen nearest neighbor:\nsynthetic_sample = original_sample + (random_neighbor - original_sample) * random_weight\nHere, random_weight is a random number between 0 and 1. The interpolation process creates a new sample along the line segment connecting the two points.\nRepeat steps 2 and 3 for a user-defined number of synthetic samples per original minority class sample. This parameter is also known as the oversampling percentage or the number of SMOTE neighbors.\nCombine the original dataset with the generated synthetic samples to create a new, balanced dataset.\n\nIt’s worth noting that SMOTE can be applied in various ways, such as:\n\nRegular SMOTE: The basic SMOTE algorithm as described above.\nBorderline-SMOTE: Focuses on generating synthetic samples near the decision boundary between the majority and minority classes, aiming to improve the classifier’s performance on borderline instances.\nSVM-SMOTE: Uses an SVM classifier to identify support vectors in the minority class and generates synthetic samples based on the support vectors and their nearest neighbors.\nK-Means SMOTE: Applies K-means clustering to the minority class samples and generates synthetic samples based on the cluster centroids and their nearest neighbors.\n\nWhen using SMOTE, it’s important to apply the technique only to the training data and not the validation or test data, as doing so could lead to leakage and an overestimation of the classifier’s performance.\n\nRegular SMOTE\n\nX_train_resampled, y_train_resampled = SMOTE(random_state=42).fit_resample(xs_train, y_train)\ny_train_resampled.value_counts()\n\n0    27866\n1    27866\nName: count, dtype: int64\n\n\n\nrf_classifier = RandomForestClassifier(random_state=42)\nrf_classifier.fit(X_train_resampled, y_train_resampled)\n\ny_test_preds = rf_classifier.predict_proba(xs_test)[:, 1]\n\nprint(classification_report(y_test, (y_test_preds &gt; 0.5).astype(int)))\n\nprint(f\"ROC AUC Score: {roc_auc_score(y_test, y_test_preds)}\")\nprint(f\"Gini Score: {2 * roc_auc_score(y_test, y_test_preds) - 1}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nConfusionMatrixDisplay(confusion_matrix(y_test, (y_test_preds &gt; 0.5).astype(int))).plot(ax=axes[0])\nRocCurveDisplay.from_estimator(rf_classifier, xs_test, y_test, ax=axes[1])\naxes[1].set_xlim((0, 1))\naxes[1].set_ylim((0, 1))\naxes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"red\")\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97      9289\n           1       0.05      0.22      0.08       100\n\n    accuracy                           0.95      9389\n   macro avg       0.52      0.59      0.53      9389\nweighted avg       0.98      0.95      0.96      9389\n\nROC AUC Score: 0.736138443320056\nGini Score: 0.4722768866401119\n\n\n\n\n\n\n\nBorderline SMOTE\n\nX_train_resampled, y_train_resampled = BorderlineSMOTE(random_state=42).fit_resample(xs_train, y_train)\ny_train_resampled.value_counts()\n\n0    27866\n1    27866\nName: count, dtype: int64\n\n\n\nrf_classifier = RandomForestClassifier(random_state=42)\nrf_classifier.fit(X_train_resampled, y_train_resampled)\n\ny_test_preds = rf_classifier.predict_proba(xs_test)[:, 1]\n\nprint(classification_report(y_test, (y_test_preds &gt; 0.5).astype(int)))\n\nprint(f\"ROC AUC Score: {roc_auc_score(y_test, y_test_preds)}\")\nprint(f\"Gini Score: {2 * roc_auc_score(y_test, y_test_preds) - 1}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nConfusionMatrixDisplay(confusion_matrix(y_test, (y_test_preds &gt; 0.5).astype(int))).plot(ax=axes[0])\nRocCurveDisplay.from_estimator(rf_classifier, xs_test, y_test, ax=axes[1])\naxes[1].set_xlim((0, 1))\naxes[1].set_ylim((0, 1))\naxes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"red\")\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      9289\n           1       0.16      0.14      0.15       100\n\n    accuracy                           0.98      9389\n   macro avg       0.58      0.57      0.57      9389\nweighted avg       0.98      0.98      0.98      9389\n\nROC AUC Score: 0.7377419528474539\nGini Score: 0.47548390569490784\n\n\n\n\n\n\n\n\nAdjusting class weights\nThe class_weight parameter in RandomForestClassifier is used to adjust the weights of classes during the training process. Assigning higher weights to the minority class helps the classifier pay more attention to these samples, potentially improving its performance on the minority class without needing to resample the data.\nThere are several ways to use the parameter:\n\n1. None (default)\nWhen class_weight is set to None, all classes have equal weight. This means the classifier does not take class imbalance into account when making predictions.\n\n\n2. balanced\nWhen class_weight is set to 'balanced', the weights for each class are computed based on the number of samples in each class. The weights are calculated as follows:\nclass_weight = n_samples / (n_classes * np.bincount(y))\nThis results in higher weights for minority classes and lower weights for majority classes. Consequently, the classifier will pay more attention to the minority class samples during training.\n\nrf_classifier = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\nrf_classifier.fit(xs_train, y_train)\n\ny_test_preds = rf_classifier.predict_proba(xs_test)[:, 1]\n\nprint(classification_report(y_test, (y_test_preds &gt; 0.5).astype(int)))\n\nprint(f\"ROC AUC Score: {roc_auc_score(y_test, y_test_preds)}\")\nprint(f\"Gini Score: {2 * roc_auc_score(y_test, y_test_preds) - 1}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nConfusionMatrixDisplay(confusion_matrix(y_test, (y_test_preds &gt; 0.5).astype(int))).plot(ax=axes[0])\nRocCurveDisplay.from_estimator(rf_classifier, xs_test, y_test, ax=axes[1])\naxes[1].set_xlim((0, 1))\naxes[1].set_ylim((0, 1))\naxes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"red\")\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      9289\n           1       0.67      0.12      0.20       100\n\n    accuracy                           0.99      9389\n   macro avg       0.83      0.56      0.60      9389\nweighted avg       0.99      0.99      0.99      9389\n\nROC AUC Score: 0.6974808913768975\nGini Score: 0.394961782753795\n\n\n\n\n\n\n\n3. balanced_subsample\nThe balanced_subsample mode is the same as balanced except that weights are computed based on the bootstrap sample for every tree grown.\n\nrf_classifier = RandomForestClassifier(random_state=42, class_weight=\"balanced_subsample\")\nrf_classifier.fit(xs_train, y_train)\n\ny_test_preds = rf_classifier.predict_proba(xs_test)[:, 1]\n\nprint(classification_report(y_test, (y_test_preds &gt; 0.5).astype(int)))\n\nprint(f\"ROC AUC Score: {roc_auc_score(y_test, y_test_preds)}\")\nprint(f\"Gini Score: {2 * roc_auc_score(y_test, y_test_preds) - 1}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nConfusionMatrixDisplay(confusion_matrix(y_test, (y_test_preds &gt; 0.5).astype(int))).plot(ax=axes[0])\nRocCurveDisplay.from_estimator(rf_classifier, xs_test, y_test, ax=axes[1])\naxes[1].set_xlim((0, 1))\naxes[1].set_ylim((0, 1))\naxes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"red\")\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      9289\n           1       0.65      0.13      0.22       100\n\n    accuracy                           0.99      9389\n   macro avg       0.82      0.56      0.61      9389\nweighted avg       0.99      0.99      0.99      9389\n\nROC AUC Score: 0.7098584347077188\nGini Score: 0.4197168694154376"
  },
  {
    "objectID": "06_Computer_Vision/02_image_transforms.html",
    "href": "06_Computer_Vision/02_image_transforms.html",
    "title": "Image Transforms",
    "section": "",
    "text": "skip_exec: true\nimport math\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom skimage import data\nfrom skimage.transform import ProjectiveTransform, SimilarityTransform, warp\n\npd.set_option(\"display.max_columns\", None)\n\nPROJECT_ROOT = Path.cwd().parent.parent\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0"
  },
  {
    "objectID": "06_Computer_Vision/02_image_transforms.html#load-and-display-image",
    "href": "06_Computer_Vision/02_image_transforms.html#load-and-display-image",
    "title": "Image Transforms",
    "section": "Load and display image",
    "text": "Load and display image\n\nimg = data.text()\n\n\nprint(img.shape)\n\nimg\n\n(172, 448)\n\n\narray([[ 91,  94,  99, ..., 135, 135, 134],\n       [ 99, 104, 104, ..., 141, 141, 139],\n       [105, 109, 109, ..., 145, 142, 141],\n       ...,\n       [146, 146, 141, ..., 142, 142, 142],\n       [144, 140, 141, ..., 146, 141, 136],\n       [145, 136, 143, ..., 143, 134, 126]], dtype=uint8)\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(img, cmap=\"gray\")\nax.set_title(\"Original Image\")\nplt.show()"
  },
  {
    "objectID": "06_Computer_Vision/02_image_transforms.html#basic-geometric-transformation",
    "href": "06_Computer_Vision/02_image_transforms.html#basic-geometric-transformation",
    "title": "Image Transforms",
    "section": "Basic geometric transformation",
    "text": "Basic geometric transformation\nWe can use scikit-image to transform images in various ways. For instance we can rotate an image 90 degrees like so:\n\ntform = SimilarityTransform(\n    scale=1,\n    rotation=math.pi / 4,\n    translation=(img.shape[0] / 2, -100),\n)\nprint(tform.params)\n\n[[   0.70710678   -0.70710678   86.        ]\n [   0.70710678    0.70710678 -100.        ]\n [   0.            0.            1.        ]]\n\n\n\nrotated = warp(img, tform)\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(rotated, cmap=\"gray\")\nax.set_title(\"Original Image\")\nplt.show()"
  },
  {
    "objectID": "06_Computer_Vision/02_image_transforms.html#parameter-estimation",
    "href": "06_Computer_Vision/02_image_transforms.html#parameter-estimation",
    "title": "Image Transforms",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nOften however we will know what we want the final image to look like but not know what transformation parameters will achieve that. We can estimate the parameters using the least squares method. This can amongst other things be used for image registration or rectification, where you have a set of control points or homologous/corresponding points in two images.\nLet’s assume we want to recognize letters on a photograph which was not taken from the front but at a certain angle. In the simplest case of a plane paper surface the letters are projectively distorted. Simple matching algorithms would not be able to match such symbols. One solution to this problem would be to warp the image so that the distortion is removed and then apply a matching algorithm.\nThe sckit-image API is a little bit counter-intuitive. I think of the problem as being about “lifting” the image into a new projection. So I would think of the coordinates on the original image as being the source.\nHowever, sckit-image thinks in terms of defining a mapping from a “canonical” or “standard” shape to a desired transformed shape. So the coordinates on the original image are actually the destination. Let’s plot them now:\n\ndst = np.array(\n    [\n        [155, 15],\n        [65, 40],\n        [260, 130],\n        [360, 95],\n    ]\n)\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(img, cmap=\"gray\")\nax.plot(dst[:, 0], dst[:, 1], \"ro\")\nax.set_title(\"Original Image\")\nplt.show()\n\n\n\n\nThe source coordinates are the four corners of the desired image.\n\nsrc = np.array(\n    [\n        [0, 0],\n        [0, 50],\n        [300, 50],\n        [300, 0],\n    ]\n)\n\nWe can now initialse a ProjectiveTransform. The estimate method then finds a transformation matrix that maps points from src to dst. The warp method takes the original image and applies the transformation.\n\ntform = ProjectiveTransform()\ntform.estimate(src, dst)\nwarped = warp(img, tform, output_shape=img.shape)\n\n\nwarped.shape\n\n(172, 448)\n\n\nWe can now visualise the image and we see that the image has been warped such that the points that were at the dst coordinates are now at the src coordinates.\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(warped, cmap=\"gray\")\nax.plot(src[:, 0], src[:, 1], \"ro\")\nax.set_title(\"Warped Image\")\nplt.show()\n\n\n\n\nWe could if we wanted to adjust the output shape to just contain the area within the coordinates.\n\nwarped = warp(img, tform, output_shape=(50, 300))\n\nThis way we have effectively lifted that one line of text off the page and flattened it out.\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(warped, cmap=\"gray\")\nax.set_title(\"Warped Image\")\nplt.show()"
  },
  {
    "objectID": "Gradient Boosting/rossman_xgboost.html",
    "href": "Gradient Boosting/rossman_xgboost.html",
    "title": "XGBoost",
    "section": "",
    "text": "from pathlib import Path\n\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\nPROJECT_ROOT = Path.cwd().parent.parent\n\n\ntrain_df = pd.read_csv(f\"{PROJECT_ROOT}/data/rossmann-store-sales/train.csv\", low_memory=False)\ntest_df = pd.read_csv(f\"{PROJECT_ROOT}/data/rossmann-store-sales/test.csv\", low_memory=False)\nstore_df = pd.read_csv(f\"{PROJECT_ROOT}/data/rossmann-store-sales/store.csv\", low_memory=False)\nsample_submission_df = pd.read_csv(f\"{PROJECT_ROOT}/data/rossmann-store-sales/sample_submission.csv\", low_memory=False)\n\n\ntrain_df\n\n\n\n\n\n\n\n\nStore\nDayOfWeek\nDate\nSales\nCustomers\nOpen\nPromo\nStateHoliday\nSchoolHoliday\n\n\n\n\n0\n1\n5\n2015-07-31\n5263\n555\n1\n1\n0\n1\n\n\n1\n2\n5\n2015-07-31\n6064\n625\n1\n1\n0\n1\n\n\n2\n3\n5\n2015-07-31\n8314\n821\n1\n1\n0\n1\n\n\n3\n4\n5\n2015-07-31\n13995\n1498\n1\n1\n0\n1\n\n\n4\n5\n5\n2015-07-31\n4822\n559\n1\n1\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1017204\n1111\n2\n2013-01-01\n0\n0\n0\n0\na\n1\n\n\n1017205\n1112\n2\n2013-01-01\n0\n0\n0\n0\na\n1\n\n\n1017206\n1113\n2\n2013-01-01\n0\n0\n0\n0\na\n1\n\n\n1017207\n1114\n2\n2013-01-01\n0\n0\n0\n0\na\n1\n\n\n1017208\n1115\n2\n2013-01-01\n0\n0\n0\n0\na\n1\n\n\n\n\n1017209 rows × 9 columns\n\n\n\n\ntest_df\n\n\n\n\n\n\n\n\nId\nStore\nDayOfWeek\nDate\nOpen\nPromo\nStateHoliday\nSchoolHoliday\n\n\n\n\n0\n1\n1\n4\n2015-09-17\n1.0\n1\n0\n0\n\n\n1\n2\n3\n4\n2015-09-17\n1.0\n1\n0\n0\n\n\n2\n3\n7\n4\n2015-09-17\n1.0\n1\n0\n0\n\n\n3\n4\n8\n4\n2015-09-17\n1.0\n1\n0\n0\n\n\n4\n5\n9\n4\n2015-09-17\n1.0\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41083\n41084\n1111\n6\n2015-08-01\n1.0\n0\n0\n0\n\n\n41084\n41085\n1112\n6\n2015-08-01\n1.0\n0\n0\n0\n\n\n41085\n41086\n1113\n6\n2015-08-01\n1.0\n0\n0\n0\n\n\n41086\n41087\n1114\n6\n2015-08-01\n1.0\n0\n0\n0\n\n\n41087\n41088\n1115\n6\n2015-08-01\n1.0\n0\n0\n1\n\n\n\n\n41088 rows × 8 columns\n\n\n\n\nstore_df\n\n\n\n\n\n\n\n\nStore\nStoreType\nAssortment\nCompetitionDistance\nCompetitionOpenSinceMonth\nCompetitionOpenSinceYear\nPromo2\nPromo2SinceWeek\nPromo2SinceYear\nPromoInterval\n\n\n\n\n0\n1\nc\na\n1270.0\n9.0\n2008.0\n0\nNaN\nNaN\nNaN\n\n\n1\n2\na\na\n570.0\n11.0\n2007.0\n1\n13.0\n2010.0\nJan,Apr,Jul,Oct\n\n\n2\n3\na\na\n14130.0\n12.0\n2006.0\n1\n14.0\n2011.0\nJan,Apr,Jul,Oct\n\n\n3\n4\nc\nc\n620.0\n9.0\n2009.0\n0\nNaN\nNaN\nNaN\n\n\n4\n5\na\na\n29910.0\n4.0\n2015.0\n0\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1110\n1111\na\na\n1900.0\n6.0\n2014.0\n1\n31.0\n2013.0\nJan,Apr,Jul,Oct\n\n\n1111\n1112\nc\nc\n1880.0\n4.0\n2006.0\n0\nNaN\nNaN\nNaN\n\n\n1112\n1113\na\nc\n9260.0\nNaN\nNaN\n0\nNaN\nNaN\nNaN\n\n\n1113\n1114\na\nc\n870.0\nNaN\nNaN\n0\nNaN\nNaN\nNaN\n\n\n1114\n1115\nd\nc\n5350.0\nNaN\nNaN\n1\n22.0\n2012.0\nMar,Jun,Sept,Dec\n\n\n\n\n1115 rows × 10 columns\n\n\n\n\nsample_submission_df\n\n\n\n\n\n\n\n\nId\nSales\n\n\n\n\n0\n1\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n4\n0\n\n\n4\n5\n0\n\n\n...\n...\n...\n\n\n41083\n41084\n0\n\n\n41084\n41085\n0\n\n\n41085\n41086\n0\n\n\n41086\n41087\n0\n\n\n41087\n41088\n0\n\n\n\n\n41088 rows × 2 columns\n\n\n\n\ntrain_df = pd.merge(train_df, store_df, on=\"Store\", how=\"left\")\ntest_df = pd.merge(test_df, store_df, on=\"Store\", how=\"left\")\n\n\ndef split_date(df):\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    df[\"Year\"] = df.Date.dt.year\n    df[\"Month\"] = df.Date.dt.month\n    df[\"Day\"] = df.Date.dt.day\n    df[\"WeekOfYear\"] = df.Date.dt.isocalendar().week\n    return df\n\n\ntrain_df = split_date(train_df)\ntest_df = split_date(test_df)\n\n\ntrain_df\n\n\n\n\n\n\n\n\nStore\nDayOfWeek\nDate\nSales\nCustomers\nOpen\nPromo\nStateHoliday\nSchoolHoliday\nStoreType\n...\nCompetitionOpenSinceMonth\nCompetitionOpenSinceYear\nPromo2\nPromo2SinceWeek\nPromo2SinceYear\nPromoInterval\nYear\nMonth\nDay\nWeekOfYear\n\n\n\n\n0\n1\n5\n2015-07-31\n5263\n555\n1\n1\n0\n1\nc\n...\n9.0\n2008.0\n0\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n\n\n1\n2\n5\n2015-07-31\n6064\n625\n1\n1\n0\n1\na\n...\n11.0\n2007.0\n1\n13.0\n2010.0\nJan,Apr,Jul,Oct\n2015\n7\n31\n31\n\n\n2\n3\n5\n2015-07-31\n8314\n821\n1\n1\n0\n1\na\n...\n12.0\n2006.0\n1\n14.0\n2011.0\nJan,Apr,Jul,Oct\n2015\n7\n31\n31\n\n\n3\n4\n5\n2015-07-31\n13995\n1498\n1\n1\n0\n1\nc\n...\n9.0\n2009.0\n0\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n\n\n4\n5\n5\n2015-07-31\n4822\n559\n1\n1\n0\n1\na\n...\n4.0\n2015.0\n0\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1017204\n1111\n2\n2013-01-01\n0\n0\n0\n0\na\n1\na\n...\n6.0\n2014.0\n1\n31.0\n2013.0\nJan,Apr,Jul,Oct\n2013\n1\n1\n1\n\n\n1017205\n1112\n2\n2013-01-01\n0\n0\n0\n0\na\n1\nc\n...\n4.0\n2006.0\n0\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n\n\n1017206\n1113\n2\n2013-01-01\n0\n0\n0\n0\na\n1\na\n...\nNaN\nNaN\n0\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n\n\n1017207\n1114\n2\n2013-01-01\n0\n0\n0\n0\na\n1\na\n...\nNaN\nNaN\n0\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n\n\n1017208\n1115\n2\n2013-01-01\n0\n0\n0\n0\na\n1\nd\n...\nNaN\nNaN\n1\n22.0\n2012.0\nMar,Jun,Sept,Dec\n2013\n1\n1\n1\n\n\n\n\n1017209 rows × 22 columns\n\n\n\n\ntrain_df = train_df.query(\"Open == 1\").copy()\n\n\ntrain_df\n\n\n\n\n\n\n\n\nStore\nDayOfWeek\nDate\nSales\nCustomers\nOpen\nPromo\nStateHoliday\nSchoolHoliday\nStoreType\n...\nCompetitionOpenSinceMonth\nCompetitionOpenSinceYear\nPromo2\nPromo2SinceWeek\nPromo2SinceYear\nPromoInterval\nYear\nMonth\nDay\nWeekOfYear\n\n\n\n\n0\n1\n5\n2015-07-31\n5263\n555\n1\n1\n0\n1\nc\n...\n9.0\n2008.0\n0\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n\n\n1\n2\n5\n2015-07-31\n6064\n625\n1\n1\n0\n1\na\n...\n11.0\n2007.0\n1\n13.0\n2010.0\nJan,Apr,Jul,Oct\n2015\n7\n31\n31\n\n\n2\n3\n5\n2015-07-31\n8314\n821\n1\n1\n0\n1\na\n...\n12.0\n2006.0\n1\n14.0\n2011.0\nJan,Apr,Jul,Oct\n2015\n7\n31\n31\n\n\n3\n4\n5\n2015-07-31\n13995\n1498\n1\n1\n0\n1\nc\n...\n9.0\n2009.0\n0\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n\n\n4\n5\n5\n2015-07-31\n4822\n559\n1\n1\n0\n1\na\n...\n4.0\n2015.0\n0\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1016776\n682\n2\n2013-01-01\n3375\n566\n1\n0\na\n1\nb\n...\n9.0\n2006.0\n0\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n\n\n1016827\n733\n2\n2013-01-01\n10765\n2377\n1\n0\na\n1\nb\n...\n10.0\n1999.0\n0\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n\n\n1016863\n769\n2\n2013-01-01\n5035\n1248\n1\n0\na\n1\nb\n...\nNaN\nNaN\n1\n48.0\n2012.0\nJan,Apr,Jul,Oct\n2013\n1\n1\n1\n\n\n1017042\n948\n2\n2013-01-01\n4491\n1039\n1\n0\na\n1\nb\n...\nNaN\nNaN\n0\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n\n\n1017190\n1097\n2\n2013-01-01\n5961\n1405\n1\n0\na\n1\nb\n...\n3.0\n2002.0\n0\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n\n\n\n\n844392 rows × 22 columns\n\n\n\n\ndef comp_months(df):\n    df[\"CompetitionOpen\"] = 12 * (df[\"Year\"] - df[\"CompetitionOpenSinceYear\"]) + (\n        df[\"Month\"] - df[\"CompetitionOpenSinceMonth\"]\n    )\n    df[\"CompetitionOpen\"] = df[\"CompetitionOpen\"].map(lambda x: 0 if x &lt; 0 else x).fillna(0)\n    return df\n\n\ntrain_df = comp_months(train_df)\ntest_df = comp_months(test_df)\n\n\ndef check_promo_month(row):\n    month2str = {\n        1: \"Jan\",\n        2: \"Feb\",\n        3: \"Mar\",\n        4: \"Apr\",\n        5: \"May\",\n        6: \"Jun\",\n        7: \"Jul\",\n        8: \"Aug\",\n        9: \"Sept\",\n        10: \"Oct\",\n        11: \"Nov\",\n        12: \"Dec\",\n    }\n    try:\n        # given input row the promointerval column is indexed and split by \",\"\n        months = (row[\"PromoInterval\"] or \"\").split(\",\")\n        # if the row indexed promo2open column and the row month are in months you return 1\n        if row[\"Promo2Open\"] and month2str[row[\"Month\"]] in months:\n            return 1\n        else:\n            return 0\n    except Exception:\n        return 0\n\n\ndef promo_cols(df):\n    # Months since Promo2 was open\n    df[\"Promo2Open\"] = 12 * (df.Year - df.Promo2SinceYear) + (df.WeekOfYear - df.Promo2SinceWeek) * 7 / 30.5\n    df[\"Promo2Open\"] = df[\"Promo2Open\"].fillna(0).map(lambda x: 0 if x &lt; 0 else x) * df[\"Promo2\"]\n    # Whether a new round of promotions was started in the current month\n    df[\"IsPromo2Month\"] = df.apply(check_promo_month, axis=1) * df[\"Promo2\"]\n    return df\n\n\ntrain_df = promo_cols(train_df)\ntest_df = promo_cols(test_df)\n\n\ntrain_df\n\n\n\n\n\n\n\n\nStore\nDayOfWeek\nDate\nSales\nCustomers\nOpen\nPromo\nStateHoliday\nSchoolHoliday\nStoreType\n...\nPromo2SinceWeek\nPromo2SinceYear\nPromoInterval\nYear\nMonth\nDay\nWeekOfYear\nCompetitionOpen\nPromo2Open\nIsPromo2Month\n\n\n\n\n0\n1\n5\n2015-07-31\n5263\n555\n1\n1\n0\n1\nc\n...\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n82.0\n0.000000\n0\n\n\n1\n2\n5\n2015-07-31\n6064\n625\n1\n1\n0\n1\na\n...\n13.0\n2010.0\nJan,Apr,Jul,Oct\n2015\n7\n31\n31\n92.0\n64.131148\n1\n\n\n2\n3\n5\n2015-07-31\n8314\n821\n1\n1\n0\n1\na\n...\n14.0\n2011.0\nJan,Apr,Jul,Oct\n2015\n7\n31\n31\n103.0\n51.901639\n1\n\n\n3\n4\n5\n2015-07-31\n13995\n1498\n1\n1\n0\n1\nc\n...\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n70.0\n0.000000\n0\n\n\n4\n5\n5\n2015-07-31\n4822\n559\n1\n1\n0\n1\na\n...\nNaN\nNaN\nNaN\n2015\n7\n31\n31\n3.0\n0.000000\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1016776\n682\n2\n2013-01-01\n3375\n566\n1\n0\na\n1\nb\n...\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n76.0\n0.000000\n0\n\n\n1016827\n733\n2\n2013-01-01\n10765\n2377\n1\n0\na\n1\nb\n...\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n159.0\n0.000000\n0\n\n\n1016863\n769\n2\n2013-01-01\n5035\n1248\n1\n0\na\n1\nb\n...\n48.0\n2012.0\nJan,Apr,Jul,Oct\n2013\n1\n1\n1\n0.0\n1.213115\n1\n\n\n1017042\n948\n2\n2013-01-01\n4491\n1039\n1\n0\na\n1\nb\n...\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n0.0\n0.000000\n0\n\n\n1017190\n1097\n2\n2013-01-01\n5961\n1405\n1\n0\na\n1\nb\n...\nNaN\nNaN\nNaN\n2013\n1\n1\n1\n130.0\n0.000000\n0\n\n\n\n\n844392 rows × 25 columns\n\n\n\n\ninput_cols = [\n    \"Store\",\n    \"DayOfWeek\",\n    \"Promo\",\n    \"StateHoliday\",\n    \"SchoolHoliday\",\n    \"StoreType\",\n    \"Assortment\",\n    \"CompetitionDistance\",\n    \"CompetitionOpen\",\n    \"Day\",\n    \"Month\",\n    \"Year\",\n    \"WeekOfYear\",\n    \"Promo2\",\n    \"Promo2Open\",\n    \"IsPromo2Month\",\n]\ntarget_col = \"Sales\"\n\n\ninputs = train_df[input_cols].copy()\ntargets = train_df[target_col].copy()\n\n\ntest_inputs = test_df[input_cols].copy()\n\n\nnumeric_cols = [\n    \"Store\",\n    \"Promo\",\n    \"SchoolHoliday\",\n    \"CompetitionDistance\",\n    \"CompetitionOpen\",\n    \"Promo2\",\n    \"Promo2Open\",\n    \"IsPromo2Month\",\n    \"Day\",\n    \"Month\",\n    \"Year\",\n    \"WeekOfYear\",\n]\ncategorical_cols = [\"DayOfWeek\", \"StateHoliday\", \"StoreType\", \"Assortment\"]\n\n\ninputs[numeric_cols].isna().sum()\n\nStore                     0\nPromo                     0\nSchoolHoliday             0\nCompetitionDistance    2186\nCompetitionOpen           0\nPromo2                    0\nPromo2Open                0\nIsPromo2Month             0\nDay                       0\nMonth                     0\nYear                      0\nWeekOfYear                0\ndtype: int64\n\n\n\ntest_inputs[numeric_cols].isna().sum()\n\nStore                   0\nPromo                   0\nSchoolHoliday           0\nCompetitionDistance    96\nCompetitionOpen         0\nPromo2                  0\nPromo2Open              0\nIsPromo2Month           0\nDay                     0\nMonth                   0\nYear                    0\nWeekOfYear              0\ndtype: int64\n\n\n\nmax_distance = inputs[\"CompetitionDistance\"].max()\n\n\ninputs[\"CompetitionDistance\"] = inputs[\"CompetitionDistance\"].fillna(max_distance)\ntest_inputs[\"CompetitionDistance\"] = test_inputs[\"CompetitionDistance\"].fillna(max_distance)\n\n\nscaler = MinMaxScaler().fit(inputs[numeric_cols])\n\n\ninputs[numeric_cols] = scaler.transform(inputs[numeric_cols])\ntest_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])\n\n\nencoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\").fit(inputs[categorical_cols])\nencoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n\n\ninputs[encoded_cols] = encoder.transform(inputs[categorical_cols])\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n\n\nX = inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols]"
  },
  {
    "objectID": "Calculus/directional_derivatives.html",
    "href": "Calculus/directional_derivatives.html",
    "title": "Directional Derivatives",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "Calculus/directional_derivatives.html#references",
    "href": "Calculus/directional_derivatives.html#references",
    "title": "Directional Derivatives",
    "section": "References",
    "text": "References\n\nAn introduction to the directional derivative and the gradient"
  },
  {
    "objectID": "Calculus/partial_derivatives.html",
    "href": "Calculus/partial_derivatives.html",
    "title": "Partial Derivatives",
    "section": "",
    "text": "Your heating bill depends on the average temperature outside. If all other factors remain constant, then the heating bill will increase when temperatures drop. Let’s denote average temperature by \\(t\\) and define a function \\(h:\\mathbb{R}\\to\\mathbb{R}\\) where \\(h(t)\\) is the heating bill as a function of \\(t\\).\nWe can then interpret the ordinary derivative as indicating how much the heating bill will change as you change the temperature:\n\\[\n\\frac{dh}{dt}(a) = \\frac{\\text{change in h}}{\\text{change in t}}(\\text{at t = a})\n\\]\nIf we plot \\(h\\) as a function of \\(t\\) then \\(\\frac{dh}{dt}(a)\\) gives the slope of the graph at the point where \\(t = a\\). We say that \\(\\frac{dh}{dt}\\) is the derivative of \\(h\\) with respect to \\(t\\). If \\(t\\) is given in degrees Celsius then \\(\\frac{dh}{dt}(a)\\) is the change in the heating cost per degree Celsius of temperature increase when the temperature is \\(a\\). Since \\(h\\) decreases as \\(t\\) increases, we would expect \\(\\frac{dh}{dt}\\) to be negative (the rate of change in heating cost per degree of Celsius of temperature decrease is positive. But this positive rate is equal to \\(-\\frac{dh}{dt}\\)).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nIn this example we have a curve:\n\\[\nh(t) = -0.05t^2 - 2t + 50\n\\]\nwhere \\(t\\) is the average temperature in degrees Celsius.\nThe derivative of this curve is:\n\\[\n\\frac{dh}{dt} = -0.1t - 2\n\\]\nThe derivative gives us the slope of the curve at any point. Since we would like to plot the tangent of the curve were \\(t = 1\\), we plug that in and get the slope of the tangent line \\(m = -0.1 \\cdot 1 - 2 = -2.1\\).\nTo find the y-intercept of the tangent line, we first find the value of \\(h\\) at \\(t = 1\\):\n\\[\n\\begin{align}\nh(1) &= -0.05 \\cdot 1^2 - 2 \\cdot 1 + 50 \\\\\n&= 47.95\n\\end{align}\n\\]\nWe can find then the y-intercept by using the formula:\n\\[\n(y - y_1) = m(x - x_1)\n\\]\nTherefore:\n\\[\n\\begin{align}\nh - 47.95 &= -2.1(t - 1) \\\\\nh &= -2.1t + 2.1 + 47.95 \\\\\nh &= -2.1t + 50.05\n\\end{align}\n\\]\n\ndef f(x):\n    return -0.05 * x**2 - 2 * x + 50\n\n\ndef df(x):\n    return -0.1 * x - 2\n\n\ndef tangent_line(x, x0, y0):\n    slope = df(x0)\n    return slope * (x - x0) + y0\n\n\nx0 = 1\ny0 = f(x0)\n\nx = np.linspace(-5, 20, 100)\n\ny = f(x)\n\ntangent_y = tangent_line(x, x0, y0)\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(x, y, label=\"f(x)\")\nax.plot(x, tangent_y, label=f\"Tangent at ({x0}, {y0})\", linestyle=\"--\")\n\nax.set_xmargin(0)\nax.set_ymargin(0)\n\nax.spines[\"left\"].set_position(\"zero\")\nax.spines[\"bottom\"].set_position(\"zero\")\n\nax.spines[\"right\"].set_color(\"none\")\nax.spines[\"top\"].set_color(\"none\")\n\nax.set(xticks=np.arange(-5, 21, 1))\nax.set(yticks=np.arange(-5, 61, 5))\n\nax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Calculus/partial_derivatives.html#ordinary-derivatives-in-one-variable-calculus",
    "href": "Calculus/partial_derivatives.html#ordinary-derivatives-in-one-variable-calculus",
    "title": "Partial Derivatives",
    "section": "",
    "text": "Your heating bill depends on the average temperature outside. If all other factors remain constant, then the heating bill will increase when temperatures drop. Let’s denote average temperature by \\(t\\) and define a function \\(h:\\mathbb{R}\\to\\mathbb{R}\\) where \\(h(t)\\) is the heating bill as a function of \\(t\\).\nWe can then interpret the ordinary derivative as indicating how much the heating bill will change as you change the temperature:\n\\[\n\\frac{dh}{dt}(a) = \\frac{\\text{change in h}}{\\text{change in t}}(\\text{at t = a})\n\\]\nIf we plot \\(h\\) as a function of \\(t\\) then \\(\\frac{dh}{dt}(a)\\) gives the slope of the graph at the point where \\(t = a\\). We say that \\(\\frac{dh}{dt}\\) is the derivative of \\(h\\) with respect to \\(t\\). If \\(t\\) is given in degrees Celsius then \\(\\frac{dh}{dt}(a)\\) is the change in the heating cost per degree Celsius of temperature increase when the temperature is \\(a\\). Since \\(h\\) decreases as \\(t\\) increases, we would expect \\(\\frac{dh}{dt}\\) to be negative (the rate of change in heating cost per degree of Celsius of temperature decrease is positive. But this positive rate is equal to \\(-\\frac{dh}{dt}\\)).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nIn this example we have a curve:\n\\[\nh(t) = -0.05t^2 - 2t + 50\n\\]\nwhere \\(t\\) is the average temperature in degrees Celsius.\nThe derivative of this curve is:\n\\[\n\\frac{dh}{dt} = -0.1t - 2\n\\]\nThe derivative gives us the slope of the curve at any point. Since we would like to plot the tangent of the curve were \\(t = 1\\), we plug that in and get the slope of the tangent line \\(m = -0.1 \\cdot 1 - 2 = -2.1\\).\nTo find the y-intercept of the tangent line, we first find the value of \\(h\\) at \\(t = 1\\):\n\\[\n\\begin{align}\nh(1) &= -0.05 \\cdot 1^2 - 2 \\cdot 1 + 50 \\\\\n&= 47.95\n\\end{align}\n\\]\nWe can find then the y-intercept by using the formula:\n\\[\n(y - y_1) = m(x - x_1)\n\\]\nTherefore:\n\\[\n\\begin{align}\nh - 47.95 &= -2.1(t - 1) \\\\\nh &= -2.1t + 2.1 + 47.95 \\\\\nh &= -2.1t + 50.05\n\\end{align}\n\\]\n\ndef f(x):\n    return -0.05 * x**2 - 2 * x + 50\n\n\ndef df(x):\n    return -0.1 * x - 2\n\n\ndef tangent_line(x, x0, y0):\n    slope = df(x0)\n    return slope * (x - x0) + y0\n\n\nx0 = 1\ny0 = f(x0)\n\nx = np.linspace(-5, 20, 100)\n\ny = f(x)\n\ntangent_y = tangent_line(x, x0, y0)\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\n\nax.plot(x, y, label=\"f(x)\")\nax.plot(x, tangent_y, label=f\"Tangent at ({x0}, {y0})\", linestyle=\"--\")\n\nax.set_xmargin(0)\nax.set_ymargin(0)\n\nax.spines[\"left\"].set_position(\"zero\")\nax.spines[\"bottom\"].set_position(\"zero\")\n\nax.spines[\"right\"].set_color(\"none\")\nax.spines[\"top\"].set_color(\"none\")\n\nax.set(xticks=np.arange(-5, 21, 1))\nax.set(yticks=np.arange(-5, 61, 5))\n\nax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Calculus/partial_derivatives.html#partial-derivatives-are-analogous-to-ordinary-derivatives",
    "href": "Calculus/partial_derivatives.html#partial-derivatives-are-analogous-to-ordinary-derivatives",
    "title": "Partial Derivatives",
    "section": "Partial derivatives are analogous to ordinary derivatives",
    "text": "Partial derivatives are analogous to ordinary derivatives\nClearly, writing the heating bill as a function of temperature is a gross oversimplification. The heating bill will depend on other factors. For instance the amount of insulation in your house, which we’ll denote by \\(i\\). We can define a new function \\(h:\\mathbb{R}^2\\to\\mathbb{R}\\) where \\(h(t,i)\\) gives the heating bill as function of both temperature \\(t\\) and insulation \\(i\\).\nSuppose you aren’t changing the amount of insulation in your house, so that we view \\(i\\) as a fixed number. Then, if we look at how the heating bill changes as temperature changes, we’re back to our first case above. The only difference is that we now view \\(h\\) as a function of both \\(t\\) and \\(i\\), and we are explicitly leaving one of the variables (\\(i\\)) constant. In this case, we call the change in \\(h\\) the partial derivative of \\(h\\) with respect to \\(t\\), a term that reflects the fact some variables remain constant. We also change our notation by writing the \\(d\\) as a \\(\\partial\\), so that\n\\[\n\\frac{\\partial h}{\\partial t}(a, b) = \\frac{\\text{change in h}}{\\text{change in t}}(\\text{at t = a while holding i constant at b})\n\\]\nIf \\(t\\) is given in degrees Celsius, then \\(\\frac{\\partial h}{\\partial t}(a, b)\\) is change in heating cost per degree Celsius of temperature increase when the outside temperature is \\(a\\) and the amount of insulation is \\(b\\).\nNow, imagine you are considering the possibility of lowering your heating bill by installing additional insulation. To help you decide if it will be worth your money, you may want to know how much adding insulation will decrease the heating bill, assuming the temperature remains constant. In other words, you want to know the partial derivative of \\(h\\) with respect to \\(i\\):\n\\[\n\\frac{\\partial h}{\\partial i}(a, b) = \\frac{\\text{change in h}}{\\text{change in i}}(\\text{at i = b while holding t constant at a})\n\\]\nIf \\(i\\) is given in centimeters of insulation, then \\(\\frac{\\partial h}{\\partial i}(a, b)\\) is change in heating cost per added centimeter of insulation when the outside temperature is \\(a\\) and the amount of insulation is \\(b\\).\nThe partial derivative \\(\\frac{\\partial h}{\\partial i}\\) indicates how much effect additional insulation will have on the heating bill. Since additional insulation will presumably lower the heating bill, \\(\\frac{\\partial h}{\\partial i}\\) will be negative. If additional insulation will have a large effect, then \\(\\frac{\\partial h}{\\partial i}\\) will be a large, negative number. If, for your house, \\(\\frac{\\partial h}{\\partial i}\\) is large and negative, you may be inclined to add insulation to save money.\nIn the graph of \\(h(t,i)\\), the partial derivatives can be viewed as the slopes of the graphs in the \\(t\\) direction and in the \\(i\\) direction.\n\n# TODO Add partial derivative plots to the surface plot\n\n\n# Define a multivariable function\ndef f(x, y):\n    return -0.05 * x**2 - 0.05 * y**2 - 2 * x - 2 * y + 50\n\n\n# Define the partial derivatives with respect to x and y\ndef df_dx(x, y):\n    return 2 * x\n\n\ndef df_dy(x, y):\n    return 2 * y\n\n\ndef tangent_line_x(x, x0, y0):\n    slope = df_dx(x0)\n    return slope * (x - x0) + y0\n\n\ndef tangent_line_y(x, x0, y0):\n    slope = df_dy(x0)\n    return slope * (x - x0) + y0\n\n\n# Generate x and y values for plotting\nt = np.linspace(-5, 20, 100)\ni = np.linspace(0, 10, 10)\n\n# Create a meshgrid to compute the function values and partial derivatives at each point\nX, Y = np.meshgrid(t, i)\nZ = f(X, Y)\n\n# Create a 3D plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Plot the function's surface\nax.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.5)\n\nax.set_xmargin(0)\nax.set_ymargin(0)\n\n# Customize the plot\nax.set_xlabel(\"t\")\nax.set_ylabel(\"i\")\nax.set_zlabel(\"h\")\nax.set_title(\"Curve on a 3D Plane with Partial Derivatives as Tangents\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "Calculus/partial_derivatives.html#examples-of-calculating-partial-derivatives",
    "href": "Calculus/partial_derivatives.html#examples-of-calculating-partial-derivatives",
    "title": "Partial Derivatives",
    "section": "Examples of calculating partial derivatives",
    "text": "Examples of calculating partial derivatives\nOnce you understand the concept of a partial derivative as the rate that something is changing, calculating partial derivatives usually isn’t difficult. (Unfortunately, there are special cases where calculating the partial derivatives is hard.) As these examples show, calculating a partial derivatives is usually just like calculating an ordinary derivative of one-variable calculus. You just have to remember with which variable you are taking the derivative.\n\nExample 1\nLet \\(f(x,y) = y^3x^2\\). Calulate \\(\\frac{\\partial f}{\\partial x}(x,y)\\)\nSolution:\nWe simply view \\(y\\) as being a fixed number and calculate the ordinary derivative with respect to \\(x\\). \\(x^2\\) becomes \\(2x\\) so we are left with:\n\\[\n\\frac{\\partial f}{\\partial x}(x,y) = 2y^3x\n\\]\n\n\nExample 2\nAlso for \\(f(x,y) = y^3x^2\\). Calulate \\(\\frac{\\partial f}{\\partial y}(x,y)\\)\nSolution:\nBecause this time we are finding the derivative with respect to \\(y\\) we treat \\(x\\) as a constant. \\(y^3\\) becomes \\(3y^2\\) so we have:\n\\[\n\\frac{\\partial f}{\\partial y}(x,y) = 3x^2y^2\n\\]\n\n\nExample 3\nAlso for \\(f(x,y) = y^3x^2\\). Calulate \\(\\frac{\\partial f}{\\partial y}(1,2)\\)\nSolution:\n\\[\n\\begin{align}\n\\frac{\\partial f}{\\partial x}(x,y) &= 2y^3x \\\\\n\\frac{\\partial f}{\\partial x}(1,2) &= 2 \\times 2^3 \\times 1 \\\\\n&= 16\n\\end{align}\n\\]\n\n\nExample 4\nFor\n\\[\nf(x_1, x_2, x_3, x_4) = 3(\\frac{cos(x_1x_4)sin(x_2^5)}{\\frac{e^{x_2} + (1 + x_2^2)}{x_1x_2x_4}}) + 5x_1x_3x_4\n\\]\ncalculate \\(\\frac{\\partial f}{\\partial x_3}(a,b,c,d)\\)\nSolution:\nAlthough this initially looks hard, it’s really any easy problem. The ugly term does not depend on \\(x_3\\), so in calculating partial derivative with respect to \\(x_3\\), we treat it as a constant. The derivative of a constant is 0, so that term drops out. The derivative is just the derivative of the last term with respect to \\(x_3\\), which is\n\\[\n\\frac{\\partial f}{\\partial x_3}(x_1, x_2, x_3, x_4) = 5x_1x_4\n\\]\nSubstituting in the values \\((x_1, x_2, x_3, x_4)=(a,b,c,d)\\), we obtain the final answer\n\\[\n\\frac{\\partial f}{\\partial x_3}(a,b,c,d) = 5ad\n\\]\n\n\nExample 5\nGiven\n\\[\np(y_1, y_2, y_3) = 9(\\frac{y_1, y_2, y_3}{y_1 + y_2 + y_3})\n\\]\ncalculate \\(\\frac{\\partial p}{\\partial y_3}(y_1, y_2, y_3)\\) at the point \\((y_1, y_2, y_3) = (1, -2, 4)\\)\nSolution:\nIn calculating partial derivatives, we can use all the rules for ordinary derivatives. We can calculate \\(\\frac{\\partial p}{\\partial y_3}\\) using the quotient rule:\n\\[\n\\frac{\\partial p}{\\partial y_3}(y_1, y_2, y_3) = 9(\\frac{(y_1 + y_2 + y_3)}{(y_1 + y_2 + y_3)^2})\n\\]\nTODO: finish this example"
  },
  {
    "objectID": "Calculus/partial_derivatives.html#references",
    "href": "Calculus/partial_derivatives.html#references",
    "title": "Partial Derivatives",
    "section": "References",
    "text": "References\n\nIntroduction to partial derivatives\nhttps://mathinsight.org/partial_derivative_examples"
  },
  {
    "objectID": "05_Interpretability/decile_analysis.html",
    "href": "05_Interpretability/decile_analysis.html",
    "title": "Decile Analysis",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\")\nrng = np.random.RandomState(seed=42)\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n\ncategorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\n\nRF train accuracy: 0.972\nRF test accuracy: 0.787\n\n\n\ny_test = y_test.astype(int)\n\n\ny_test.mean()\n\n0.38109756097560976"
  },
  {
    "objectID": "05_Interpretability/decile_analysis.html#train-a-model-and-get-predictions",
    "href": "05_Interpretability/decile_analysis.html#train-a-model-and-get-predictions",
    "title": "Decile Analysis",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\")\nrng = np.random.RandomState(seed=42)\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n\ncategorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\n\nRF train accuracy: 0.972\nRF test accuracy: 0.787\n\n\n\ny_test = y_test.astype(int)\n\n\ny_test.mean()\n\n0.38109756097560976"
  },
  {
    "objectID": "05_Interpretability/decile_analysis.html#gain-curve",
    "href": "05_Interpretability/decile_analysis.html#gain-curve",
    "title": "Decile Analysis",
    "section": "Gain Curve",
    "text": "Gain Curve\nTo calculate the gain for a given proportion of a sample, we first sort the true y values by the size of the model’s predicted probability. We count the number of positives inside this proportion and divide it by the total number of positives inside the whole sample.\n\\[\n\\text{gain} = \\frac{\\text{\\# positives in subset}}{\\text{\\# positives in sample}}\n\\]\n\ndef plot_gain_curve(preds, ys, title=\"Gain Curve\", ax=None):\n    sorted_ys = ys.iloc[np.argsort(preds)[::-1]]\n    gain_values = sorted_ys.cumsum() / sum(sorted_ys == 1)\n    if ax is None:\n        ax = plt.axes()\n    ax.set_xlabel(\"Proportion of sample\")\n    ax.set_ylabel(\"Gain\")\n    ax.set_title(title)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    sns.lineplot(x=[0, 1], y=[0, 1], linestyle=\"dashed\", ax=ax)\n    sns.lineplot(x=[x / len(gain_values) for x in range(len(gain_values))], y=gain_values, ax=ax)\n\n\nfig = plt.figure(figsize=(12, 3))\ngs = fig.add_gridspec(1, 4, hspace=0.2, wspace=0.2)\naxes = gs.subplots(sharex=\"col\", sharey=\"row\")\npreds_perfect = y_test\nplot_gain_curve(preds_perfect, y_test, ax=axes[0], title=\"Perfect Gain Curve\")\npreds_test = rf.predict_proba(X_test)[:, 1]\nplot_gain_curve(preds_test, y_test, ax=axes[1], title=\"Random Forest Gain Curve\")\npreds_random = np.random.rand(len(y_test))\nplot_gain_curve(preds_random, y_test, ax=axes[2], title=\"Random Gain Curve\")\npreds_opposite = y_test.apply(lambda x: not (x) * 1)\nplot_gain_curve(preds_opposite, y_test, ax=axes[3], title=\"Opposite Gain Curve\")\nfor ax in fig.get_axes():\n    ax.label_outer()\nplt.show()\n\nAttributeError: 'GridSpec' object has no attribute 'subplots'\n\n\n&lt;Figure size 864x216 with 0 Axes&gt;\n\n\nWithin our test set 38% of people died. A theoretically perfect model would be able to find you those 38% by returning postive values for 38% of the dataset for a 100% “gain”. In practice to get that 38% you have to accept some false positives. It takes more than 38% of the dataset to get you the 38% of people who died. In fact it takes 100% to get all the way there because the model will also return false negatives with very low predicted probabilities of dying. However we can see it still does better than random, and for completeness sake, a model that always returns the wrong answer."
  },
  {
    "objectID": "05_Interpretability/decile_analysis.html#lift-curve",
    "href": "05_Interpretability/decile_analysis.html#lift-curve",
    "title": "Decile Analysis",
    "section": "Lift Curve",
    "text": "Lift Curve\nTo calculate the lift for a given proportion of the sample, we find the ratio of positives inside this subset and divide it by the ratio of positives inside the whole sample.\n\\[\n\\text{lift} = \\dfrac{\\dfrac{\\text{\\# positives in subset}}{\\text{\\# positives in sample}}}{\\dfrac{\\text{\\# positives in sample}}{\\text{\\# total sample}}}\n\\]\n\ndef plot_lift(y_proba, y_real, ax=None, title=\"Lift Curve\"):\n    aux_df = pd.DataFrame.from_dict({\"y_real\": y_real, \"y_proba\": y_proba})\n    aux_df = aux_df.sort_values(\"y_proba\", ascending=False)\n    # Find the total positive ratio of the whole dataset\n    total_positive_ratio = sum(aux_df[\"y_real\"] == 1) / aux_df.shape[0]\n    # For each line of data, get the ratio of positives of the given subset and calculate the lift\n    lift_values = []\n    for i in aux_df.index:\n        threshold = aux_df.loc[i][\"y_proba\"]\n        subset = aux_df[aux_df[\"y_proba\"] &gt;= threshold]\n        subset_positive_ratio = sum(subset[\"y_real\"] == 1) / subset.shape[0]\n        lift = subset_positive_ratio / total_positive_ratio\n        lift_values.append(lift)\n    if ax == None:\n        ax = plt.axes()\n    ax.set_xlabel(\"Proportion of sample\")\n    ax.set_ylabel(\"Lift\")\n    ax.set_title(title)\n    ax.set_xlim(0, 1)\n    sns.lineplot(x=[x / len(lift_values) for x in range(len(lift_values))], y=lift_values, ax=ax)\n    ax.axhline(1, linestyle=\"dashed\", linewidth=1)\n\n\n1 / 0.38\n\n2.6315789473684212\n\n\n\nfig = plt.figure(figsize=(12, 3))\ngs = fig.add_gridspec(1, 4, hspace=0.2, wspace=0.2)\naxes = gs.subplots(sharex=\"col\", sharey=\"row\")\npreds_perfect = y_test\nplot_lift(preds_perfect, y_test, ax=axes[0], title=\"Perfect Lift Curve\")\npreds_test = rf.predict_proba(X_test)[:, 1]\nplot_lift(preds_test, y_test, ax=axes[1], title=\"Random Forest Lift Curve\")\npreds_random = np.random.rand(len(y_test))\nplot_lift(preds_random, y_test, ax=axes[2], title=\"Random Lift Curve\")\npreds_opposite = y_test.apply(lambda x: not (x) * 1)\nplot_lift(preds_opposite, y_test, ax=axes[3], title=\"Opposite Lift Curve\")\nfor ax in fig.get_axes():\n    ax.label_outer()\nplt.show()\n\n\n\n\nAs 38% of values in the test set are postive the highest lift value possible is \\(\\frac{1}{0.38} = 2.63\\).\nA perfect model will have a lift value of 2.63 for the first 38% of the dataset since there are 2.63 times as many positives in the first 38% of the dataset as there are in the whole dataset (Remember the data has been sorted by the predicted probability of the model).\nA model that is no better than random will have a lift value of 1."
  },
  {
    "objectID": "05_Interpretability/decile_analysis.html#decile-analysis",
    "href": "05_Interpretability/decile_analysis.html#decile-analysis",
    "title": "Decile Analysis",
    "section": "Decile Analysis",
    "text": "Decile Analysis\nDecile analysis is a method of analyzing the performance of a model by dividing the data into 10 equal parts, or deciles. The deciles are ranked from 1 to 10, with 1 being what the model thinks is “best” and 10 being the model thinks is “worst” . The deciles are then analyzed to determine the performance of the model.\nFor instance, we may be ranking the deciles by how likely a model thinks they are to buy a product if shown and advert, or default on a loan.\nSo we would:\n\nGet the predicted probabilities from the model\nSort them in descending order\nDivide the data into 10 parts with an equal number of rows in each part\nCompute the number of actually positive cases in each decile\nFrom this it is possible to derive gain and lift values for each decile\n\n\npreds_test = rf.predict_proba(X_test)[:, 1]\n\n\ndf = pd.DataFrame({\"y_proba\": preds_test, \"y_real\": y_test})\ndf = df.sort_values(\"y_proba\", ascending=False)\ndf = df.reset_index(drop=True)\ndf[\"decile\"] = pd.qcut(df.index, 10, labels=False)\n\n\ndf\n\n\n\n\n\n\n\n\ny_proba\ny_real\ndecile\n\n\n\n\n0\n1.0\n1\n0\n\n\n1\n1.0\n1\n0\n\n\n2\n1.0\n1\n0\n\n\n3\n1.0\n1\n0\n\n\n4\n1.0\n1\n0\n\n\n...\n...\n...\n...\n\n\n323\n0.0\n0\n9\n\n\n324\n0.0\n0\n9\n\n\n325\n0.0\n0\n9\n\n\n326\n0.0\n0\n9\n\n\n327\n0.0\n0\n9\n\n\n\n\n328 rows × 3 columns\n\n\n\n\ndecile_df = pd.DataFrame()\ndecile_df[\"decile\"] = df[\"decile\"].unique()\ndecile_df[\"total\"] = df.groupby(\"decile\")[\"y_real\"].count()\ndecile_df[\"total_cum\"] = decile_df[\"total\"].cumsum()\ndecile_df[\"pct_cum\"] = decile_df[\"total_cum\"] / df.shape[0]\ndecile_df[\"positives\"] = df.groupby(\"decile\")[\"y_real\"].sum()\ndecile_df[\"pos_rate\"] = decile_df[\"positives\"] / decile_df[\"total\"]\ndecile_df[\"pos_pct\"] = decile_df[\"positives\"] / df[\"y_real\"].sum()\ndecile_df[\"gain\"] = decile_df[\"pos_pct\"].cumsum()\ndecile_df[\"lift\"] = decile_df[\"gain\"] / decile_df[\"pct_cum\"]\ndecile_df[[\"prob_min\", \"prob_max\"]] = df.groupby(\"decile\")[\"y_proba\"].agg([\"min\", \"max\"])\ndecile_df = decile_df.set_index(\"decile\")\ndecile_df = decile_df.round(2)\ndecile_df\n\n\n\n\n\n\n\n\ntotal\ntotal_cum\npct_cum\npositives\npos_rate\npos_pct\ngain\nlift\nprob_min\nprob_max\n\n\ndecile\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n33\n33\n0.1\n32\n0.97\n0.26\n0.26\n2.54\n0.95\n1.00\n\n\n1\n33\n66\n0.2\n29\n0.88\n0.23\n0.49\n2.43\n0.83\n0.95\n\n\n2\n33\n99\n0.3\n16\n0.48\n0.13\n0.62\n2.04\n0.64\n0.81\n\n\n3\n32\n131\n0.4\n15\n0.47\n0.12\n0.74\n1.84\n0.45\n0.63\n\n\n4\n33\n164\n0.5\n11\n0.33\n0.09\n0.82\n1.65\n0.28\n0.45\n\n\n5\n33\n197\n0.6\n5\n0.15\n0.04\n0.86\n1.44\n0.19\n0.28\n\n\n6\n32\n229\n0.7\n6\n0.19\n0.05\n0.91\n1.31\n0.11\n0.19\n\n\n7\n33\n262\n0.8\n4\n0.12\n0.03\n0.94\n1.18\n0.05\n0.11\n\n\n8\n33\n295\n0.9\n3\n0.09\n0.02\n0.97\n1.08\n0.02\n0.05\n\n\n9\n33\n328\n1.0\n4\n0.12\n0.03\n1.00\n1.00\n0.00\n0.02"
  },
  {
    "objectID": "05_Interpretability/decile_analysis.html#references-and-links",
    "href": "05_Interpretability/decile_analysis.html#references-and-links",
    "title": "Decile Analysis",
    "section": "References and Links",
    "text": "References and Links\n\nLoads taken from here\nNice introduction to decile analysis here\nThe scikit-plot library offers a lot of this out of the box"
  },
  {
    "objectID": "05_Interpretability/psi.html",
    "href": "05_Interpretability/psi.html",
    "title": "Population Stability Index (PSI)",
    "section": "",
    "text": "PSI is a measure of how much a population has shifted over time or between two different samples of a population in a single number. It does this by binning the two distributions and comparing the proportion of items in each bin. This results in a single number you can use to understand how different the populations are.\nfrom typing import List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom fastai.tabular.all import add_datepart, cont_cat_split, TabularPandas, Categorify, FillMissing\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0"
  },
  {
    "objectID": "05_Interpretability/psi.html#example",
    "href": "05_Interpretability/psi.html#example",
    "title": "Population Stability Index (PSI)",
    "section": "Example",
    "text": "Example\nWe’ll train a model on the Blue Book for Bulldozers dataset. The training set for this is the data from before October 2011 and the validation set is everything after that. The PSI score will help us understand if the population has shifted between the two sets.\n\ndf = pd.read_csv(f\"../data/bluebook-for-bulldozers/TrainAndValid.csv\", low_memory=False, parse_dates=[\"saledate\"])\n\n\ndf[\"SalePrice\"] = np.log(df[\"SalePrice\"])\n\n\ndf = add_datepart(df, \"saledate\", drop=False)\n\n\ndf[\"ProductSize\"] = df[\"ProductSize\"].astype(\"category\")\ndf[\"ProductSize\"] = df[\"ProductSize\"].cat.set_categories(\n    [\"Compact\", \"Mini\", \"Small\", \"Medium\", \"Large / Medium\", \"Large\"], ordered=True\n)\n\ndf[\"UsageBand\"] = df[\"UsageBand\"].astype(\"category\")\ndf[\"UsageBand\"] = df[\"UsageBand\"].cat.set_categories([\"Low\", \"Medium\", \"High\"], ordered=True)\n\n\nconts, cats = cont_cat_split(df.drop(columns=[\"saledate\"]), max_card=20, dep_var=\"SalePrice\")\n\n\nprocs = [Categorify, FillMissing]\n\n\ncond = (df[\"saleYear\"] &lt; 2011) | (df[\"saleMonth\"] &lt; 10)\ntrain_idx = np.where(cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx), list(valid_idx))\n\n\nto = TabularPandas(df.drop(columns=[\"saledate\"]), procs, cats, conts, y_names=\"SalePrice\", splits=splits)\n\n\nlen(conts), len(cats)\n\n(11, 55)\n\n\n\nm = RandomForestRegressor(n_jobs=-1)\nm.fit(np.array(to.train.xs), np.array(to.train.y))\n\nRandomForestRegressor(n_jobs=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(n_jobs=-1)\n\n\n\nprint(m.score(np.array(to.train.xs), np.array(to.train.y)))\nprint(m.score(np.array(to.valid.xs), np.array(to.valid.y)))\n\n0.9881219285666549\n0.8943878785209834\n\n\n\ntrain_preds = m.predict(np.array(to.train.xs))\nvalid_preds = m.predict(np.array(to.valid.xs))\n\n\ntrain_preds.min(), train_preds.max(), train_preds.mean()\n\n(8.490840711593627, 11.847966022491455, 10.104334967219676)"
  },
  {
    "objectID": "05_Interpretability/psi.html#psi",
    "href": "05_Interpretability/psi.html#psi",
    "title": "Population Stability Index (PSI)",
    "section": "PSI",
    "text": "PSI\nWe calculate the PSI like this:\n\nTake two distributions, the reference and the target\nCreate n_bins number of bins\n\nIf the bin_mode is fixed then the bins are created with equal width (the delta between the min and max value in each bin is the same)\nIf the bin_mode is quantile then the bins are created so an equal number of items fall into each bin\n\nNote that the bins are created for the reference distribution and then applied to the target distribution\nCalculate the proportion of items in each bin for the reference and target distributions\nAdd a small value epsilon to any proportions with a value of 0 to avoid dividing by or taking the log of 0 in PSI formula\nCalculate the PSI. The formula is:\n\\[\nPSI = \\sum_{b=1}^{B}((\\text{TargetProp($b$)} - \\text{RefProp($b$)})) \\times \\ln(\\frac{\\text{TargetProp($b$)}}{\\text{RefProp($b$)}})\n\\]\n\\(\\text{RefProp(b)}\\) = Proportionof counts within bin \\(b\\) from the reference distribution\n\\(\\text{TargetProp(b)}\\) = Proportion of counts within bin \\(b\\) from the target distribution\n\\(\\text{B}\\) = Number of bins\n\n\ndef get_population_proportions(\n    ref: np.ndarray, target: np.ndarray, n_bins: int, bin_mode: str, eps: float = 0.001\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    if bin_mode == \"fixed\":\n        ref_counts, breakpoints = np.histogram(ref, n_bins)\n    elif bin_mode == \"quantile\":\n        breakpoints = np.quantile(ref, np.arange(0, n_bins + 1) / (n_bins) * 100 / 100)\n        ref_counts = np.histogram(ref, breakpoints)[0]\n\n    target_counts = np.histogram(target, breakpoints)[0]\n\n    ref_proportions = ref_counts / ref_counts.sum()\n    target_proportions = target_counts / target_counts.sum()\n\n    ref_proportions = np.where(ref_proportions == 0, eps, ref_proportions)\n    target_proportions = np.where(target_proportions == 0, eps, target_proportions)\n\n    return ref_proportions, target_proportions\n\n\ndef psi(ref: np.ndarray, target: np.ndarray, n_bins: int, bin_mode: str, eps: float = 0.001) -&gt; float:\n    ref_proportions, target_proportions = get_population_proportions(ref, target, n_bins, bin_mode, eps)\n    psi = np.sum((target_proportions - ref_proportions) * np.log(target_proportions / ref_proportions))\n    return psi\n\n\npsi(train_preds, valid_preds, 10, \"fixed\")\n\n0.05345701915243874\n\n\n\npsi(train_preds, valid_preds, 10, \"quantile\")\n\n0.05602506051260836\n\n\nWhen it comes to interpreting the PSI score, the general rule of thumb is that:\n\nPSI &lt; 0.1: no significant population change\nPSI &lt; 0.2: moderate population change\nPSI &gt;= 0.2: significant population change\n\nIn the case of our example it looks there isn’t any significant population change between the training and validation sets."
  },
  {
    "objectID": "05_Interpretability/psi.html#visualising-psi",
    "href": "05_Interpretability/psi.html#visualising-psi",
    "title": "Population Stability Index (PSI)",
    "section": "Visualising PSI",
    "text": "Visualising PSI\nPlotting the proportions in each bin can help us see how the two distributions vary.\n\ndef plot_bin_proportions(ref: np.ndarray, target: np.ndarray, n_bins: int, bin_mode: str, eps: float = 0.001):\n    ref_proportions, target_proportions = get_population_proportions(ref, target, n_bins, bin_mode, eps)\n    fig, ax = plt.subplots(figsize=(10, 5))\n    df = pd.DataFrame({\"ref\": ref_proportions, \"target\": target_proportions, \"bin\": range(n_bins)})\n    df.plot.bar(x=\"bin\", ax=ax)\n    ax.set_title(f\"Bin Proportions ({bin_mode} binning)\")\n    ax.set_xlabel(\"Bin\")\n    ax.set_ylabel(\"Proportion\")\n    ax.legend([\"Reference\", \"Target\"])\n    plt.show()\n\n\nplot_bin_proportions(valid_preds, train_preds, 10, \"fixed\")\n\n\n\n\n\nplot_bin_proportions(valid_preds, train_preds, 10, \"quantile\")\n\n\n\n\nWe can see something similar without resorting to binning by creating a kernel density estimate (KDE) plot. This represents the data using a continuous probability density curve in one or more dimensions. To quote the Seaborn documentation:\n\n“Relative to a histogram, KDE can produce a plot that is less cluttered and more interpretable, especially when drawing multiple distributions. But it has the potential to introduce distortions if the underlying distribution is bounded or not smooth. Like a histogram, the quality of the representation also depends on the selection of good smoothing parameters.”\n\n\ndef plot_densities(ref: np.ndarray, target: np.ndarray, figsize: Tuple[int, int] = None):\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.kdeplot(ref, shade=True, label=\"Initial\", ax=ax)\n    sns.kdeplot(target, shade=True, label=\"New\", ax=ax)\n    ax.set_title(\"KDE of Predicted Probabilities\")\n    ax.set_xlabel(\"Predicted Sale Price\")\n    ax.legend([\"Reference\", \"Target\"])\n    plt.show()\n\n\nplot_densities(train_preds, valid_preds)"
  },
  {
    "objectID": "05_Interpretability/psi.html#feature-stability-importance-fsi",
    "href": "05_Interpretability/psi.html#feature-stability-importance-fsi",
    "title": "Population Stability Index (PSI)",
    "section": "Feature Stability Importance (FSI)",
    "text": "Feature Stability Importance (FSI)\nFeature (also known as Characteristic) Stability Importance (FSI) is a measure of how much a feature varies between two distributions. It is calculated by taking the PSI score for each feature and then normalising it by the PSI score for the whole dataset. This results in a single number for each feature that can be used to understand how stable the feature is.\n\ndef fsi(\n    ref_df: pd.DataFrame, target_df: pd.DataFrame, features: List[str], n_bins: int, bin_mode: str\n) -&gt; pd.DataFrame:\n    df = pd.DataFrame(columns=[\"feature\", \"fsi\"])\n    for col in features:\n        csi_values = psi(ref_df[col].values, target_df[col].values, n_bins, bin_mode)\n        csi = np.mean(csi_values)\n        df = pd.concat([df, pd.DataFrame({\"feature\": [col], \"fsi\": [csi]})], ignore_index=True)\n    df = df.sort_values(\"fsi\", ascending=False)\n    return df\n\n\nsorted_fsis = fsi(to.train.xs, to.valid.xs, to.train.xs.columns, 10, \"quantile\").sort_values(\"fsi\", ascending=False)\nsorted_fsis\n\n\n\n\n\n\n\n\nfeature\nfsi\n\n\n\n\n65\nsaleElapsed\n6.163579\n\n\n55\nSalesID\n5.589695\n\n\n0\ndatasource\n5.385627\n\n\n61\nsaleYear\n5.104866\n\n\n45\nsaleMonth\n4.977286\n\n\n...\n...\n...\n\n\n53\nauctioneerID_na\n0.000000\n\n\n51\nsaleIs_year_end\n0.000000\n\n\n50\nsaleIs_quarter_start\n0.000000\n\n\n49\nsaleIs_quarter_end\n0.000000\n\n\n52\nsaleIs_year_start\n0.000000\n\n\n\n\n66 rows × 2 columns\n\n\n\nIt’s often easy to see why some features drift a great deal between different populations. For instance anything to do with time given how we split our data. But other features may not be so obvious. Making KDE plots for each feature can help us interpret the FSI scores.\n\ndef plot_fsi(ref_df: pd.DataFrame, target_df: pd.DataFrame, features: List[str], n_cols: int = 5):\n    n_rows = int(np.ceil(len(features) / n_cols))\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n    for feature, ax in zip(features, axes.ravel()):\n        sns.kdeplot(ref_df[feature], label=\"Reference\", fill=True, ax=ax)\n        sns.kdeplot(target_df[feature], label=\"Target\", fill=True, ax=ax)\n        ax.set_title(feature)\n        ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n\nplot_fsi(to.train.xs, to.valid.xs, sorted_fsis[\"feature\"].values)\n\n/Users/henrydashwood/.pyenv/versions/3.10.3/envs/py3103/lib/python3.10/site-packages/seaborn/distributions.py:316: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n  warnings.warn(msg, UserWarning)\n/Users/henrydashwood/.pyenv/versions/3.10.3/envs/py3103/lib/python3.10/site-packages/seaborn/distributions.py:316: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n  warnings.warn(msg, UserWarning)\n/Users/henrydashwood/.pyenv/versions/3.10.3/envs/py3103/lib/python3.10/site-packages/seaborn/distributions.py:316: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n  warnings.warn(msg, UserWarning)\n/Users/henrydashwood/.pyenv/versions/3.10.3/envs/py3103/lib/python3.10/site-packages/seaborn/distributions.py:316: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n  warnings.warn(msg, UserWarning)\n/Users/henrydashwood/.pyenv/versions/3.10.3/envs/py3103/lib/python3.10/site-packages/seaborn/distributions.py:316: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n  warnings.warn(msg, UserWarning)"
  },
  {
    "objectID": "05_Interpretability/psi.html#references",
    "href": "05_Interpretability/psi.html#references",
    "title": "Population Stability Index (PSI)",
    "section": "References",
    "text": "References\nThe following resources were useful in introducing me to the concept of PSI and clarifying my thinking:\n\nIs your ML model stable? Checking model stability and population drift with PSI and CSI - Vinícius Trevisan\nPopulation Stability Index - Matthew Burke\nMeasuring Data Drift: Population Stability Index - Murtuza Shergadwala\n\nThe example model is used in the FastAI book and at least two of the Fast AI courses."
  },
  {
    "objectID": "05_Interpretability/partial_dependence.html",
    "href": "05_Interpretability/partial_dependence.html",
    "title": "Partial Dependence Plots",
    "section": "",
    "text": "Partial dependence plots show the marginal effect one or two features have on the predicted outcome of a machine learning model. They work by calculating the average prediction of the model as one feature varies, while keeping all other features constant. Here’s a basic description of how PDPs are generated:\n\nChoose a feature (or features) of interest for which you want to compute the PDP.\nFor each unique value of the chosen feature:\n\nChange all instances of that feature in your dataset to that value, leaving all other features as they are.\nMake predictions for this modified dataset and take the average of these predictions. This average prediction corresponds to the partial dependence of the chosen feature’s value.\n\nPlot the unique feature values against the corresponding average predictions.\n\nA PD plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. For example, when applied to a linear regression model, partial dependence plots always show a linear relationship.\nPDPs assume that the feature(s) being plotted are independent of the other features in the dataset. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible and this will make our plots unrealistic.\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml, load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.model_selection import train_test_split\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nROOT_DIR = Path.cwd().parent.parent\n\nFor classification where the machine learning model outputs probabilities, the partial dependence plot displays the probability for a certain class given different values for feature(s) in the dataset. An easy way to deal with multiple classes is to draw one line or plot per class.\n\ndata = load_breast_cancer()\n\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\nprint(f\"Accuracy on test data: {clf.score(X_test, y_test):.2f}\")\n\nAccuracy on test data: 0.97\n\n\n\nX_train\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n287\n12.890\n13.12\n81.89\n515.9\n0.06955\n0.03729\n0.02260\n0.01171\n0.1337\n0.05581\n0.1532\n0.4690\n1.1150\n12.68\n0.004731\n0.013450\n0.016520\n0.005905\n0.01619\n0.002081\n13.620\n15.54\n87.40\n577.0\n0.09616\n0.11470\n0.11860\n0.05366\n0.2309\n0.06915\n\n\n512\n13.400\n20.52\n88.64\n556.7\n0.11060\n0.14690\n0.14450\n0.08172\n0.2116\n0.07325\n0.3906\n0.9306\n3.0930\n33.67\n0.005414\n0.022650\n0.034520\n0.013340\n0.01705\n0.004005\n16.410\n29.66\n113.30\n844.4\n0.15740\n0.38560\n0.51060\n0.20510\n0.3585\n0.11090\n\n\n402\n12.960\n18.29\n84.18\n525.2\n0.07351\n0.07899\n0.04057\n0.01883\n0.1874\n0.05899\n0.2357\n1.2990\n2.3970\n20.21\n0.003629\n0.037130\n0.034520\n0.010650\n0.02632\n0.003705\n14.130\n24.61\n96.31\n621.9\n0.09329\n0.23180\n0.16040\n0.06608\n0.3207\n0.07247\n\n\n446\n17.750\n28.03\n117.30\n981.6\n0.09997\n0.13140\n0.16980\n0.08293\n0.1713\n0.05916\n0.3897\n1.0770\n2.8730\n43.95\n0.004714\n0.020150\n0.036970\n0.011100\n0.01237\n0.002556\n21.530\n38.54\n145.40\n1437.0\n0.14010\n0.37620\n0.63990\n0.19700\n0.2972\n0.09075\n\n\n210\n20.580\n22.14\n134.70\n1290.0\n0.09090\n0.13480\n0.16400\n0.09561\n0.1765\n0.05024\n0.8601\n1.4800\n7.0290\n111.70\n0.008124\n0.036110\n0.054890\n0.027650\n0.03176\n0.002365\n23.240\n27.84\n158.30\n1656.0\n0.11780\n0.29200\n0.38610\n0.19200\n0.2909\n0.05865\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n71\n8.888\n14.64\n58.79\n244.0\n0.09783\n0.15310\n0.08606\n0.02872\n0.1902\n0.08980\n0.5262\n0.8522\n3.1680\n25.44\n0.017210\n0.093680\n0.056710\n0.017660\n0.02541\n0.021930\n9.733\n15.67\n62.56\n284.4\n0.12070\n0.24360\n0.14340\n0.04786\n0.2254\n0.10840\n\n\n106\n11.640\n18.33\n75.17\n412.5\n0.11420\n0.10170\n0.07070\n0.03485\n0.1801\n0.06520\n0.3060\n1.6570\n2.1550\n20.62\n0.008540\n0.023100\n0.029450\n0.013980\n0.01565\n0.003840\n13.140\n29.26\n85.51\n521.7\n0.16880\n0.26600\n0.28730\n0.12180\n0.2806\n0.09097\n\n\n270\n14.290\n16.82\n90.30\n632.6\n0.06429\n0.02675\n0.00725\n0.00625\n0.1508\n0.05376\n0.1302\n0.7198\n0.8439\n10.77\n0.003492\n0.003710\n0.004826\n0.003608\n0.01536\n0.001381\n14.910\n20.65\n94.44\n684.6\n0.08567\n0.05036\n0.03866\n0.03333\n0.2458\n0.06120\n\n\n435\n13.980\n19.62\n91.12\n599.5\n0.10600\n0.11330\n0.11260\n0.06463\n0.1669\n0.06544\n0.2208\n0.9533\n1.6020\n18.85\n0.005314\n0.017910\n0.021850\n0.009567\n0.01223\n0.002846\n17.040\n30.80\n113.90\n869.3\n0.16130\n0.35680\n0.40690\n0.18270\n0.3179\n0.10550\n\n\n102\n12.180\n20.52\n77.22\n458.7\n0.08013\n0.04038\n0.02383\n0.01770\n0.1739\n0.05677\n0.1924\n1.5710\n1.1830\n14.68\n0.005080\n0.006098\n0.010690\n0.006797\n0.01447\n0.001532\n13.340\n32.84\n84.58\n547.8\n0.11230\n0.08862\n0.11450\n0.07431\n0.2694\n0.06878\n\n\n\n\n426 rows × 30 columns\n\n\n\n\nfig, ax = plt.subplots(ncols=3, nrows=10, figsize=(10, 25), constrained_layout=True)\ndisplay = PartialDependenceDisplay.from_estimator(clf, X_train, features=X_train.columns, kind=\"average\", ax=ax)\nplt.show()\n\n\n\n\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\")\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n\ncategorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)\n\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\n\nRF train accuracy: 0.972\nRF test accuracy: 0.787\n\n\n\ndisplay = PartialDependenceDisplay.from_estimator(\n    rf, X_train, features=X_train.columns, kind=\"average\", categorical_features=categorical_columns\n)\ndisplay.figure_.set_size_inches(10, 10)\ndisplay.figure_.set_constrained_layout(True)\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Notes",
    "section": "",
    "text": "Part of henrydashwood.com"
  },
  {
    "objectID": "index.html#useful-tools",
    "href": "index.html#useful-tools",
    "title": "Data Science Notes",
    "section": "Useful Tools",
    "text": "Useful Tools\n\nGeogebra\nWolfram Alha\nChat GPT"
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "Data Science Notes",
    "section": "TODO",
    "text": "TODO\n\nexpand linear regression notes\n\nlinearity\n\n\nerror term\n\n\nconfidence intervals\n\nMultiple Regression, use other columns in the autos dataset\nSomers D\nWhat is shap?\nUnbalanced data"
  },
  {
    "objectID": "index.html#toread",
    "href": "index.html#toread",
    "title": "Data Science Notes",
    "section": "TOREAD",
    "text": "TOREAD\n\nInterpretable Machine Learning\nImbalanced Classification with Python\nStatisical Rethinking\nNatural Language Processing with Transformers"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html",
    "href": "08_Decision_Optimisation/profit_curves.html",
    "title": "Profit Curves",
    "section": "",
    "text": "from pathlib import Path\nfrom typing import Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, log_loss, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBClassifier\n\npd.set_option(\"display.max_columns\", None)\n\nPROJECT_ROOT = Path.cwd().parent.parent\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#getting-the-data",
    "href": "08_Decision_Optimisation/profit_curves.html#getting-the-data",
    "title": "Profit Curves",
    "section": "Getting the data",
    "text": "Getting the data\nThe dataset is from the Telco Customer Churn Kaggle Competition. The target is the simple binary outcome in the Churn column.\n\ndata = pd.read_csv(f\"{PROJECT_ROOT}/data/WA_Fn-UseC_-Telco-Customer-Churn.csv\", low_memory=False)\n\n\ndata\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n0\n7590-VHVEG\nFemale\n0\nYes\nNo\n1\nNo\nNo phone service\nDSL\nNo\nYes\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.85\n29.85\nNo\n\n\n1\n5575-GNVDE\nMale\n0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\nNo\nYes\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\n\n\n2\n3668-QPYBK\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\nYes\nYes\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n108.15\nYes\n\n\n3\n7795-CFOCW\nMale\n0\nNo\nNo\n45\nNo\nNo phone service\nDSL\nYes\nNo\nYes\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.30\n1840.75\nNo\n\n\n4\n9237-HQITU\nFemale\n0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7038\n6840-RESVB\nMale\n0\nYes\nYes\n24\nYes\nYes\nDSL\nYes\nNo\nYes\nYes\nYes\nYes\nOne year\nYes\nMailed check\n84.80\n1990.5\nNo\n\n\n7039\n2234-XADUH\nFemale\n0\nYes\nYes\n72\nYes\nYes\nFiber optic\nNo\nYes\nYes\nNo\nYes\nYes\nOne year\nYes\nCredit card (automatic)\n103.20\n7362.9\nNo\n\n\n7040\n4801-JZAZL\nFemale\n0\nYes\nYes\n11\nNo\nNo phone service\nDSL\nYes\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.60\n346.45\nNo\n\n\n7041\n8361-LTMKD\nMale\n1\nYes\nNo\n4\nYes\nYes\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n74.40\n306.6\nYes\n\n\n7042\n3186-AJIEK\nMale\n0\nNo\nNo\n66\nYes\nNo\nFiber optic\nYes\nNo\nYes\nYes\nYes\nYes\nTwo year\nYes\nBank transfer (automatic)\n105.65\n6844.5\nNo\n\n\n\n\n7043 rows × 21 columns\n\n\n\n\ntarget = \"Churn\"\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(target, axis=1), data[target] == \"Yes\", test_size=0.2, random_state=0\n)\ncols_to_use = [\n    \"tenure\",\n    \"PhoneService\",\n    \"MultipleLines\",\n    \"InternetService\",\n    \"OnlineSecurity\",\n    \"OnlineBackup\",\n    \"DeviceProtection\",\n    \"TechSupport\",\n    \"StreamingTV\",\n    \"StreamingMovies\",\n    \"Contract\",\n    \"PaperlessBilling\",\n    \"PaymentMethod\",\n    \"MonthlyCharges\",\n]"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#training-a-model",
    "href": "08_Decision_Optimisation/profit_curves.html#training-a-model",
    "title": "Profit Curves",
    "section": "Training a model",
    "text": "Training a model\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\n            \"one_hot\",\n            OneHotEncoder(),\n            make_column_selector(dtype_include=\"object\"),\n        ),\n    ],\n    remainder=\"passthrough\",  # Leave numerical variables unchanged\n)\n\npipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", XGBClassifier())])\npipeline.fit(X_train[cols_to_use], y_train)\ny_pred = pipeline.predict_proba(X_test[cols_to_use])[:, 1]\nroc_auc = roc_auc_score(y_test, y_pred)\nlog_loss_val = log_loss(y_test, y_pred)\n\n\nprint(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred)}\")\nprint(f\"GINI: {2 * roc_auc_score(y_test, y_pred) - 1}\")\nprint(f\"Log loss: {log_loss(y_test, y_pred)}\")\n\nROC AUC Score: 0.8152565467986469\nGINI: 0.6305130935972938\nLog loss: 0.4718305882942505"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#plotting-our-predictions",
    "href": "08_Decision_Optimisation/profit_curves.html#plotting-our-predictions",
    "title": "Profit Curves",
    "section": "Plotting our predictions",
    "text": "Plotting our predictions\nWe can plot the distribution of the model’s predictions like so\n\nfig, ax = plt.subplots(figsize=(5, 3))\nax.hist(y_pred, bins=100, label=\"Predictions\")\nax.set_xlim(0, 1)\nax.set_xlabel(\"Predicted probability\")\nax.set_ylabel(\"Frequency\")\nax.set_title(\"Histogram of predicted probabilities\")\nplt.show()\n\n\n\n\nPicking a decision threshold from here is not always obvious. We can see the trade offs by plotting confusion matrices for a range of thresholds.\n\nfig, axes = plt.subplots(1, 3, figsize=(9, 3), sharey=True)\nfor i, threshold in enumerate([0.1, 0.5, 0.9]):\n    cm = confusion_matrix(y_test, (y_pred &gt; threshold).astype(int))\n    ConfusionMatrixDisplay(cm, display_labels=pipeline.classes_).plot(ax=axes[i], colorbar=False)\n    if i != 0:\n        axes[i].set_ylabel(None)\n    axes[i].set_title(f\"Threshold: {threshold}\")\nplt.show()"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#the-payoff-matrix",
    "href": "08_Decision_Optimisation/profit_curves.html#the-payoff-matrix",
    "title": "Profit Curves",
    "section": "The payoff matrix",
    "text": "The payoff matrix\nWe can use a payoff matrix to help us decide which threshold to use. This is a table that shows how much retaining a customer is worth to us, and how much it costs us to retain them.\nFor instance, say the matrix looks like the one below:\n\npayoff_matrix = np.array(\n    [\n        [0, 0],\n        [-80, 200],\n    ]\n)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nnorm = plt.Normalize(vmin=payoff_matrix.min(), vmax=payoff_matrix.max())\ncolormap = plt.cm.viridis\n\nax.imshow(payoff_matrix, cmap=\"viridis\", norm=norm)\nax.set_xticks(np.arange(2), [\"False\", \"True\"])\nax.set_xlabel(\"Would have churned\")\nax.set_yticks(np.arange(2), [\"False\", \"True\"])\nax.set_ylabel(\"Were given discount\")\nax.set_title(\"Payoff Matrix (£)\")\n\nfor i in range(payoff_matrix.shape[0]):\n    for j in range(payoff_matrix.shape[1]):\n        cell_value = payoff_matrix[i, j]\n        text_color = \"black\" if np.array(colormap(norm(cell_value))[:3]).mean() &gt; 0.5 else \"white\"\n        ax.text(j, i, cell_value, ha=\"center\", va=\"center\", color=text_color)\n\nplt.show()\n\n\n\n\nThis matrix says that retaining a customer who was going to churn by giving them a discount is woth £200 to us. It also says that giving a customer who wasn’t going to church a discount costs us £80. The payoffs for the scenarios where we don’t give a discount are set to 0.\nLet’s combine our confusion and payoff matrices to see how much deploying this model could be worth.\nIf we used the 0.5 threshold we would end up with:\n\\[\n\\begin{array}{cccc}\n& 189 & \\times & 200 & \\\\\n+ & 179 & \\times & -80 & \\\\\n\\hline\n= & 32430 & \\\\\n\\end{array}\n\\]\nSo £23,480 of value created within our test set."
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#the-profit-curve",
    "href": "08_Decision_Optimisation/profit_curves.html#the-profit-curve",
    "title": "Profit Curves",
    "section": "The profit curve",
    "text": "The profit curve\nHow do we know that the threshold we chose was the most profitable one? We can plot the profit curve to see how the profit changes as we change the threshold.\n\ndef profit_curve(y_true: np.ndarray, y_pred: np.ndarray, payoff_matrix: np.ndarray, n_points: int = 101) -&gt; Tuple:\n    \"\"\"\n    Calculate profit curve for a binary classifier.\n\n    Args:\n        y_true (array-like): True labels.\n        y_pred (array-like): Predicted probabilities.\n        payoff_matrix (array-like): Payoff matrix.\n        n_points (int): Number of points to calculate.\n\n    Returns:\n        tuple: x and y values for the profit curve.\n    \"\"\"\n\n    thresholds = np.linspace(0, 1, n_points)\n    profits = []\n    for threshold in thresholds:\n        y_pred_binary = np.where(y_pred &gt;= threshold, 1, 0)\n        cm = confusion_matrix(y_true, y_pred_binary)\n        profit = (cm * payoff_matrix.T).sum()\n        profits.append(profit)\n    return thresholds, profits\n\n\nthresholds, profits = profit_curve(y_test, y_pred, payoff_matrix)\n\n\ndef plot_profit_curve(*data, precision: int):\n    fig, ax = plt.subplots(figsize=(5, 3))\n\n    for tup in data:\n        thresholds, profits, label = tup\n\n        ax.plot(thresholds, profits, label=label)\n\n        ax.set_xlim([0, 1])\n        ax.set_xticks(np.arange(0, 1.1, 0.1))\n        ax.set_xlabel(\"Threshold\")\n\n        y_min = (min(profits) // precision) * precision\n        y_max = ((max(profits) + precision - 1) // precision) * precision\n        ax.set_ylim([y_min, y_max])\n        ax.set_yticks(np.arange(y_min, y_max + 1, precision))\n        ax.set_ylabel(\"Profit\")\n\n    ax.grid(True)\n    ax.legend(loc=\"best\")\n    ax.set_title(\"Profit Curve\")\n\n    plt.show()\n\n\nplot_profit_curve((thresholds, profits, \"Model 1\"), precision=10000)\n\n\n\n\n\ndef find_best_threshold_and_profit(thresholds, profits):\n    \"\"\"Find the best threshold and profit.\n\n    Args:\n        thresholds (array-like): Threshold values.\n        profits (array-like): Profit values.\n\n    Returns:\n        tuple: Best threshold and profit.\n    \"\"\"\n    best_profit = max(profits)\n    best_threshold = thresholds[profits.index(best_profit)]\n    return best_threshold, best_profit\n\n\nbest_thresh, best_profits = find_best_threshold_and_profit(thresholds, profits)\nprint(f\"Ideal threshold is {best_thresh:.2f} which yields profits of £{best_profits:.0f}\")\n\nIdeal threshold is 0.22 which yields profits of £32360"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#limitations",
    "href": "08_Decision_Optimisation/profit_curves.html#limitations",
    "title": "Profit Curves",
    "section": "Limitations",
    "text": "Limitations\nSome limitations of the profit curve:\n\nIt depends upon us knowing the value of true and false positive predictions. If your payoff matrix is meaningless, your profit curve will be too.\nIt assumes that all customers are worth roughly the same. This may not be true in practice.\nWe can only use it for classification problems like the “will they churn / won’t they churn” example above."
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#using-the-profit-curve-to-compare-models",
    "href": "08_Decision_Optimisation/profit_curves.html#using-the-profit-curve-to-compare-models",
    "title": "Profit Curves",
    "section": "Using the profit curve to compare models",
    "text": "Using the profit curve to compare models\nLet’s image with lose a feature so we have a new model. How will this affect our profit curve?\n\nfield_lost_in_broken_pipeline = \"Contract\"\n\nnew_cols_to_use = [col for col in cols_to_use if col != field_lost_in_broken_pipeline]\npipeline.fit(X_train[new_cols_to_use], y_train)\nnew_y_pred = pipeline.predict_proba(X_test[new_cols_to_use])[:, 1]\n\nroc_auc = roc_auc_score(y_test, new_y_pred)\nlog_loss_val = log_loss(y_test, new_y_pred)\n\n_, new_profits = profit_curve(y_test, new_y_pred, payoff_matrix)\n\n\nplot_profit_curve(\n    (thresholds, profits, \"First model\"),\n    (thresholds, new_profits, \"New model\"),\n    precision=10000,\n)\n\n\n\n\n\nnew_best_thresh, new_best_profits = find_best_threshold_and_profit(thresholds, new_profits)\nprint(f\"Ideal threshold is {new_best_thresh:.2f} which yields profits of £{new_best_profits:.0f}\")\nprint(f\"Change in profit due to lost field: £{new_best_profits - best_profits:.0f}\")\n\nIdeal threshold is 0.24 which yields profits of £30800\nChange in profit due to lost field: £-1560"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#beyond-simple-number-decision-thresholds",
    "href": "08_Decision_Optimisation/profit_curves.html#beyond-simple-number-decision-thresholds",
    "title": "Profit Curves",
    "section": "Beyond Simple-Number Decision Thresholds",
    "text": "Beyond Simple-Number Decision Thresholds\nWhen we have some customers who are worth a lot more than others, it makes sense to focus our attention and discounts on them. We may therefore want to set different thesholds for different groups of customers.\n\nfig, ax = plt.subplots(figsize=(5, 3))\ndata[\"MonthlyCharges\"].hist(ax=ax, bins=20, color=\"lightblue\")\nprecision = 10\n\nx_max = ((data[\"MonthlyCharges\"].max() + precision - 1) // precision) * precision\nax.set_xlim(0, data[\"MonthlyCharges\"].max())\nax.set_xticks(np.arange(0, x_max + 1, precision))\nax.set_xlabel(\"Monthly Charges\")\n\nax.set_ylim(0, 1200)\nax.set_yticks(np.arange(0, 1501, 250))\nax.set_ylabel(\"Count\")\n\nax.set_title(\"Distribution of Monthly Charges\")\nplt.show()\n\n\n\n\nLet’s assume that the value of keeping a customer is 3 times their monthly charge. The cost of giving a discount to someone who wasn’t going to churn can stay at £80.\nLet’s group customers in 3 groups based on the amount they pay per month. Each group will have a different payoff matrix based on the average monthly charge of the group.\n\ndef group_specific_profit_curve(\n    X_test: pd.DataFrame, y_test: pd.Series, y_pred: np.ndarray, low_thresh: float, high_thresh: float\n):\n    in_group = (X_test[\"MonthlyCharges\"] &gt; low_thresh) & (X_test[\"MonthlyCharges\"] &lt;= high_thresh)\n    y_test_in_group = y_test[in_group]\n    y_pred_in_group = y_pred[in_group]\n    mean_monthly_charge = X_test[\"MonthlyCharges\"][in_group].mean()\n    payoff_matrix = np.array(\n        [\n            [0, 0],\n            [-80, 3 * mean_monthly_charge],\n        ]\n    )\n    thresholds, profits = profit_curve(y_test_in_group, y_pred_in_group, payoff_matrix)\n    return thresholds, profits\n\n\nthresholds, low_profits = group_specific_profit_curve(X_test, y_test, y_pred, 0, 20)\n_, medium_profits = group_specific_profit_curve(X_test, y_test, y_pred, 20, 60)\n_, high_profits = group_specific_profit_curve(X_test, y_test, y_pred, 60, 100)\n_, very_high_profits = group_specific_profit_curve(X_test, y_test, y_pred, 100, 200)\n\n\ntotal_profits = np.array([low_profits, medium_profits, high_profits, very_high_profits]).sum(axis=0)\n\nWe can see from plotting the profit curve for each group that it makes sense to use different thresholds for different groups. Generally we want a lower threshold for higher value groups since the cost of losing them is greater.\nWe can also see that by doing this our total profit across all customers is higher than if we had used a single threshold for all customers.\n\nplot_profit_curve(\n    (thresholds, low_profits, \"Low customer profits\"),\n    (thresholds, medium_profits, \"Med customer profits\"),\n    (thresholds, high_profits, \"High customer profits\"),\n    (thresholds, very_high_profits, \"V high customer profits\"),\n    (thresholds, total_profits, \"Total profits\"),\n    precision=10000,\n)\n\n\n\n\n\nthresh_low, profits_low = find_best_threshold_and_profit(thresholds, low_profits)\nthresh_medium, profits_medium = find_best_threshold_and_profit(thresholds, medium_profits)\nthresh_high, profits_high = find_best_threshold_and_profit(thresholds, high_profits)\nthresh_very_high, profits_very_high = find_best_threshold_and_profit(thresholds, very_high_profits)\n\n\nprint(f\"Ideal threshold for low customers: {thresh_low}\")\nprint(f\"Ideal threshold for medium customers: {thresh_medium}\")\nprint(f\"Ideal threshold for high customers: {thresh_high}\")\nprint(f\"Ideal threshold for very high customers: {thresh_very_high}\")\n\nIdeal threshold for low customers: 0.45\nIdeal threshold for medium customers: 0.64\nIdeal threshold for high customers: 0.19\nIdeal threshold for very high customers: 0.07\n\n\n\nprint(\n    f\"\"\"Profit with a signle threshold: £{best_profits}\nProfit with flexible thresholds: £{(profits_low + profits_medium + profits_high + profits_very_high):.0f}\"\"\"\n)\n\nProfit with a signle threshold: £32360\nProfit with flexible thresholds: £44919"
  },
  {
    "objectID": "08_Decision_Optimisation/profit_curves.html#references",
    "href": "08_Decision_Optimisation/profit_curves.html#references",
    "title": "Profit Curves",
    "section": "References",
    "text": "References\n\nMachine Learning for Business Decision Optimization - Dan Becker"
  },
  {
    "objectID": "08_Decision_Optimisation/dynamic_decision_opt.html",
    "href": "08_Decision_Optimisation/dynamic_decision_opt.html",
    "title": "Simulation for Dynamic Decision Optimisation",
    "section": "",
    "text": "from pathlib import Path\n\nimport graphviz\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom torch.utils.data import Dataset, DataLoader\n\npd.set_option(\"display.max_columns\", None)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nPROJECT_ROOT = Path.cwd().parent.parent"
  },
  {
    "objectID": "08_Decision_Optimisation/dynamic_decision_opt.html#introduction",
    "href": "08_Decision_Optimisation/dynamic_decision_opt.html#introduction",
    "title": "Simulation for Dynamic Decision Optimisation",
    "section": "Introduction",
    "text": "Introduction\nIn static optimisation, the decisions you make in any given period don’t affect the decisions you need to make in a subsequent period. For instance, if the goods you stock will perish before your next order arrives you can treat the orders as being independent. However, if the items do not perish then and overstock in week 1 will affect the what the optimal amount to order in week 2 is.\nIn static optimisation you have a state. The agent is the decision maker i.e us or the code that we write. The agent takes an action i.e. placing an order with the wholesaler. The environment is what happens i.e. what the demand is in the coming week. We can then calculate a reward which in this case could be the profit we make.\n\ns = \"\"\"\nState[shape=none label=\"\"]\nAgent[shape=box]\nAction[shape=none label=\"\"]\nEnvironment[shape=box]\nReward[shape=none]\nState-&gt;Agent[label=\"State\"]\nAgent-&gt;Environment[label=\"Action\"]\nEnvironment-&gt;Reward\n\"\"\"\ngraphviz.Source(f'digraph G{{rankdir=\"LR\"; margin=0; pad=0; bgcolor=\"transparent\"; {s}}}')\n\n\n\n\nIn dynamic optimisation, the State of the world is determined in part from a path that is your Agent making decisions e.g. how much to order. The Environment e.g. the demand that week will determine the outcome and therefore the Reward. However it will also feed back into the State that we are in next time we need to make a decision, in our example by carrying forward surplus stock to the next week.\n\ns = \"\"\"\nAgent[shape=box]\nAction[shape=none label=\"\"]\nEnvironment[shape=box]\nReward[shape=none]\nAgent-&gt;Environment[label=\"Action\"]\nEnvironment-&gt;Agent[label=\"State\"]\nEnvironment-&gt;Reward\n\"\"\"\ngraphviz.Source(f'digraph G{{rankdir=\"LR\"; margin=0; pad=0; bgcolor=\"transparent\"; {s}}}')\n\n\n\n\n\nWhen to use dynamic optimisation\nSimulation and dynamic optimsiation are much more complicated than static optimisation. So when do you need it?\n\nOne shot vs sequentially connected decisions: The most important factor to consider is whether each decision is one-shot or whether the decision you make today will affect the situation you are in tommorow.\nCertain vs uncertain long-term payoffs: If you are reasonably sure what the payoffs of each decsion will be then you can use static optimisation. If not, you may need to model out the different paths a decision could lead you down.\nImmediate vs long-term payoffs: You could have situations where the decsisions are technically sequential but the pay offs are so soon that you can use a static model.\n\n\nExamples:\n\nTelecom churn prediction and reduction: This can probably be done with static optimisation. However if a person stays with you, you may want to think about whether you give them a discount next month or the month after that. The decision you make may depend upon the discounts you have or have not given them in the preceding months. In this case, you may want to introduce a dynamic element to your optimisation.\nRetail Inventory Management: As discussed elsewhere this usually requires a dynamic framing unless the stock will all perish before the next order arrives.\nHotel Pricing: This one is also best treated dynamically. If you have room available several months in the future and you don’t sell it today you could sell still sell it later possibly at a different price.\nCredit Underwriting: This could be static problem. However, if you think this could turn into a recurring business relationship with a customer you may want to introduce a dynamic element to your model."
  },
  {
    "objectID": "08_Decision_Optimisation/dynamic_decision_opt.html#data-preparation",
    "href": "08_Decision_Optimisation/dynamic_decision_opt.html#data-preparation",
    "title": "Simulation for Dynamic Decision Optimisation",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nall_data = pd.read_csv(f\"{PROJECT_ROOT}/data/grupo-bimbo-inventory-demand/train.csv\", low_memory=True)\n\nprint(all_data.shape)\nall_data.head()\n\n(74180464, 11)\n\n\n\n\n\n\n\n\n\nSemana\nAgencia_ID\nCanal_ID\nRuta_SAK\nCliente_ID\nProducto_ID\nVenta_uni_hoy\nVenta_hoy\nDev_uni_proxima\nDev_proxima\nDemanda_uni_equil\n\n\n\n\n0\n3\n1110\n7\n3301\n15766\n1212\n3\n25.14\n0\n0.0\n3\n\n\n1\n3\n1110\n7\n3301\n15766\n1216\n4\n33.52\n0\n0.0\n4\n\n\n2\n3\n1110\n7\n3301\n15766\n1238\n4\n39.32\n0\n0.0\n4\n\n\n3\n3\n1110\n7\n3301\n15766\n1240\n4\n33.52\n0\n0.0\n4\n\n\n4\n3\n1110\n7\n3301\n15766\n1242\n3\n22.92\n0\n0.0\n3\n\n\n\n\n\n\n\nThese columns represent:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nSemana\nThe week number\n\n\nAgencia_ID\nThe ID of the store\n\n\nCanal_ID\nThe ID of the sales channel\n\n\nRuta_SAK\nThe ID of the route\n\n\nCliente_ID\nThe ID of the client\n\n\nProducto_ID\nThe ID of the product\n\n\nVenta_uni_hoy\nNumber of units sold\n\n\nVenta_hoy\nSales value\n\n\nDev_uni_proxima\nNumber of units returned\n\n\nDev_proxima\nReturns value\n\n\nDemanda_uni_equil\nDemand. This is our target columm in a static model\n\n\n\nHow can we set up our data for a dynamic model? Normally in machine learning we want our training data to be from a time before our validation data.\nWe only have data from week 3 to week 9. For a project like this it is probably best to separate this into a couple of weeks of training data followed by a couple of weeks of validation data followed a long time to test how different decision making strategies play out over time. For this example we just use weeks 3 and 4 as training data and week 5 on wards as testing data.\n\nall_data[\"Semana\"].value_counts().sort_index()\n\nSemana\n3    11165207\n4    11009593\n5    10615397\n6    10191837\n7    10382849\n8    10406868\n9    10408713\nName: count, dtype: int64\n\n\n\nMIN_ML_MODEL_WEEK = 3\nMAX_ML_MODEL_WEEK = 4\nMIN_DECISION_MODEL_WEEK = 5\nMAX_DECISION_MODEL_WEEK = 9\n\nThe next thing to do is to check how much missing data we have. Each row is defined by the combination of IDs linked to it. So we can group the items by that list of IDs. We then see how in many different weeks that combination of IDs occurs.\n\nstore_product_group_cols = [\"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\", \"Producto_ID\"]\nstore_product_value_counts = all_data.groupby(store_product_group_cols).size()\n\n\nall_data.groupby(store_product_group_cols)[\"Semana\"].nunique().describe()\n\ncount    2.639665e+07\nmean     2.810223e+00\nstd      1.964561e+00\nmin      1.000000e+00\n25%      1.000000e+00\n50%      2.000000e+00\n75%      4.000000e+00\nmax      7.000000e+00\nName: Semana, dtype: float64\n\n\nMost of the combinations of IDs do not appear in every week. In a real project we would investigate why. Perhaps the data is missing. Perhaps those combinations represent products introduced or retired during the time period. Perhaps it means there was just no demand and we can fill those cases with zeros. For this example we will just drop the rows which don’t have the full 7 weeks of data.\n\nfull_filled_data = all_data.set_index(store_product_group_cols)\nfull_filled_data = full_filled_data.loc[store_product_value_counts == 7]\nfull_filled_data.reset_index(inplace=True)\n\n\nfull_filled_data.shape\n\n(17606645, 11)\n\n\nWe will now split our data by week as discussed above\n\nprediction_data = full_filled_data.query(\"Semana &gt;= @MIN_ML_MODEL_WEEK and Semana &lt;= @MAX_ML_MODEL_WEEK\")\ndecision_data = full_filled_data.query(\"Semana &gt;= @MIN_DECISION_MODEL_WEEK and Semana &lt;= @MAX_DECISION_MODEL_WEEK\")"
  },
  {
    "objectID": "08_Decision_Optimisation/dynamic_decision_opt.html#model-training",
    "href": "08_Decision_Optimisation/dynamic_decision_opt.html#model-training",
    "title": "Simulation for Dynamic Decision Optimisation",
    "section": "Model training",
    "text": "Model training\n\nSAMPLE_SIZE = 500_000\n\n\nnum_unique_vals = {col: prediction_data[col].nunique() for col in store_product_group_cols}\ndata = prediction_data.sample(SAMPLE_SIZE, random_state=0)\n\n\nX_categorical = data[store_product_group_cols].values\nX_numerical = data[[\"Venta_uni_hoy\", \"Venta_hoy\"]].values\nencoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\nX_categorical = encoder.fit_transform(X_categorical)\ny = data[\"Demanda_uni_equil\"].values\n\n\nX_train_cat, X_val_cat, y_train, y_val = train_test_split(X_categorical, y, test_size=0.2, random_state=0)\nX_train_num, X_val_num, _, _ = train_test_split(X_numerical, y, test_size=0.2, random_state=0)\n\n\nclass BimboDataset(Dataset):\n    def __init__(self, X_cat, X_num, y):\n        self.X_cat = [torch.tensor(X_cat[:, i], dtype=torch.long) for i in range(X_cat.shape[1])]\n        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        x_cat = [x[idx] for x in self.X_cat]\n        # x_cat = self.X_cat[idx]\n        x_num = self.X_num[idx]\n        y = self.y[idx]\n        return (x_cat, x_num), y\n\n\ntrain_dataset = BimboDataset(X_train_cat, X_train_num, y_train)\nval_dataset = BimboDataset(X_val_cat, X_val_num, y_val)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, categorical_cols, num_unique_vals, hidden_size=128, num_features=2):\n        super(SimpleModel, self).__init__()\n        embedding_size = 50\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(num_unique_vals[col], embedding_size) for col in categorical_cols]\n        )\n        self.num_layer = nn.Linear(num_features, embedding_size)  # define a linear layer for numerical inputs\n        self.fc1 = nn.Linear(\n            embedding_size * len(num_unique_vals) + embedding_size, hidden_size\n        )  # add embedding_size to input features of fc1 to account for the numerical inputs\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n\n    def forward(self, x_cat, x_num):\n        x_cat = [embedding(x_i.clip(0)) for x_i, embedding in zip(x_cat, self.embeddings)]\n        x_cat = torch.cat(x_cat, dim=-1)\n        x_num = torch.relu(self.num_layer(x_num))\n        x = torch.cat([x_cat, x_num], dim=-1)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x).squeeze(-1)\n        return x\n\n\ndef train_model(loss_fn, num_epochs=2):\n    model = SimpleModel(store_product_group_cols, num_unique_vals)\n    optimizer = optim.Adam(model.parameters(), lr=0.005)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for (inputs_cat, inputs_num), targets in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs_cat, inputs_num).squeeze()\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        # Validation loop\n        model.eval()\n        val_loss = 0.0\n        val_preds = []\n        val_targets = []\n        with torch.no_grad():\n            for (inputs_cat, inputs_num), targets in val_loader:\n                outputs = model(inputs_cat, inputs_num).squeeze()\n                loss = loss_fn(outputs, targets)\n                val_loss += loss.item()\n                val_preds.extend(outputs.tolist())\n                val_targets.extend(targets.tolist())\n\n        val_loss /= len(val_loader)\n        r2 = r2_score(val_targets, val_preds)\n        print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.3f} | Val loss: {val_loss:.3f} | R2: {r2:.3f}\")\n    return model\n\n\nloss = nn.MSELoss()\nmodel = train_model(loss, num_epochs=5)\n\nEpoch: 1 | Train loss: 64.210 | Val loss: 5.106 | R2: 0.996\nEpoch: 2 | Train loss: 16.711 | Val loss: 5.293 | R2: 0.996\nEpoch: 3 | Train loss: 34.655 | Val loss: 6.724 | R2: 0.994\nEpoch: 4 | Train loss: 10.297 | Val loss: 81.655 | R2: 0.933\nEpoch: 5 | Train loss: 8.200 | Val loss: 7.465 | R2: 0.994"
  },
  {
    "objectID": "08_Decision_Optimisation/dynamic_decision_opt.html#simulation-loop",
    "href": "08_Decision_Optimisation/dynamic_decision_opt.html#simulation-loop",
    "title": "Simulation for Dynamic Decision Optimisation",
    "section": "Simulation loop",
    "text": "Simulation loop\nAt a high level the heart of the simulation code is going to be a for loop. We will loop through all the periods i.e weeks. First we’ll calculate what happens in week 1. Given what happens in week 1 we will calculate what happens in week 2. Given what happens in week 2 we will calculate what happens in week 3 and so on.\nIn some cases it is necessary to see what the decisions were in period 1 to know what the inputs to the model will be in period 2. In our case the thing our model is predicting is demand, not how much we sell but how much customers want to buy. The decsisions we make around stocking will not affect how much customers want to buy (there is a caveat here around whether the measure of demand is accurate. For instance the record of historical demand may appear to have a cap at the amount of stock that the shop had in the past. We’ll set that aside for now). This means we can calculate our model’s predictions up front.\nWe’ll begin my by getting predictions for a single combination of IDs over the test period.\n\nsample_store_and_product = decision_data.query(\n    \"Agencia_ID == 1110 & Canal_ID == 7 & Ruta_SAK == 3301 & Cliente_ID == 15766 & Producto_ID == 1238\"\n)\n\n\nModel predictions\nThe following function will return our model’s predictions as a new column called predicted_demand\n\ndef add_preds_to_df(df, categorical_cols, continuous_cols):\n    categorical_for_prediction = df[categorical_cols].values\n    categorical_encoded = encoder.transform(categorical_for_prediction)\n    categorical_tensor = torch.from_numpy(categorical_encoded).long()\n    categorical_tensor = [categorical_tensor[:, i] for i in range(categorical_tensor.shape[1])]\n    numerical_tensor = torch.from_numpy(df[continuous_cols].values).float()\n    model.eval()\n    with torch.no_grad():\n        prediction = model(categorical_tensor, numerical_tensor)\n    return df.assign(predicted_demand=prediction.numpy())\n\n\nadd_preds_to_df(sample_store_and_product, store_product_group_cols, [\"Venta_uni_hoy\", \"Venta_hoy\"])\n\n\n\n\n\n\n\n\nAgencia_ID\nCanal_ID\nRuta_SAK\nCliente_ID\nProducto_ID\nSemana\nVenta_uni_hoy\nVenta_hoy\nDev_uni_proxima\nDev_proxima\nDemanda_uni_equil\npredicted_demand\n\n\n\n\n5030470\n1110\n7\n3301\n15766\n1238\n5\n1\n9.83\n0\n0.0\n1\n1.239577\n\n\n7545705\n1110\n7\n3301\n15766\n1238\n6\n2\n19.66\n0\n0.0\n2\n1.792842\n\n\n10060940\n1110\n7\n3301\n15766\n1238\n7\n2\n19.66\n0\n0.0\n2\n1.792842\n\n\n12576175\n1110\n7\n3301\n15766\n1238\n8\n3\n29.49\n0\n0.0\n3\n2.943027\n\n\n15091410\n1110\n7\n3301\n15766\n1238\n9\n2\n19.66\n0\n0.0\n2\n1.792842\n\n\n\n\n\n\n\n\n\nSimulation loop\nLet’s demonstrate how the predicted and actual demand will affect the sales, suplus, and shortage we have over a few weeks. This is the sort of thing you might want to do in a spreadsheet. However we can also do it in code. We’ll start by inventing some dummy values for predicted and actual demand.\nIn the first week of our simulation we obviously have no old stock.\nWe’ll assume a rule of “order 10% more than the forecasted demand.\nThe amount we actually sell is the minimum of the actual demand and how much we we have in stock. How much we have in stock is the sum of the old stock and the new stock we ordered.\nThe shortage is how much people want to buy minus how much we are actually able to sell them.\nThe amount spoiled will depend on the specific shelf life of whatever product or products you are modelling. It may also depend on whether you can always sell old stock before new stock. We are going to make the assumptions we can keep stock on sale for 1 week and that we can always sell old stock before new stock. So the amount spoiled will be the maximum of zero and the old stock minus actual demand. So:\n\nif actual demand was 20 and and we had 5 old stock, the amount spoiled would be 0.\nif actual demand was 2 and and we had 5 old stock, the amount spoiled would be 3.\n\nFinally leftover stock can never be less than 0. Also because if we still have unsold old stock at the end of the week it will have spoiled we will only be able to carry the new stock over to the next week. So\n\nif actual demand was 20 and and we had 5 old stock and 10 new stock, the leftover stock would be 0.\nif actual demand was 10 and and we had 5 old stock and 10 new stock, the leftover stock would be 5.\nif actual demand was 10 and and we had 15 old stock and 10 new stock, the leftover stock would be 10.\n\n\ndef create_simulation_table(actual_demand, forecasted_demand, decision_rule):\n    df = pd.DataFrame()\n    weeks = range(1, len(actual_demand) + 1)\n    for week, actual_demand, forecasted_demand in zip(weeks, actual_demand, forecasted_demand):\n        df.loc[week, \"week\"] = week\n        df.loc[week, \"actual_demand\"] = actual_demand\n        df.loc[week, \"predicted_demand\"] = forecasted_demand\n        df.loc[week, \"old_stock\"] = 0 if week == 1 else df.loc[week - 1][\"leftover_stock\"]\n        df.loc[week, \"new_stock\"] = decision_rule(df.loc[week])\n        df.loc[week, \"actually_sell\"] = min(actual_demand, df.loc[week, \"old_stock\"] + df.loc[week, \"new_stock\"])\n        df.loc[week, \"shortage\"] = actual_demand - df.loc[week, \"actually_sell\"]\n        df.loc[week, \"spoiled\"] = max(df.loc[week, \"old_stock\"] - actual_demand, 0)\n        df.loc[week, \"leftover_stock\"] = max(\n            0, min((df.loc[week, \"old_stock\"] + df.loc[week, \"new_stock\"]) - actual_demand, df.loc[week, \"new_stock\"])\n        )\n    return df\n\nHere we can see our simulation play out with some dummy data and a decision rule where we stock 10% more than the forecasted demand each week\n\ndef add_ten_percent(state):\n    return np.ceil(state[\"predicted_demand\"] * 1.1)\n\n\ncreate_simulation_table([5, 7, 3, 1, 6], [7, 3, 2, 4, 1], add_ten_percent)\n\n\n\n\n\n\n\n\nweek\nactual_demand\npredicted_demand\nold_stock\nnew_stock\nactually_sell\nshortage\nspoiled\nleftover_stock\n\n\n\n\n1\n1.0\n5.0\n7.0\n0.0\n8.0\n5.0\n0.0\n0.0\n3.0\n\n\n2\n2.0\n7.0\n3.0\n3.0\n4.0\n7.0\n0.0\n0.0\n0.0\n\n\n3\n3.0\n3.0\n2.0\n0.0\n3.0\n3.0\n0.0\n0.0\n0.0\n\n\n4\n4.0\n1.0\n4.0\n0.0\n5.0\n1.0\n0.0\n0.0\n4.0\n\n\n5\n5.0\n6.0\n1.0\n4.0\n2.0\n6.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nNow let’s apply it to the subset of data we used to test our model’s predictions\n\ndef add_simulation_to_table(df, categorical_cols, continuous_cols, decision_rule):\n    df = add_preds_to_df(df, categorical_cols, continuous_cols)\n    simulation_table = create_simulation_table(df[\"Demanda_uni_equil\"], df[\"predicted_demand\"], decision_rule)\n    simulation_cols = [\"old_stock\", \"new_stock\", \"actually_sell\", \"shortage\", \"spoiled\", \"leftover_stock\"]\n    df[simulation_cols] = simulation_table[simulation_cols].values\n    return df\n\nThe decision rule here is even simpler; we just stock one item each week\n\ndef first_decision_rule(state):\n    return 1\n\n\nadd_simulation_to_table(\n    sample_store_and_product, store_product_group_cols, [\"Venta_uni_hoy\", \"Venta_hoy\"], first_decision_rule\n)\n\n\n\n\n\n\n\n\nAgencia_ID\nCanal_ID\nRuta_SAK\nCliente_ID\nProducto_ID\nSemana\nVenta_uni_hoy\nVenta_hoy\nDev_uni_proxima\nDev_proxima\nDemanda_uni_equil\npredicted_demand\nold_stock\nnew_stock\nactually_sell\nshortage\nspoiled\nleftover_stock\n\n\n\n\n5030470\n1110\n7\n3301\n15766\n1238\n5\n1\n9.83\n0\n0.0\n1\n1.239577\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n7545705\n1110\n7\n3301\n15766\n1238\n6\n2\n19.66\n0\n0.0\n2\n1.792842\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n\n\n10060940\n1110\n7\n3301\n15766\n1238\n7\n2\n19.66\n0\n0.0\n2\n1.792842\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n\n\n12576175\n1110\n7\n3301\n15766\n1238\n8\n3\n29.49\n0\n0.0\n3\n2.943027\n0.0\n1.0\n1.0\n2.0\n0.0\n0.0\n\n\n15091410\n1110\n7\n3301\n15766\n1238\n9\n2\n19.66\n0\n0.0\n2\n1.792842\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nScaling to multiple stores\nTo make the data a more manageable for our illustrative purposes we’ll arbitrarily use the rows with an Agencia_ID of 1110 to pick our rule and the rows with an Agencia_ID of 24049 to test it.\n\ndecision_validation_data = decision_data.query(\"Agencia_ID == 1110\")\ndecision_holdout_data = decision_data.query(\"Agencia_ID == 24049\")\n\nThe next step is to apply perform the simulation on multiple stores. We’ll do this by grouping the data by Agencia_ID and then applying the simulation to each group.\nSomething to note here is the objective function. This is just an example of something you might come up with to help put a number on the combination of different outcomes that may arise as a result of different decision rules.\n\ndef objective_function(df):\n    return (\n        df[\"actually_sell\"].sum() - 3 * df[\"shortage\"].sum() - 0.5 * df[\"spoiled\"].sum() - 0.5 * df[\"old_stock\"].sum()\n    )\n\n\ndef simulate_multiple_stores_and_products(raw_data, decision_function):\n    groups = raw_data.groupby(store_product_group_cols)\n    outcomes = pd.concat(\n        [\n            add_simulation_to_table(group, store_product_group_cols, [\"Venta_uni_hoy\", \"Venta_hoy\"], decision_function)\n            for _, group in groups\n        ]\n    )\n    return {\n        \"number_of_orders\": outcomes[\"new_stock\"].count(),\n        \"total_inventory_orders\": outcomes[\"new_stock\"].sum(),\n        \"number_of_shortages\": (outcomes[\"shortage\"] &gt; 0).sum(),\n        \"total_shortage\": outcomes[\"shortage\"].sum(),\n        \"total_sold\": outcomes[\"actually_sell\"].sum(),\n        \"total_old_stock\": outcomes[\"old_stock\"].sum(),\n        \"total_spoiled\": outcomes[\"spoiled\"].sum(),\n        \"objective_function\": objective_function(outcomes),\n    }\n\nWe are also going to try out a few other possible decision rules so we can see how they may affect things. These are all a bit more realistic than just setting everything to be 1.\n\ndef predicted_need(state):\n    return np.ceil(state[\"predicted_demand\"] - state[\"old_stock\"])\n\n\ndef predicted_need_plus_one(state):\n    return predicted_need(state) + 1\n\n\ndef predicted_demand(state):\n    return np.ceil(state[\"predicted_demand\"])\n\n\noutcomes = []\nfor rule in [predicted_need, predicted_need_plus_one, predicted_demand]:\n    outcomes.append(simulate_multiple_stores_and_products(decision_validation_data, rule))\n\n\noutcomes_df = pd.DataFrame(outcomes)\noutcomes_df[\"rule\"] = [\"predicted_need\", \"predicted_need_plus_one\", \"predicted_demand\"]\noutcomes_df\n\n\n\n\n\n\n\n\nnumber_of_orders\ntotal_inventory_orders\nnumber_of_shortages\ntotal_shortage\ntotal_sold\ntotal_old_stock\ntotal_spoiled\nobjective_function\nrule\n\n\n\n\n0\n14270\n327148.0\n6068\n16361.0\n326756.0\n1355.0\n105.0\n276943.0\npredicted_need\n\n\n1\n14270\n334979.0\n3160\n10293.0\n332824.0\n7794.0\n274.0\n297911.0\npredicted_need_plus_one\n\n\n2\n14270\n328503.0\n5960\n16179.0\n326938.0\n2913.0\n343.0\n276773.0\npredicted_demand\n\n\n\n\n\n\n\nWhat are these results showing us? The predicted_demand rule simply orders however much we think we will sell. It ignores the amount we are carrying over from the previous week. It does the worst. It results in the most old stock being carried forward. This means it results in far more stock being spoiled. It also doesn’t even result in lower shortages compared to the columns based on predicted need.\nIn this case ordering our predicted need with a buffer has come out slightly ahead of predicted need in our objective function. This makes sense has that function deliberately weights shortages more than spoilages. Whether those weights have been chosen correctly will depend upon your circumstances.\n\nfig, ax = plt.subplots(3, 3, figsize=(16, 10), sharey=True)\nax = ax.flatten()\n\nax[0].barh(outcomes_df[\"rule\"], outcomes_df[\"objective_function\"])\nax[0].set_title(\"Objective function\")\nax[1].barh(outcomes_df[\"rule\"], outcomes_df[\"total_old_stock\"])\nax[1].set_title(\"Total old stock\")\nax[2].barh(outcomes_df[\"rule\"], outcomes_df[\"total_spoiled\"])\nax[2].set_title(\"Total spoiled\")\nax[3].barh(outcomes_df[\"rule\"], outcomes_df[\"number_of_shortages\"])\nax[3].set_title(\"Number of shortages\")\nax[4].barh(outcomes_df[\"rule\"], outcomes_df[\"total_shortage\"])\nax[4].set_title(\"Total shortage\")\nax[5].barh(outcomes_df[\"rule\"], outcomes_df[\"total_sold\"])\nax[5].set_title(\"Total sold\")\nax[6].barh(outcomes_df[\"rule\"], outcomes_df[\"total_inventory_orders\"])\nax[6].set_title(\"Total inventory orders\")\n\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\n\nProgrammatic Optimisation\nAt this point you may choose to programmatically optimise you decision function parameters using something like a hyperparameter optimisation library from the scikit-learn ecosystem or Weights and Bias’s sweep capabilities."
  },
  {
    "objectID": "08_Decision_Optimisation/dynamic_decision_opt.html#references",
    "href": "08_Decision_Optimisation/dynamic_decision_opt.html#references",
    "title": "Simulation for Dynamic Decision Optimisation",
    "section": "References",
    "text": "References\n\nMachine Learning for Business Decision Optimization - Dan Becker"
  },
  {
    "objectID": "07_Geospatial/interactive_folium.html",
    "href": "07_Geospatial/interactive_folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "import math\nfrom pathlib import Path\n\nimport folium\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nPROJECT_ROOT = Path.cwd().parent.parent\nWe begin by creating a relatively simple map with folium.Map().\nm_1 = folium.Map(location=[42.32, -71.0589], tiles=\"openstreetmap\", zoom_start=10)\n\nm_1\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nSeveral arguments customize the appearance of the map:"
  },
  {
    "objectID": "07_Geospatial/interactive_folium.html#boston-crime-data",
    "href": "07_Geospatial/interactive_folium.html#boston-crime-data",
    "title": "Interactive Maps with Folium",
    "section": "Boston Crime Data",
    "text": "Boston Crime Data\n\ncrimes = pd.read_csv(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/crimes-in-boston/crimes-in-boston/crime.csv\", encoding=\"latin-1\"\n)\n\n# Drop rows with missing locations\ncrimes.dropna(subset=[\"Lat\", \"Long\", \"DISTRICT\"], inplace=True)\n\n# Focus on major crimes in 2018\ncrimes = crimes[\n    crimes.OFFENSE_CODE_GROUP.isin(\n        [\n            \"Larceny\",\n            \"Auto Theft\",\n            \"Robbery\",\n            \"Larceny From Motor Vehicle\",\n            \"Residential Burglary\",\n            \"Simple Assault\",\n            \"Harassment\",\n            \"Ballistics\",\n            \"Aggravated Assault\",\n            \"Other Burglary\",\n            \"Arson\",\n            \"Commercial Burglary\",\n            \"HOME INVASION\",\n            \"Homicide\",\n            \"Criminal Harassment\",\n            \"Manslaughter\",\n        ]\n    )\n]\ncrimes = crimes[crimes.YEAR &gt;= 2018]\n\ncrimes.head()\n\n\n\n\n\n\n\n\nINCIDENT_NUMBER\nOFFENSE_CODE\nOFFENSE_CODE_GROUP\nOFFENSE_DESCRIPTION\nDISTRICT\nREPORTING_AREA\nSHOOTING\nOCCURRED_ON_DATE\nYEAR\nMONTH\nDAY_OF_WEEK\nHOUR\nUCR_PART\nSTREET\nLat\nLong\nLocation\n\n\n\n\n0\nI182070945\n619\nLarceny\nLARCENY ALL OTHERS\nD14\n808\nNaN\n2018-09-02 13:00:00\n2018\n9\nSunday\n13\nPart One\nLINCOLN ST\n42.357791\n-71.139371\n(42.35779134, -71.13937053)\n\n\n6\nI182070933\n724\nAuto Theft\nAUTO THEFT\nB2\n330\nNaN\n2018-09-03 21:25:00\n2018\n9\nMonday\n21\nPart One\nNORMANDY ST\n42.306072\n-71.082733\n(42.30607218, -71.08273260)\n\n\n8\nI182070931\n301\nRobbery\nROBBERY - STREET\nC6\n177\nNaN\n2018-09-03 20:48:00\n2018\n9\nMonday\n20\nPart One\nMASSACHUSETTS AVE\n42.331521\n-71.070853\n(42.33152148, -71.07085307)\n\n\n19\nI182070915\n614\nLarceny From Motor Vehicle\nLARCENY THEFT FROM MV - NON-ACCESSORY\nB2\n181\nNaN\n2018-09-02 18:00:00\n2018\n9\nSunday\n18\nPart One\nSHIRLEY ST\n42.325695\n-71.068168\n(42.32569490, -71.06816778)\n\n\n24\nI182070908\n522\nResidential Burglary\nBURGLARY - RESIDENTIAL - NO FORCE\nB2\n911\nNaN\n2018-09-03 18:38:00\n2018\n9\nMonday\n18\nPart One\nANNUNCIATION RD\n42.335062\n-71.093168\n(42.33506218, -71.09316781)\n\n\n\n\n\n\n\n\nPlotting points\n\ndaytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == \"Robbery\") & (crimes.HOUR.isin(range(9, 18))))]\n\n\n\nfolium.Marker\n\nm_2 = folium.Map(location=[42.32, -71.0589], tiles=\"cartodbpositron\", zoom_start=12)\n\nfor idx, row in daytime_robberies.iterrows():\n    Marker([row[\"Lat\"], row[\"Long\"]]).add_to(m_2)\n\nm_2\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nfolium.plugins.MarkerCluster\nIf we have a lot of markers to add, folium.plugins.MarkerCluster() can help to declutter the map. Each marker is added to a MarkerCluster object.\n\nm_3 = folium.Map(location=[42.32, -71.0589], tiles=\"cartodbpositron\", zoom_start=12)\n\nmc = MarkerCluster()\nfor idx, row in daytime_robberies.iterrows():\n    if not math.isnan(row[\"Long\"]) and not math.isnan(row[\"Lat\"]):\n        mc.add_child(Marker([row[\"Lat\"], row[\"Long\"]]))\nm_3.add_child(mc)\n\nm_3\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nBubble Maps\nA bubble map uses circles instead of markers. By varying the size and color of each circle, we can also show the relationship between location and two other variables.\nWe create a bubble map by using folium.Circle() to iteratively add circles. In the code cell below, robberies that occurred in hours 9-12 are plotted in green, whereas robberies from hours 13-17 are plotted in red.\n\nm_4 = folium.Map(location=[42.32, -71.0589], tiles=\"cartodbpositron\", zoom_start=12)\n\n\ndef color_producer(val):\n    if val &lt;= 12:\n        return \"forestgreen\"\n    else:\n        return \"darkred\"\n\n\nfor i in range(0, len(daytime_robberies)):\n    Circle(\n        location=[daytime_robberies.iloc[i][\"Lat\"], daytime_robberies.iloc[i][\"Long\"]],\n        radius=20,\n        color=color_producer(daytime_robberies.iloc[i][\"HOUR\"]),\n    ).add_to(m_4)\n\nm_4\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nNote that folium.Circle() takes several arguments:\n\nlocation is a list containing the center of the circle, in latitude and longitude.\nradius sets the radius of the circle.\n\nNote that in a traditional bubble map, the radius of each circle is allowed to vary. We can implement this by defining a function similar to the color_producer() function that is used to vary the color of each circle.\n\ncolor sets the color of each circle.\n\nThe color_producer() function is used to visualize the effect of the hour on robbery location.\n\n\n\n\nHeatmaps\nTo create a heatmap, we use folium.plugins.HeatMap(). This shows the density of crime in different areas of the city, where red areas have relatively more criminal incidents.\nAs we’d expect for a big city, most of the crime happens near the center.\n\nm_5 = folium.Map(location=[42.32, -71.0589], tiles=\"cartodbpositron\", zoom_start=11)\n\nHeatMap(data=crimes[[\"Lat\", \"Long\"]], radius=10).add_to(m_5)\n\nm_5\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAs you can see in the code cell above, folium.plugins.HeatMap() takes a couple of arguments:\n\ndata is a DataFrame containing the locations that we’d like to plot.\nradius controls the smoothness of the heatmap. Higher values make the heatmap look smoother (i.e., with fewer gaps).\n\n\n\nChoropleth Maps\nTo understand how crime varies by police district, we’ll create a choropleth map.\nAs a first step, we create a GeoDataFrame where each district is assigned a different row, and the geometry column contains the geographical boundaries.\n\ndistricts_full = gpd.read_file(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/Police_Districts/Police_Districts/Police_Districts.shp\"\n)\ndistricts = districts_full[[\"DISTRICT\", \"geometry\"]].set_index(\"DISTRICT\")\ndistricts.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\nDISTRICT\n\n\n\n\n\nA15\nMULTIPOLYGON (((-71.07416 42.39051, -71.07415 ...\n\n\nA7\nMULTIPOLYGON (((-70.99644 42.39557, -70.99644 ...\n\n\nA1\nPOLYGON ((-71.05200 42.36884, -71.05169 42.368...\n\n\nC6\nPOLYGON ((-71.04406 42.35403, -71.04412 42.353...\n\n\nD4\nPOLYGON ((-71.07416 42.35724, -71.07359 42.357...\n\n\n\n\n\n\n\nWe also create a Pandas Series that shows the number of crimes in each district.\n\nplot_dict = crimes.DISTRICT.value_counts()\nplot_dict.head()\n\nDISTRICT\nD4     2885\nB2     2231\nA1     2130\nC11    1899\nB3     1421\nName: count, dtype: int64\n\n\nIt’s very important that plot_dict has the same index as districts - this is how the code knows how to match the geographical boundaries with appropriate colors.\nUsing the folium.Choropleth() class, we can create a choropleth map.\n\nm_6 = folium.Map(location=[42.32, -71.0589], tiles=\"cartodbpositron\", zoom_start=11)\n\nChoropleth(\n    geo_data=districts.__geo_interface__,\n    data=plot_dict,\n    key_on=\"feature.id\",\n    fill_color=\"YlGnBu\",\n    legend_name=\"Major criminal incidents (Jan-Aug 2018)\",\n).add_to(m_6)\n\nm_6\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nNote that folium.Choropleth() takes several arguments:\n\ngeo_data is a GeoJSON FeatureCollection containing the boundaries of each geographical area.\n\nIn the code above, we convert the districts GeoDataFrame to a GeoJSON FeatureCollection with the __geo_interface__ attribute.\n\ndata is a Pandas Series containing the values that will be used to color-code each geographical area.\nkey_on will always be set to feature.id.\n\nThis refers to the fact that the GeoDataFrame used for geo_data and the Pandas Series provided in data have the same index. To understand the details, we’d have to look more closely at the structure of a GeoJSON Feature Collection (where the value corresponding to the “features” key is a list, wherein each entry is a dictionary containing an “id” key).\n\nfill_color sets the color scale.\nlegend_name labels the legend in the top right corner of the map."
  },
  {
    "objectID": "07_Geospatial/geopandas.html",
    "href": "07_Geospatial/geopandas.html",
    "title": "GeoPandas",
    "section": "",
    "text": "from pathlib import Path\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom shapely.geometry import LineString\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nPROJECT_ROOT = Path.cwd().parent.parent\nlands_data = gpd.read_file(f\"{PROJECT_ROOT}/data/kaggle_geospatial/DEC_lands/DEC_lands/DEC_lands.shp\")\n\nPOI_data = gpd.read_file(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp\"\n)\n\nroads_trails = gpd.read_file(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp\"\n)\n\ncounties = gpd.read_file(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp\"\n)\nEvery GeoDataFrame contains a special geometry column.\nWhile this column can contain a variety of different datatypes, each entry will typically be a Point, LineString, or Polygon. These three types of geometric objects have built-in attributes that you can use to quickly analyze the dataset.\nFor instance, you can get the x- and y-coordinates of a Point from the x and y attributes, respectively.\nPOI_data[\"geometry\"].head()\n\n0    POINT (505138.696 4649388.247)\n1    POINT (333481.874 4692737.800)\n2    POINT (525210.784 4833837.295)\n3    POINT (231909.125 4712818.500)\n4    POINT (193656.734 4679632.500)\nName: geometry, dtype: geometry\nPOI_data[\"geometry\"].x.head()\n\n0    505138.696169\n1    333481.873865\n2    525210.783740\n3    231909.124843\n4    193656.734407\ndtype: float64\nAnd, you can get the length of a LineString from the length attribute.\nroads_trails[\"geometry\"].head()\n\n0    LINESTRING (511367.312 4804744.000, 511442.406...\n1    LINESTRING (512806.962 4804560.806, 512796.625...\n2    MULTILINESTRING ((515866.032 4804650.001, 5158...\n3    LINESTRING (533975.501 4796126.500, 533992.812...\n4    LINESTRING (531496.700 4799892.919, 531482.375...\nName: geometry, dtype: geometry\nroads_trails[\"geometry\"].length.head()\n\n0     1577.112643\n1      945.366235\n2     3853.119292\n3    10485.458536\n4     1432.490031\ndtype: float64\nOr, you can get the area of a Polygon from the area attribute.\nlands_data[\"geometry\"].head()\n\n0    POLYGON ((486093.245 4635308.586, 486787.235 4...\n1    POLYGON ((491931.514 4637416.256, 491305.424 4...\n2    POLYGON ((486000.287 4635834.453, 485007.550 4...\n3    POLYGON ((541716.775 4675243.268, 541217.579 4...\n4    POLYGON ((583896.043 4909643.187, 583891.200 4...\nName: geometry, dtype: geometry\nlands_data[\"geometry\"].area.head()\n\n0    2.990365e+06\n1    1.143940e+06\n2    9.485476e+05\n3    1.822293e+06\n4    2.821959e+05\ndtype: float64\nThe geometry data can be displayed by calling the plot method displayed when we call the plot() method.\nwild_lands = lands_data.loc[lands_data[\"CLASS\"].isin([\"WILD FOREST\", \"WILDERNESS\"])].copy()\ncampsites = POI_data.loc[POI_data[\"ASSET\"] == \"PRIMITIVE CAMPSITE\"].copy()\ntrails = roads_trails.loc[roads_trails[\"ASSET\"] == \"FOOT TRAIL\"].copy()\nfig, ax = plt.subplots(figsize=(8, 8))\ncounties.plot(color=\"none\", edgecolor=\"grey\", zorder=3, ax=ax, label=\"County\")\nwild_lands.plot(color=\"lightgreen\", ax=ax, label=\"Wild lands\")\ncampsites.plot(color=\"maroon\", markersize=2, ax=ax, label=\"Campsites\")\ntrails.plot(color=\"black\", markersize=1, ax=ax, label=\"Trails\")\nax.set_axis_off()\nplt.show()"
  },
  {
    "objectID": "07_Geospatial/geopandas.html#coordinate-systems",
    "href": "07_Geospatial/geopandas.html#coordinate-systems",
    "title": "GeoPandas",
    "section": "Coordinate Systems",
    "text": "Coordinate Systems\nMap projections are the ways the 3D world can be represented on a 2D surface. They can’t be 100% accurate. Each projection distorts the surface of the Earth in some way, while retaining some useful property. For instance:\n\nThe equal-area projections (like “Lambert Cylindrical Equal Area”, or “Africa Albers Equal Area Conic”) preserve area. This is a good choice, if you’d like to calculate the area of a country or city, for example.\nThe equidistant projections (like “Azimuthal Equidistant projection”) preserve distance. This would be a good choice for calculating flight distance.\n\nWe use a coordinate reference system (CRS) to show how the projected points correspond to real locations on Earth.\nWhen we create a GeoDataFrame from a shapefile, the CRS is already imported for us:\n\nworld.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\nregions = gpd.read_file(f\"{PROJECT_ROOT}/data/kaggle_geospatial/ghana/ghana/Regions/Map_of_Regions_in_Ghana.shp\")\n\n\nregions\n\n\n\n\n\n\n\n\nRegion\ngeometry\n\n\n\n\n0\nAshanti\nPOLYGON ((686446.075 842986.894, 686666.193 84...\n\n\n1\nBrong Ahafo\nPOLYGON ((549970.457 968447.094, 550073.003 96...\n\n\n2\nCentral\nPOLYGON ((603176.584 695877.238, 603248.424 69...\n\n\n3\nEastern\nPOLYGON ((807307.254 797910.553, 807311.908 79...\n\n\n4\nGreater Accra\nPOLYGON ((858081.638 676424.913, 858113.115 67...\n\n\n5\nNorthern\nPOLYGON ((818287.468 1185632.455, 818268.664 1...\n\n\n6\nUpper East\nPOLYGON ((811994.328 1230449.528, 812004.699 1...\n\n\n7\nUpper West\nPOLYGON ((658854.315 1220818.656, 659057.210 1...\n\n\n8\nVolta\nPOLYGON ((899718.788 875120.098, 899564.444 87...\n\n\n9\nWestern\nPOLYGON ((490349.315 771271.143, 490530.091 77...\n\n\n\n\n\n\n\n\nregions.crs\n\n&lt;Projected CRS: EPSG:32630&gt;\nName: WGS 84 / UTM zone 30N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 6°W and 0°W, northern hemisphere between equator and 84°N, onshore and offshore. Algeria. Burkina Faso. Côte' Ivoire (Ivory Coast). Faroe Islands - offshore. France. Ghana. Gibraltar. Ireland - offshore Irish Sea. Mali. Mauritania. Morocco. Spain. United Kingdom (UK).\n- bounds: (-6.0, 0.0, 0.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 30N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nCoordinate reference systems are referenced by European Petroleum Survey Group (EPSG) codes.\nThe regions GeoDataFrame uses EPSG 32630, which is more commonly called the “Mercator” projection. This projection preserves angles (making it useful for sea navigation) and slightly distorts area.\nHowever, when creating a GeoDataFrame from a CSV file, we have to set the CRS. EPSG 4326 corresponds to coordinates in latitude and longitude.\n\nfacilities_df = pd.read_csv(f\"{PROJECT_ROOT}/data/kaggle_geospatial/ghana/ghana/health_facilities.csv\")\n\nfacilities = gpd.GeoDataFrame(\n    facilities_df,\n    geometry=gpd.points_from_xy(\n        facilities_df[\"Longitude\"],\n        facilities_df[\"Latitude\"],\n    ),\n    crs=\"EPSG:4326\",\n)\n\nfacilities\n\n\n\n\n\n\n\n\nRegion\nDistrict\nFacilityName\nType\nTown\nOwnership\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nAshanti\nOffinso North\nA.M.E Zion Clinic\nClinic\nAfrancho\nCHAG\n7.40801\n-1.96317\nPOINT (-1.96317 7.40801)\n\n\n1\nAshanti\nBekwai Municipal\nAbenkyiman Clinic\nClinic\nAnwiankwanta\nPrivate\n6.46312\n-1.58592\nPOINT (-1.58592 6.46312)\n\n\n2\nAshanti\nAdansi North\nAboabo Health Centre\nHealth Centre\nAboabo No 2\nGovernment\n6.22393\n-1.34982\nPOINT (-1.34982 6.22393)\n\n\n3\nAshanti\nAfigya-Kwabre\nAboabogya Health Centre\nHealth Centre\nAboabogya\nGovernment\n6.84177\n-1.61098\nPOINT (-1.61098 6.84177)\n\n\n4\nAshanti\nKwabre\nAboaso Health Centre\nHealth Centre\nAboaso\nGovernment\n6.84177\n-1.61098\nPOINT (-1.61098 6.84177)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3751\nWestern\nSefwi-Akontombra\nAckaakrom CHPS\nCHPS\nAckaakrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3752\nWestern\nSefwi-Akontombra\nApprutu CHPS\nCHPS\nApprutu\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3753\nWestern\nSefwi-Akontombra\nKojokrom CHPS\nCHPS\nKojokrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3754\nWestern\nSefwi-Akontombra\nYawkrom CHPS\nCHPS\nYawkrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3755\nWestern\nSefwi-Akontombra\nKofikrom CHPS\nCHPS\nKofikrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n\n\n3756 rows × 9 columns\n\n\n\nRe-projecting refers to the process of changing the CRS. This is done in GeoPandas with the to_crs() method. When plotting multiple GeoDataFrames, it’s important that they all use the same CRS. In the code cell below, we change the CRS of the facilities GeoDataFrame to match the CRS of regions before plotting it. The `to_crs()`` method modifies only the “geometry” column: all other columns are left as-is.\n\nfacilities.to_crs(epsg=32630)\n\n\n\n\n\n\n\n\nRegion\nDistrict\nFacilityName\nType\nTown\nOwnership\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nAshanti\nOffinso North\nA.M.E Zion Clinic\nClinic\nAfrancho\nCHAG\n7.40801\n-1.96317\nPOINT (614422.662 818986.851)\n\n\n1\nAshanti\nBekwai Municipal\nAbenkyiman Clinic\nClinic\nAnwiankwanta\nPrivate\n6.46312\n-1.58592\nPOINT (656373.863 714616.547)\n\n\n2\nAshanti\nAdansi North\nAboabo Health Centre\nHealth Centre\nAboabo No 2\nGovernment\n6.22393\n-1.34982\nPOINT (682573.395 688243.477)\n\n\n3\nAshanti\nAfigya-Kwabre\nAboabogya Health Centre\nHealth Centre\nAboabogya\nGovernment\n6.84177\n-1.61098\nPOINT (653484.490 756478.812)\n\n\n4\nAshanti\nKwabre\nAboaso Health Centre\nHealth Centre\nAboaso\nGovernment\n6.84177\n-1.61098\nPOINT (653484.490 756478.812)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3751\nWestern\nSefwi-Akontombra\nAckaakrom CHPS\nCHPS\nAckaakrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3752\nWestern\nSefwi-Akontombra\nApprutu CHPS\nCHPS\nApprutu\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3753\nWestern\nSefwi-Akontombra\nKojokrom CHPS\nCHPS\nKojokrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3754\nWestern\nSefwi-Akontombra\nYawkrom CHPS\nCHPS\nYawkrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n3755\nWestern\nSefwi-Akontombra\nKofikrom CHPS\nCHPS\nKofikrom\nGovernment\nNaN\nNaN\nPOINT EMPTY\n\n\n\n\n3756 rows × 9 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nregions.plot(figsize=(8, 8), color=\"none\", linestyle=\":\", edgecolor=\"black\", ax=ax)\nfacilities.to_crs(epsg=32630).plot(markersize=1, ax=ax)\nax.set_axis_off()\nplt.show()\n\n\n\n\nIn case the EPSG code is not available in GeoPandas, we can change the CRS with what’s known as the “proj4 string” of the CRS. For instance, the proj4 string to convert to latitude/longitude coordinates is as follows:\n\nregions.to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n\n\n\n\n\n\n\n\nRegion\ngeometry\n\n\n\n\n0\nAshanti\nPOLYGON ((-1.30985 7.62302, -1.30786 7.62198, ...\n\n\n1\nBrong Ahafo\nPOLYGON ((-2.54567 8.76089, -2.54473 8.76071, ...\n\n\n2\nCentral\nPOLYGON ((-2.06723 6.29473, -2.06658 6.29420, ...\n\n\n3\nEastern\nPOLYGON ((-0.21751 7.21009, -0.21747 7.20993, ...\n\n\n4\nGreater Accra\nPOLYGON ((0.23456 6.10986, 0.23484 6.10974, 0....\n\n\n5\nNorthern\nPOLYGON ((-0.09041 10.71194, -0.09061 10.70918...\n\n\n6\nUpper East\nPOLYGON ((-0.14402 11.11730, -0.14393 11.11679...\n\n\n7\nUpper West\nPOLYGON ((-1.54582 11.04027, -1.54411 11.01019...\n\n\n8\nVolta\nPOLYGON ((0.62403 7.90125, 0.62264 7.90174, 0....\n\n\n9\nWestern\nPOLYGON ((-3.08737 6.97758, -3.08573 6.97675, ...\n\n\n\n\n\n\n\n\nregions.loc[:, \"area\"] = regions[\"geometry\"].area / 10**6\n\nprint(f\"Area of Ghana: {regions['area'].sum()} square kilometers\")\nprint(\"CRS:\", regions.crs)\nregions\n\nArea of Ghana: 239584.5760055668 square kilometers\nCRS: EPSG:32630\n\n\n\n\n\n\n\n\n\nRegion\ngeometry\narea\n\n\n\n\n0\nAshanti\nPOLYGON ((686446.075 842986.894, 686666.193 84...\n24379.017777\n\n\n1\nBrong Ahafo\nPOLYGON ((549970.457 968447.094, 550073.003 96...\n40098.168231\n\n\n2\nCentral\nPOLYGON ((603176.584 695877.238, 603248.424 69...\n9665.626760\n\n\n3\nEastern\nPOLYGON ((807307.254 797910.553, 807311.908 79...\n18987.625847\n\n\n4\nGreater Accra\nPOLYGON ((858081.638 676424.913, 858113.115 67...\n3706.511145\n\n\n5\nNorthern\nPOLYGON ((818287.468 1185632.455, 818268.664 1...\n69830.576358\n\n\n6\nUpper East\nPOLYGON ((811994.328 1230449.528, 812004.699 1...\n8629.357677\n\n\n7\nUpper West\nPOLYGON ((658854.315 1220818.656, 659057.210 1...\n19022.080963\n\n\n8\nVolta\nPOLYGON ((899718.788 875120.098, 899564.444 87...\n20948.296066\n\n\n9\nWestern\nPOLYGON ((490349.315 771271.143, 490530.091 77...\n24317.315180\n\n\n\n\n\n\n\nIn the code cell above, since the CRS of the regions GeoDataFrame is set to EPSG 32630 (a “Mercator” projection), the area calculation is slightly less accurate than if we had used an equal-area projection like “Africa Albers Equal Area Conic”. But this still yields the area of Ghana as approximately 239585 square kilometers, which is not too far off from the correct answer.\n\nbirds_df = pd.read_csv(f\"{PROJECT_ROOT}/data/kaggle_geospatial/purple_martin.csv\", parse_dates=[\"timestamp\"])\nprint(f\"There are {birds_df['tag-local-identifier'].nunique()} different birds in the dataset.\")\nbirds_df\n\nThere are 11 different birds in the dataset.\n\n\n\n\n\n\n\n\n\ntimestamp\nlocation-long\nlocation-lat\ntag-local-identifier\n\n\n\n\n0\n2014-08-15 05:56:00\n-88.146014\n17.513049\n30448\n\n\n1\n2014-09-01 05:59:00\n-85.243501\n13.095782\n30448\n\n\n2\n2014-10-30 23:58:00\n-62.906089\n-7.852436\n30448\n\n\n3\n2014-11-15 04:59:00\n-61.776826\n-11.723898\n30448\n\n\n4\n2014-11-30 09:59:00\n-61.241538\n-11.612237\n30448\n\n\n...\n...\n...\n...\n...\n\n\n94\n2014-12-30 19:59:00\n-50.709645\n-9.572583\n30263\n\n\n95\n2015-01-14 23:59:00\n-49.292113\n-8.392265\n30263\n\n\n96\n2015-01-30 03:59:00\n-49.081317\n-5.413250\n30263\n\n\n97\n2015-02-14 07:59:00\n-49.081245\n-5.413251\n30263\n\n\n98\n2015-03-01 11:59:00\n-50.192297\n-5.705042\n30263\n\n\n\n\n99 rows × 4 columns\n\n\n\n\nbirds = gpd.GeoDataFrame(\n    birds_df,\n    geometry=gpd.points_from_xy(\n        birds_df[\"location-long\"],\n        birds_df[\"location-lat\"],\n    ),\n    crs=\"EPSG:4326\",\n)\n\n\nworld = gpd.read_file(f\"{PROJECT_ROOT}/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\")\namericas = world.loc[world[\"CONTINENT\"].isin([\"North America\", \"South America\"])]\namericas.head()\n\n\n\n\n\n\n\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\nADM0_A3\nGEOU_DIF\nGEOUNIT\nGU_A3\nSU_DIF\nSUBUNIT\nSU_A3\nBRK_DIFF\nNAME\nNAME_LONG\nBRK_A3\nBRK_NAME\nBRK_GROUP\nABBREV\nPOSTAL\nFORMAL_EN\nFORMAL_FR\nNAME_CIAWF\nNOTE_ADM0\nNOTE_BRK\nNAME_SORT\nNAME_ALT\nMAPCOLOR7\nMAPCOLOR8\nMAPCOLOR9\nMAPCOLOR13\nPOP_EST\nPOP_RANK\nPOP_YEAR\nGDP_MD\nGDP_YEAR\nECONOMY\nINCOME_GRP\nFIPS_10\nISO_A2\nISO_A2_EH\nISO_A3\nISO_A3_EH\nISO_N3\nISO_N3_EH\nUN_A3\nWB_A2\nWB_A3\nWOE_ID\nWOE_ID_EH\nWOE_NOTE\nADM0_ISO\nADM0_DIFF\nADM0_TLC\nADM0_A3_US\nADM0_A3_FR\nADM0_A3_RU\nADM0_A3_ES\nADM0_A3_CN\nADM0_A3_TW\nADM0_A3_IN\nADM0_A3_NP\nADM0_A3_PK\nADM0_A3_DE\nADM0_A3_GB\nADM0_A3_BR\nADM0_A3_IL\nADM0_A3_PS\nADM0_A3_SA\nADM0_A3_EG\nADM0_A3_MA\nADM0_A3_PT\nADM0_A3_AR\nADM0_A3_JP\nADM0_A3_KO\nADM0_A3_VN\nADM0_A3_TR\nADM0_A3_ID\nADM0_A3_PL\nADM0_A3_GR\nADM0_A3_IT\nADM0_A3_NL\nADM0_A3_SE\nADM0_A3_BD\nADM0_A3_UA\nADM0_A3_UN\nADM0_A3_WB\nCONTINENT\nREGION_UN\nSUBREGION\nREGION_WB\nNAME_LEN\nLONG_LEN\nABBREV_LEN\nTINY\nHOMEPART\nMIN_ZOOM\nMIN_LABEL\nMAX_LABEL\nLABEL_X\nLABEL_Y\nNE_ID\nWIKIDATAID\nNAME_AR\nNAME_BN\nNAME_DE\nNAME_EN\nNAME_ES\nNAME_FA\nNAME_FR\nNAME_EL\nNAME_HE\nNAME_HI\nNAME_HU\nNAME_ID\nNAME_IT\nNAME_JA\nNAME_KO\nNAME_NL\nNAME_PL\nNAME_PT\nNAME_RU\nNAME_SV\nNAME_TR\nNAME_UK\nNAME_UR\nNAME_VI\nNAME_ZH\nNAME_ZHT\nFCLASS_ISO\nTLC_DIFF\nFCLASS_TLC\nFCLASS_US\nFCLASS_FR\nFCLASS_RU\nFCLASS_ES\nFCLASS_CN\nFCLASS_TW\nFCLASS_IN\nFCLASS_NP\nFCLASS_PK\nFCLASS_DE\nFCLASS_GB\nFCLASS_BR\nFCLASS_IL\nFCLASS_PS\nFCLASS_SA\nFCLASS_EG\nFCLASS_MA\nFCLASS_PT\nFCLASS_AR\nFCLASS_JP\nFCLASS_KO\nFCLASS_VN\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n\n3\nAdmin-0 country\n1\n2\nCanada\nCAN\n0\n2\nSovereign country\n1\nCanada\nCAN\n0\nCanada\nCAN\n0\nCanada\nCAN\n0\nCanada\nCanada\nCAN\nCanada\nNaN\nCan.\nCA\nCanada\nNaN\nCanada\nNaN\nNaN\nCanada\nNaN\n6\n6\n2\n2\n37589262.0\n15\n2019\n1736425\n2019\n1. Developed region: G7\n1. High income: OECD\nCA\nCA\nCA\nCAN\nCAN\n124\n124\n124\nCA\nCAN\n23424775\n23424775\nExact WOE match as country\nCAN\nNaN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\nCAN\n-99\n-99\nNorth America\nAmericas\nNorthern America\nNorth America\n6\n6\n4\n-99\n1\n0.0\n1.7\n5.7\n-101.910700\n60.324287\n1159320467\nQ16\nكندا\nকানাডা\nKanada\nCanada\nCanadá\nکانادا\nCanada\nΚαναδάς\nקנדה\nकनाडा\nKanada\nKanada\nCanada\nカナダ\n캐나다\nCanada\nKanada\nCanadá\nКанада\nKanada\nKanada\nКанада\nکینیڈا\nCanada\n加拿大\n加拿大\nAdmin-0 country\nNaN\nAdmin-0 country\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n\n\n4\nAdmin-0 country\n1\n2\nUnited States of America\nUS1\n1\n2\nCountry\n1\nUnited States of America\nUSA\n0\nUnited States of America\nUSA\n0\nUnited States\nUSA\n0\nUnited States of America\nUnited States\nUSA\nUnited States\nNaN\nU.S.A.\nUS\nUnited States of America\nNaN\nUnited States\nNaN\nNaN\nUnited States of America\nNaN\n4\n5\n1\n1\n328239523.0\n17\n2019\n21433226\n2019\n1. Developed region: G7\n1. High income: OECD\nUS\nUS\nUS\nUSA\nUSA\n840\n840\n840\nUS\nUSA\n23424977\n23424977\nExact WOE match as country\nUSA\nNaN\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\nUSA\n-99\n-99\nNorth America\nAmericas\nNorthern America\nNorth America\n24\n13\n6\n-99\n1\n0.0\n1.7\n5.7\n-97.482602\n39.538479\n1159321369\nQ30\nالولايات المتحدة\nমার্কিন যুক্তরাষ্ট্র\nVereinigte Staaten\nUnited States of America\nEstados Unidos\nایالات متحده آمریکا\nÉtats-Unis\nΗνωμένες Πολιτείες Αμερικής\nארצות הברית\nसंयुक्त राज्य अमेरिका\nAmerikai Egyesült Államok\nAmerika Serikat\nStati Uniti d'America\nアメリカ合衆国\n미국\nVerenigde Staten van Amerika\nStany Zjednoczone\nEstados Unidos\nСША\nUSA\nAmerika Birleşik Devletleri\nСполучені Штати Америки\nریاستہائے متحدہ امریکا\nHoa Kỳ\n美国\n美國\nAdmin-0 country\nNaN\nAdmin-0 country\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n\n\n9\nAdmin-0 country\n1\n2\nArgentina\nARG\n0\n2\nSovereign country\n1\nArgentina\nARG\n0\nArgentina\nARG\n0\nArgentina\nARG\n0\nArgentina\nArgentina\nARG\nArgentina\nNaN\nArg.\nAR\nArgentine Republic\nNaN\nArgentina\nNaN\nNaN\nArgentina\nNaN\n3\n1\n3\n13\n44938712.0\n15\n2019\n445445\n2019\n5. Emerging region: G20\n3. Upper middle income\nAR\nAR\nAR\nARG\nARG\n032\n032\n032\nAR\nARG\n23424747\n23424747\nExact WOE match as country\nARG\nNaN\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\nARG\n-99\n-99\nSouth America\nAmericas\nSouth America\nLatin America & Caribbean\n9\n9\n4\n-99\n1\n0.0\n2.0\n7.0\n-64.173331\n-33.501159\n1159320331\nQ414\nالأرجنتين\nআর্জেন্টিনা\nArgentinien\nArgentina\nArgentina\nآرژانتین\nArgentine\nΑργεντινή\nארגנטינה\nअर्जेण्टीना\nArgentína\nArgentina\nArgentina\nアルゼンチン\n아르헨티나\nArgentinië\nArgentyna\nArgentina\nАргентина\nArgentina\nArjantin\nАргентина\nارجنٹائن\nArgentina\n阿根廷\n阿根廷\nAdmin-0 country\nNaN\nAdmin-0 country\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n\n\n10\nAdmin-0 country\n1\n2\nChile\nCHL\n0\n2\nSovereign country\n1\nChile\nCHL\n0\nChile\nCHL\n0\nChile\nCHL\n0\nChile\nChile\nCHL\nChile\nNaN\nChile\nCL\nRepublic of Chile\nNaN\nChile\nNaN\nNaN\nChile\nNaN\n5\n1\n5\n9\n18952038.0\n14\n2019\n282318\n2019\n5. Emerging region: G20\n3. Upper middle income\nCI\nCL\nCL\nCHL\nCHL\n152\n152\n152\nCL\nCHL\n23424782\n23424782\nExact WOE match as country\nCHL\nNaN\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\nCHL\n-99\n-99\nSouth America\nAmericas\nSouth America\nLatin America & Caribbean\n5\n5\n5\n-99\n1\n0.0\n1.7\n6.7\n-72.318871\n-38.151771\n1159320493\nQ298\nتشيلي\nচিলি\nChile\nChile\nChile\nشیلی\nChili\nΧιλή\nצ'ילה\nचिली\nChile\nChili\nCile\nチリ\n칠레\nChili\nChile\nChile\nЧили\nChile\nŞili\nЧилі\nچلی\nChile\n智利\n智利\nAdmin-0 country\nNaN\nAdmin-0 country\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nMULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n\n\n16\nAdmin-0 country\n1\n5\nHaiti\nHTI\n0\n2\nSovereign country\n1\nHaiti\nHTI\n0\nHaiti\nHTI\n0\nHaiti\nHTI\n0\nHaiti\nHaiti\nHTI\nHaiti\nNaN\nHaiti\nHT\nRepublic of Haiti\nNaN\nHaiti\nNaN\nNaN\nHaiti\nNaN\n2\n1\n7\n2\n11263077.0\n14\n2019\n14332\n2019\n7. Least developed region\n5. Low income\nHA\nHT\nHT\nHTI\nHTI\n332\n332\n332\nHT\nHTI\n23424839\n23424839\nExact WOE match as country\nHTI\nNaN\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\nHTI\n-99\n-99\nNorth America\nAmericas\nCaribbean\nLatin America & Caribbean\n5\n5\n5\n-99\n1\n0.0\n4.0\n9.0\n-72.224051\n19.263784\n1159320839\nQ790\nهايتي\nহাইতি\nHaiti\nHaiti\nHaití\nهائیتی\nHaïti\nΑϊτή\nהאיטי\nहैती\nHaiti\nHaiti\nHaiti\nハイチ\n아이티\nHaïti\nHaiti\nHaiti\nРеспублика Гаити\nHaiti\nHaiti\nГаїті\nہیٹی\nHaiti\n海地\n海地\nAdmin-0 country\nNaN\nAdmin-0 country\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nPOLYGON ((-71.71236 19.71446, -71.62487 19.169...\n\n\n\n\n\n\n\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")[\"geometry\"].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry, crs=\"EPSG:4326\")\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")[\"geometry\"].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry, crs=\"EPSG:4326\")\n\nend_df = birds.groupby(\"tag-local-identifier\")[\"geometry\"].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry, crs=\"EPSG:4326\")\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\n\n\n\n\n\n\n\ntag-local-identifier\ngeometry\n\n\n\n\n0\n30048\nPOINT (-90.12992 20.73242)\n\n\n1\n30054\nPOINT (-93.60861 46.50563)\n\n\n2\n30198\nPOINT (-80.31036 25.92545)\n\n\n3\n30263\nPOINT (-76.78146 42.99209)\n\n\n4\n30275\nPOINT (-76.78213 42.99207)\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\namericas.plot(color=\"none\", ax=ax)\nstart_gdf.plot(ax=ax, cmap=\"tab20b\", markersize=30)\npath_gdf.plot(ax=ax, cmap=\"tab20b\", linestyle=\"-\", linewidth=1, zorder=1)\nend_gdf.plot(ax=ax, cmap=\"tab20b\", markersize=30)\nax.set_axis_off()\nplt.show()\n\n\n\n\n\nsouth_america = world.loc[world[\"CONTINENT\"] == \"South America\"]\n\n\ntotal_area = south_america[\"geometry\"].to_crs(epsg=3035).area.sum() / 10**6\nprint(total_area)\n\n17759005.815061226\n\n\n\nprotected_areas = gpd.read_file(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\"\n)\n\n\nprotected_area = sum(protected_areas[\"REP_AREA\"] - protected_areas[\"REP_M_AREA\"])\nprint(protected_area)\n\n5396761.9116883585\n\n\n\npercentage_protected = protected_area / total_area\nprint(round(percentage_protected * 100, 2))\n\n30.39\n\n\n\nprotected_land_areas = protected_areas[protected_areas[\"MARINE\"] != \"2\"]\n\nfig, ax = plt.subplots(figsize=(8, 8))\nsouth_america.plot(color=\"none\", ax=ax)\nprotected_land_areas.plot(ax=ax, color=\"green\", alpha=0.5)\nbirds[birds[\"geometry\"].y &lt; 0].plot(ax=ax, color=\"red\", markersize=10)\nax.set_axis_off()\nplt.show()"
  },
  {
    "objectID": "Random Forests/random_forest.html",
    "href": "Random Forests/random_forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "What is a random forest?\nThe curse of dimensionality is a largely meaningless concept in machine learning. The idea is having more columns creates a space that is more “empty” because the more dimensions you have the likelier it is that a point sits on the edge of a particular dimension. In theory this means that the distance between points is less meaningful in higher dimensions. This isn’t actually a problem. Points do still have meaniful distances across other dimensions so you can still say one point is more or less similar to another. So e.g. K nearest neighbours still works fine in high dimensions. The curse of dimensionality is a problem in statistics, but not in machine learning.\nIn fact when doing feature engineering for machine learning you should add columns if they contain any information that could be useful to your model."
  },
  {
    "objectID": "Random Forests/random_forest.html#implementation",
    "href": "Random Forests/random_forest.html#implementation",
    "title": "Random Forest",
    "section": "Implementation",
    "text": "Implementation\n\nimport math\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular.all import add_datepart, cont_cat_split, Categorify, FillMissing, TabularPandas\nfrom sklearn import set_config\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nimport graphviz\nimport IPython\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import export_graphviz\n\n\npd.set_option(\"display.max_columns\", None)\n# np.random.seed(42)\nset_config(transform_output=\"pandas\")\n\n\ndef rmse(x, y):\n    return math.sqrt(((x - y) ** 2).mean())\n\n\ndf = pd.read_csv(f\"../data/bluebook-for-bulldozers/TrainAndValid.csv\", low_memory=False, parse_dates=[\"saledate\"])\ndf[\"SalePrice\"] = np.log(df[\"SalePrice\"])\n\n\nidxs = sorted(np.random.permutation(len(df)))\ndf = df.iloc[idxs].copy()\n\n\ncond = df[\"saledate\"] &lt; \"2011-10-01\"\ntrain_idx = np.where(cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx), list(valid_idx))\n\n\ndf[\"ProductSize\"] = df[\"ProductSize\"].astype(\"category\")\ndf[\"ProductSize\"] = df[\"ProductSize\"].cat.set_categories(\n    [\"Compact\", \"Mini\", \"Small\", \"Medium\", \"Large / Medium\", \"Large\"], ordered=True\n)\n\ndf[\"UsageBand\"] = df[\"UsageBand\"].astype(\"category\")\ndf[\"UsageBand\"] = df[\"UsageBand\"].cat.set_categories([\"Low\", \"Medium\", \"High\"], ordered=True)\n\ndf[\"datasource\"] = df[\"datasource\"].astype(\"category\")\n\n\nFastAI Data Pipeline\n\ndef print_score(m, X_train, y_train, X_valid, y_valid):\n    res = {\n        \"Train RMSE\": rmse(m.predict(X_train), y_train),\n        \"Valid RMSE\": rmse(m.predict(X_valid), y_valid),\n        \"Train r^2\": m.score(X_train, y_train),\n        \"Valid r^2\": m.score(X_valid, y_valid),\n    }\n    print(m.oob_score_)\n    if hasattr(m, \"oob_score_\"):\n        res[\"OOB score\"] = m.oob_score_\n    print(res)\n\n\nfa_df = df.copy()\n\n\nfa_df = add_datepart(fa_df, \"saledate\", drop=True)\n\n\nconts, cats = cont_cat_split(fa_df, max_card=3, dep_var=\"SalePrice\")\n\nCategorical variables are made up of discrete levels, such as gender or product type for which addition and multiplication don’t have meaning (even if they’re stored as numbers). To use them in the model though we need to convert them to numbers.\n\nprocs = [Categorify, FillMissing]\n\n\nto = TabularPandas(fa_df, procs, cats, conts, y_names=\"SalePrice\", splits=splits)\n\n\nrf_to = RandomForestRegressor(\n    # random_state=42,\n    n_jobs=-1,\n    n_estimators=100,\n    max_features=0.5,\n    min_samples_leaf=3,\n    bootstrap=True,\n    oob_score=True,\n)\nrf_to.fit(to.train.xs, to.train.y)\n\nprint_score(rf_to, to.train.xs, to.train.y, to.valid.xs, to.valid.y)\n\n0.9158067174396045\n{'Train RMSE': 0.11742213340649484, 'Valid RMSE': 0.23679730877164026, 'Train r^2': 0.9712785128641945, 'Valid r^2': 0.8949445622661437, 'OOB score': 0.9158067174396045}\n\n\n\n\nScikit-Learn data pipeline\n\ndef print_score(m, X_train, y_train, X_valid, y_valid):\n    res = {\n        \"Train RMSE\": rmse(m.predict(X_train), y_train),\n        \"Valid RMSE\": rmse(m.predict(X_valid), y_valid),\n        \"Train r^2\": m.score(X_train, y_train),\n        \"Valid r^2\": m.score(X_valid, y_valid),\n    }\n    if hasattr(m[\"classifier\"], \"oob_score_\"):\n        res[\"OOB score\"] = m[\"classifier\"].oob_score_\n    print(res)\n\n\nsk_df = df.copy()\n\n\ndef add_datepart(df: pd.DataFrame, column_name: str, drop: bool = False) -&gt; pd.DataFrame:\n    prefix = re.sub(\"[Dd]ate$\", \"\", column_name)\n    attr = [\n        \"Year\",\n        \"Month\",\n        \"Week\",\n        \"Day\",\n        \"Dayofweek\",\n        \"Dayofyear\",\n        \"Is_month_end\",\n        \"Is_month_start\",\n        \"Is_quarter_end\",\n        \"Is_quarter_start\",\n        \"Is_year_end\",\n        \"Is_year_start\",\n    ]\n    col = df[column_name]\n    week = col.dt.isocalendar().week.astype(col.dt.day.dtype) if hasattr(col.dt, \"isocalendar\") else col.dt.week\n    for n in attr:\n        df[f\"{prefix}{n}\"] = getattr(col.dt, n.lower()) if n != \"Week\" else week\n    df[prefix + \"Elapsed\"] = np.where(~col.isna(), col.values.astype(np.int64) // 10**9, np.nan)\n    if drop:\n        df = df.drop(column_name, axis=1)\n    return df\n\n\ndef cont_cat_split(df, max_card=2, dep_var=None):\n    \"Helper function that returns column names of cont and cat variables from given `df`.\"\n    cont_names, cat_names = [], []\n    for label in df:\n        if label == dep_var:\n            continue\n        if (\n            pd.api.types.is_integer_dtype(df[label].dtype) and df[label].unique().shape[0] &gt; max_card\n        ) or pd.api.types.is_float_dtype(df[label].dtype):\n            cont_names.append(label)\n        else:\n            cat_names.append(label)\n    return cont_names, cat_names\n\n\nsk_df = add_datepart(sk_df, \"saledate\", drop=True)\n\n\nconts, cats = cont_cat_split(sk_df, max_card=3, dep_var=\"SalePrice\")\n\n\nsk_df_train = sk_df.iloc[train_idx].copy()\nsk_df_valid = sk_df.iloc[valid_idx].copy()\n\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1), cats),\n        (\"num\", SimpleImputer(strategy=\"mean\", add_indicator=True), conts),\n    ],\n    verbose_feature_names_out=False,\n)\n\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestRegressor(random_state=42, n_jobs=-1)),\n    ]\n)\nrf.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train.drop(\"SalePrice\", axis=1),\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid.drop(\"SalePrice\", axis=1),\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 0.07564979583541039, 'Valid RMSE': 0.2501823172602292, 'Train r^2': 0.9880787299789497, 'Valid r^2': 0.8827323440727398}"
  },
  {
    "objectID": "Random Forests/random_forest.html#visualising-the-tree",
    "href": "Random Forests/random_forest.html#visualising-the-tree",
    "title": "Random Forest",
    "section": "Visualising the tree",
    "text": "Visualising the tree\n\ndrawable_rf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestRegressor(n_jobs=-1, n_estimators=1, max_depth=3, bootstrap=False)),\n    ]\n)\ndrawable_rf.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['datasource', 'UsageBand',\n                                                   'fiModelDesc', 'fiBaseModel',\n                                                   'fiSecondaryDesc',\n                                                   'fiModelSeries',\n                                                   'fiModelDescriptor',\n                                                   'ProductSize',\n                                                   'fiProductClassDesc',\n                                                   'state', 'ProductGroup',\n                                                   'ProductGroupDesc',\n                                                   'Dr...\n                                                   'Tip_Control', 'Tire_Size', ...]),\n                                                 ('num',\n                                                  SimpleImputer(add_indicator=True),\n                                                  ['SalesID', 'MachineID',\n                                                   'ModelID', 'auctioneerID',\n                                                   'YearMade',\n                                                   'MachineHoursCurrentMeter',\n                                                   'saleYear', 'saleMonth',\n                                                   'saleWeek', 'saleDay',\n                                                   'saleDayofweek',\n                                                   'saleDayofyear',\n                                                   'saleElapsed'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestRegressor(bootstrap=False, max_depth=3,\n                                       n_estimators=1, n_jobs=-1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['datasource', 'UsageBand',\n                                                   'fiModelDesc', 'fiBaseModel',\n                                                   'fiSecondaryDesc',\n                                                   'fiModelSeries',\n                                                   'fiModelDescriptor',\n                                                   'ProductSize',\n                                                   'fiProductClassDesc',\n                                                   'state', 'ProductGroup',\n                                                   'ProductGroupDesc',\n                                                   'Dr...\n                                                   'Tip_Control', 'Tire_Size', ...]),\n                                                 ('num',\n                                                  SimpleImputer(add_indicator=True),\n                                                  ['SalesID', 'MachineID',\n                                                   'ModelID', 'auctioneerID',\n                                                   'YearMade',\n                                                   'MachineHoursCurrentMeter',\n                                                   'saleYear', 'saleMonth',\n                                                   'saleWeek', 'saleDay',\n                                                   'saleDayofweek',\n                                                   'saleDayofyear',\n                                                   'saleElapsed'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestRegressor(bootstrap=False, max_depth=3,\n                                       n_estimators=1, n_jobs=-1))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['datasource', 'UsageBand', 'fiModelDesc',\n                                  'fiBaseModel', 'fiSecondaryDesc',\n                                  'fiModelSeries', 'fiModelDescriptor',\n                                  'ProductSize', 'fiProductClassDesc', 'state',\n                                  'ProductGroup', 'ProductGroupDesc',\n                                  'Drive_System', 'Enclosure', 'Forks...\n                                  'Enclosure_Type', 'Engine_Horsepower',\n                                  'Hydraulics', 'Pushblock', 'Ripper',\n                                  'Scarifier', 'Tip_Control', 'Tire_Size', ...]),\n                                ('num', SimpleImputer(add_indicator=True),\n                                 ['SalesID', 'MachineID', 'ModelID',\n                                  'auctioneerID', 'YearMade',\n                                  'MachineHoursCurrentMeter', 'saleYear',\n                                  'saleMonth', 'saleWeek', 'saleDay',\n                                  'saleDayofweek', 'saleDayofyear',\n                                  'saleElapsed'])],\n                  verbose_feature_names_out=False)cat['datasource', 'UsageBand', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor', 'ProductSize', 'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc', 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension', 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size', 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls', 'Differential_Type', 'Steering_Controls', 'saleIs_month_end', 'saleIs_month_start', 'saleIs_quarter_end', 'saleIs_quarter_start', 'saleIs_year_end', 'saleIs_year_start']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['SalesID', 'MachineID', 'ModelID', 'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'saleYear', 'saleMonth', 'saleWeek', 'saleDay', 'saleDayofweek', 'saleDayofyear', 'saleElapsed']SimpleImputerSimpleImputer(add_indicator=True)RandomForestRegressorRandomForestRegressor(bootstrap=False, max_depth=3, n_estimators=1, n_jobs=-1)\n\n\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    s = export_graphviz(\n        t,\n        out_file=None,\n        feature_names=df.columns,\n        filled=True,\n        special_characters=True,\n        # max_depth=3,\n        rotate=True,\n        precision=precision,\n    )\n    IPython.display.display(graphviz.Source(re.sub(\"Tree {\", f\"Tree {{ size={size}; ratio={ratio}\", s)))\n\nA decision tree consists of a sequence of binary decisions (or “splits”). We have 28588 samples at the beginning. The average price is 10.098. If we simply predicted the average price for everything the mean squared error would be 0.479.\nThe split that reduces the error by the greatest amount is Coupler_System &lt;= -0.5. Doing this split reduces the error to 0.113 in the group of 3140 rows where Coupler_System &lt;= -0.5 and to 0.412 in the group of 25448 rows where Coupler_System &gt; -0.5.\nThe model determines how good a split is by taking the average mse of the two groups created by the split, weighted by the number of rows in each group. The average mse of the two groups is 0.113 _ 3140/28588 + 0.412 _ 25448/28588 = 0.379. This is the error that would be achieved if we used this split to predict the price of every row in the data set.\nHow many possible splits does the model have to test? Boolean columns have only possible values. So the model would only have to test boolean_col &lt;= 0.5. For columns with more unique values, the model would also try splits between each of those values.\nThe model will continue to split the a group until it only has 1 value in it unless declare a minimum group size that is greater than this. In scikit-learn this is the min_samples_leaf parameter. We could also explicitly set a maximum number of splits using the max_depth parameter.\n\ndraw_tree(\n    drawable_rf[\"classifier\"].estimators_[0],\n    preprocessing.transform(sk_df_valid.drop(\"SalePrice\", axis=1)),\n    precision=3,\n)\n\n\n\n\nIf we do not set a maximum depth or minimum leaf size, the model will continue to split until it has 1 value in each leaf. Because at that point it is able to perfectly predict the value of every row in the training data, the error will be 0. At this point the decision tree will almost certainly have overfit to the training data and will not generalise well to new data.\n\noverfit_decision_tree = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestRegressor(random_state=42, n_jobs=-1, n_estimators=1, bootstrap=False)),\n    ]\n)\noverfit_decision_tree.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nprint_score(\n    overfit_decision_tree,\n    sk_df_train.drop(\"SalePrice\", axis=1),\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid.drop(\"SalePrice\", axis=1),\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 9.883335585478044e-17, 'Valid RMSE': 0.37889885300613685, 'Train r^2': 1.0, 'Valid r^2': 0.7310251935047887}\n\n\nWe would like to allow the decision tree to grow so they can capture lots of interactions but we don’t want them to overfit. This is the motivation behind random forests which are an ensemble of decision trees.\nEach decision tree in a random forest is somewhat predictive. It is also only slightly correlated with the other trees in the forest. The reason the trees are only slightly correlated with each other is because each tree is trained on a random sub sample of the data. Each decision tree can be allowed to overfit on its sub sample because its errors will not be correlated with the errors of the other trees. After averaging out the errors we will be left with the true relationship between the features and the target.\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestRegressor(n_jobs=-1, n_estimators=20, bootstrap=False)),\n    ]\n)\nrf.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train.drop(\"SalePrice\", axis=1),\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid.drop(\"SalePrice\", axis=1),\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 2.7275800492681635e-15, 'Valid RMSE': 0.35154863525428565, 'Train r^2': 1.0, 'Valid r^2': 0.7684547611873922}\n\n\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", ExtraTreesRegressor(n_jobs=-1, n_estimators=30, bootstrap=False)),\n    ]\n)\nrf.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train.drop(\"SalePrice\", axis=1),\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid.drop(\"SalePrice\", axis=1),\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 3.976678340860726e-15, 'Valid RMSE': 0.2508125072507893, 'Train r^2': 1.0, 'Valid r^2': 0.882140823623702}"
  },
  {
    "objectID": "Random Forests/random_forest.html#out-of-bag-oob-score",
    "href": "Random Forests/random_forest.html#out-of-bag-oob-score",
    "title": "Random Forest",
    "section": "Out of Bag (OOB) Score",
    "text": "Out of Bag (OOB) Score\nIt is possible to get an estimate of the generalisation error of a random forest without using a validation set. The OOB score is the average error of the predictions made by each tree on the rows that were not used to train that tree.\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestRegressor(n_jobs=-1, n_estimators=100, bootstrap=True, oob_score=True)),\n    ]\n)\nrf.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train.drop(\"SalePrice\", axis=1),\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid.drop(\"SalePrice\", axis=1),\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 0.07568053841287195, 'Valid RMSE': 0.2502461954917144, 'Train r^2': 0.9880690388745583, 'Valid r^2': 0.8826724532950042, 'OOB score': 0.9132759057123575}\n\n\nIn this case the OOB score is less than the score on the training set but better than the score for the out of time validation set which is what we’d expect."
  },
  {
    "objectID": "Random Forests/random_forest.html#hyperparameter-tuning",
    "href": "Random Forests/random_forest.html#hyperparameter-tuning",
    "title": "Random Forest",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\nNumber of Estimators\nA very easy hyperparameter to set. This is the number of decision trees in the random forest. Adding more will never make the model worse. It will just make it take longer to train. As you add more the marginal improvement will eventually get close 0. It’s possible to plot this so just use as many estimators as it takes before the curve flattens out.\n\npreprocessing.transform(sk_df_valid.drop(\"SalePrice\", axis=1))\n\n\n\n\n\n\n\n\ndatasource\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nsaleIs_month_end\nsaleIs_month_start\nsaleIs_quarter_end\nsaleIs_quarter_start\nsaleIs_year_end\nsaleIs_year_start\nSalesID\nMachineID\nModelID\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nsaleYear\nsaleMonth\nsaleWeek\nsaleDay\nsaleDayofweek\nsaleDayofyear\nsaleElapsed\nmissingindicator_auctioneerID\nmissingindicator_MachineHoursCurrentMeter\n\n\n\n\n22915\n0.0\n1.0\n2265.0\n702.0\n40.0\n-1.0\n-1.0\n3.0\n47.0\n22.0\n4.0\n4.0\n-1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n5.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n-1.0\n1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n4.0\n4.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1218822.0\n1024652.0\n4124.0\n3.0\n1000.0\n6240.000000\n2011.0\n10.0\n40.0\n3.0\n0.0\n276.0\n1.317600e+09\n0.0\n0.0\n\n\n22916\n0.0\n2.0\n272.0\n99.0\n47.0\n-1.0\n16.0\n-1.0\n1.0\n47.0\n0.0\n0.0\n1.0\n5.0\n0.0\n1.0\n0.0\n0.0\n7.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1218823.0\n1067601.0\n24808.0\n3.0\n2006.0\n1802.000000\n2011.0\n10.0\n40.0\n6.0\n3.0\n279.0\n1.317859e+09\n0.0\n0.0\n\n\n22917\n0.0\n2.0\n4418.0\n1663.0\n-1.0\n-1.0\n-1.0\n-1.0\n40.0\n47.0\n2.0\n2.0\n-1.0\n5.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n3.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n1.0\n1.0\n1.0\n2.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1218824.0\n1010582.0\n9580.0\n3.0\n2006.0\n1926.000000\n2011.0\n10.0\n40.0\n6.0\n3.0\n279.0\n1.317859e+09\n0.0\n0.0\n\n\n22918\n0.0\n1.0\n4127.0\n1480.0\n-1.0\n-1.0\n-1.0\n-1.0\n43.0\n0.0\n2.0\n2.0\n-1.0\n5.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n3.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n1.0\n1.0\n1.0\n2.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1218825.0\n1026609.0\n6270.0\n3.0\n2003.0\n1340.000000\n2011.0\n10.0\n40.0\n6.0\n3.0\n279.0\n1.317859e+09\n0.0\n0.0\n\n\n22919\n0.0\n2.0\n4548.0\n1750.0\n20.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n5.0\n0.0\n1.0\n0.0\n0.0\n7.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1218826.0\n1032638.0\n17976.0\n3.0\n2007.0\n2297.000000\n2011.0\n10.0\n40.0\n6.0\n3.0\n279.0\n1.317859e+09\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n412693\n3.0\n-1.0\n478.0\n157.0\n105.0\n-1.0\n-1.0\n4.0\n12.0\n43.0\n3.0\n3.0\n-1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n11.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n2.0\n-1.0\n-1.0\n-1.0\n1.0\n18.0\n28.0\n2.0\n1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6333344.0\n1919201.0\n21435.0\n2.0\n2005.0\n3401.025748\n2012.0\n3.0\n10.0\n7.0\n2.0\n67.0\n1.331078e+09\n0.0\n1.0\n\n\n412694\n3.0\n-1.0\n479.0\n157.0\n105.0\n62.0\n-1.0\n4.0\n16.0\n8.0\n3.0\n3.0\n-1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n3.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n2.0\n-1.0\n-1.0\n-1.0\n1.0\n18.0\n28.0\n2.0\n1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6333345.0\n1882122.0\n21436.0\n2.0\n2005.0\n3401.025748\n2012.0\n1.0\n4.0\n28.0\n5.0\n28.0\n1.327709e+09\n0.0\n1.0\n\n\n412695\n3.0\n-1.0\n478.0\n157.0\n105.0\n-1.0\n-1.0\n4.0\n12.0\n8.0\n3.0\n3.0\n-1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n3.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n2.0\n-1.0\n-1.0\n-1.0\n0.0\n18.0\n28.0\n2.0\n1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6333347.0\n1944213.0\n21435.0\n2.0\n2005.0\n3401.025748\n2012.0\n1.0\n4.0\n28.0\n5.0\n28.0\n1.327709e+09\n0.0\n1.0\n\n\n412696\n3.0\n-1.0\n478.0\n157.0\n105.0\n-1.0\n-1.0\n4.0\n12.0\n43.0\n3.0\n3.0\n-1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n3.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n2.0\n-1.0\n-1.0\n-1.0\n0.0\n18.0\n28.0\n2.0\n1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6333348.0\n1794518.0\n21435.0\n2.0\n2006.0\n3401.025748\n2012.0\n3.0\n10.0\n7.0\n2.0\n67.0\n1.331078e+09\n0.0\n1.0\n\n\n412697\n3.0\n-1.0\n479.0\n157.0\n105.0\n62.0\n-1.0\n4.0\n16.0\n8.0\n3.0\n3.0\n-1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n3.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n2.0\n-1.0\n-1.0\n-1.0\n0.0\n18.0\n28.0\n2.0\n1.0\n0.0\n-1.0\n-1.0\n-1.0\n-1.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6333349.0\n1944743.0\n21436.0\n2.0\n2006.0\n3401.025748\n2012.0\n1.0\n4.0\n28.0\n5.0\n28.0\n1.327709e+09\n0.0\n1.0\n\n\n\n\n19561 rows × 66 columns\n\n\n\n\nn_estimators_preds = np.stack(\n    [\n        t.predict(preprocessing.transform(sk_df_valid.drop(\"SalePrice\", axis=1)).values)\n        for t in rf[\"classifier\"].estimators_\n    ]\n)\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.plot(\n    [\n        rmse(np.mean(n_estimators_preds[: i + 1], axis=0), sk_df_valid[\"SalePrice\"])\n        for i in range(len(rf[\"classifier\"].estimators_))\n    ],\n    label=\"RMSE\",\n)\nax.set_xlim(0, len(rf[\"classifier\"].estimators_))\nax.set_xmargin(0)\nax.set_ymargin(0)\nax.set_xlabel(\"Number of Estimators\")\nax.set_ylabel(\"RMSE\")\n\nax2 = ax.twinx()\nax2.plot(\n    [\n        r2_score(sk_df_valid[\"SalePrice\"], np.mean(n_estimators_preds[: i + 1], axis=0))\n        for i in range(len(rf[\"classifier\"].estimators_))\n    ],\n    color=\"red\",\n    label=\"R^2\",\n)\n\n\nplt.legend([ax.get_lines()[0], ax2.get_lines()[0]], [\"RMSE\", \"R^2\"], loc=\"center right\")\nplt.show()\n\n\n\n\n\n\nMinimum Samples Per Leaf\nThe minimum number of samples in a leaf. Setting this to a number greater than 1 reduces the number of decisions each tree has to make so we can train faster. It also means each tree will generalise better but be less powerful on its own.\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\n            \"classifier\",\n            RandomForestRegressor(n_jobs=-1, n_estimators=100, min_samples_leaf=3, bootstrap=True, oob_score=True),\n        ),\n    ]\n)\nrf.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train.drop(\"SalePrice\", axis=1),\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid.drop(\"SalePrice\", axis=1),\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 0.11332278041582786, 'Valid RMSE': 0.24705990836639896, 'Train r^2': 0.9732489121512647, 'Valid r^2': 0.8856412039379391, 'OOB score': 0.9121407829541297}\n\n\n\n\nMaximum Features\nWe want our trees to be as uncorrelated as possible. We could imagine a feauture that was especially predictive though. If every tree begins by splitting on this feature they will end up looking quite similar. We might not see certain interactions that are also important.\nTo avoid this we can, as well as using a random sample of rows, also use a random sample of columns. With row sampling, each new tree is trained on a different set of rows. With column sampling, at every binary split we choose from a different sample of columns.\nThe max_features parameter controls how many columns are used at each split. The default is sqrt which means the number of columns is the square root of the total number of columns. This is a good default. It is also possible to set it to a number or a fraction. If you set it to a number it will use that many columns. If you set it to a fraction it will use that fraction of the total number of columns.\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\n            \"classifier\",\n            RandomForestRegressor(\n                n_jobs=-1,\n                n_estimators=100,\n                max_features=0.5,\n                min_samples_leaf=3,\n                bootstrap=True,\n                oob_score=True,\n            ),\n        ),\n    ]\n)\nrf.fit(sk_df_train.drop(\"SalePrice\", axis=1), sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train.drop(\"SalePrice\", axis=1),\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid.drop(\"SalePrice\", axis=1),\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 0.11732676910046942, 'Valid RMSE': 0.23830678949305614, 'Train r^2': 0.971325145213223, 'Valid r^2': 0.8936009245495913, 'OOB score': 0.9156515084208113}\n\n\nAn illustration of the relationship between n_features and max_features is scan be seen here. Reducing the max_features makes individual trees weaker but makes them more uncorrelated with each other. So as the number of trees increases the OOB score improves more for the lower max_features setting."
  },
  {
    "objectID": "Random Forests/random_forest.html#permutation-importance",
    "href": "Random Forests/random_forest.html#permutation-importance",
    "title": "Random Forest",
    "section": "Permutation Importance",
    "text": "Permutation Importance\n\nimport rfpimp\n\n\nimportances = rfpimp.importances(\n    rf[\"classifier\"], preprocessing.transform(sk_df_valid.drop(\"SalePrice\", axis=1)), sk_df_valid[\"SalePrice\"]\n)\n\n\nfig, ax = plt.subplots(figsize=(10, 20))\nimportances.plot.barh(ax=ax)\nax.set_xmargin(0)\nax.set_ymargin(0)\nax.set_xlabel(\"Importance\")\nax.set_ylabel(\"Feature\")\nax.invert_yaxis()\nplt.show()\n\n\n\n\n\nviz = rfpimp.plot_corr_heatmap(sk_df_train, figsize=(10, 10))\n\n\n\n\n\nfdm = rfpimp.feature_dependence_matrix(preprocessing.transform(sk_df_train.drop(\"SalePrice\", axis=1)))\n\n\nrfpimp.plot_dependence_heatmap(fdm, figsize=(25, 25))\n\n\n\n\n\nsk_df_train_imp = sk_df_train.drop(\n    columns=[\n        \"SalesID\",\n        \"fiBaseModel\",\n        \"fiSecondaryDesc\",\n        \"fiModelDescriptor\",\n        \"saleElapsed\",\n        \"saleDayofyear\",\n        \"saleDay\",\n        \"saleWeek\",\n    ]\n)\nsk_df_valid_imp = sk_df_valid.drop(\n    columns=[\n        \"SalesID\",\n        \"fiBaseModel\",\n        \"fiSecondaryDesc\",\n        \"fiModelDescriptor\",\n        \"saleElapsed\",\n        \"saleDayofyear\",\n        \"saleDay\",\n        \"saleWeek\",\n    ]\n)\n\n\nconts, cats = cont_cat_split(sk_df_train_imp, max_card=3, dep_var=\"SalePrice\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1), cats),\n        (\"num\", SimpleImputer(strategy=\"mean\", add_indicator=True), conts),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\n            \"classifier\",\n            RandomForestRegressor(\n                n_jobs=-1,\n                n_estimators=100,\n                max_features=0.5,\n                min_samples_leaf=3,\n                bootstrap=True,\n                oob_score=True,\n            ),\n        ),\n    ]\n)\nrf.fit(sk_df_train_imp.drop(\"SalePrice\", axis=1), sk_df_train_imp[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train_imp.drop(\"SalePrice\", axis=1),\n    sk_df_train_imp[\"SalePrice\"],\n    sk_df_valid_imp.drop(\"SalePrice\", axis=1),\n    sk_df_valid_imp[\"SalePrice\"],\n)\n\n{'Train RMSE': 0.13044584127385733, 'Valid RMSE': 0.24481597336683458, 'Train r^2': 0.9645539798198519, 'Valid r^2': 0.8877091100805693, 'OOB score': 0.9096183807330281}\n\n\n\nimportances = rfpimp.importances(\n    rf[\"classifier\"], preprocessing.transform(sk_df_train_imp.drop(\"SalePrice\", axis=1)), sk_df_train_imp[\"SalePrice\"]\n)\n\nfig, ax = plt.subplots(figsize=(10, 20))\nimportances.plot.barh(ax=ax)\nax.set_xmargin(0)\nax.set_ymargin(0)\nax.set_xlabel(\"Importance\")\nax.set_ylabel(\"Feature\")\nax.invert_yaxis()\nplt.show()\n\n\n\n\n\nimportant_features = importances.iloc[:25].index\n\n\nconts, cats = cont_cat_split(sk_df_train_imp, max_card=3, dep_var=\"SalePrice\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1), cats),\n        (\"num\", SimpleImputer(strategy=\"mean\", add_indicator=True), conts),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\n            \"classifier\",\n            RandomForestRegressor(\n                n_jobs=-1,\n                n_estimators=100,\n                max_features=0.5,\n                min_samples_leaf=3,\n                bootstrap=True,\n                oob_score=True,\n            ),\n        ),\n    ]\n)\nrf.fit(sk_df_train_imp, sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train_imp,\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid_imp,\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 0.1329281099591207, 'Valid RMSE': 0.24545006031144978, 'Train r^2': 0.9631921318986634, 'Valid r^2': 0.8871266775282887, 'OOB score': 0.90895357456373}\n\n\n\nconts, cats = cont_cat_split(sk_df_train_imp, max_card=3, dep_var=\"SalePrice\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1), cats),\n        (\"num\", SimpleImputer(strategy=\"mean\", add_indicator=True), conts),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\n            \"classifier\",\n            RandomForestRegressor(\n                n_jobs=-1,\n                n_estimators=100,\n                max_features=0.5,\n                min_samples_leaf=20,\n                bootstrap=True,\n                oob_score=True,\n            ),\n        ),\n    ]\n)\nrf.fit(sk_df_train_imp, sk_df_train[\"SalePrice\"])\n\nprint_score(\n    rf,\n    sk_df_train_imp,\n    sk_df_train[\"SalePrice\"],\n    sk_df_valid_imp,\n    sk_df_valid[\"SalePrice\"],\n)\n\n{'Train RMSE': 0.2105525070888259, 'Valid RMSE': 0.25934193691691476, 'Train r^2': 0.9076519268078268, 'Valid r^2': 0.8739884009302059, 'OOB score': 0.8918486788207942}"
  },
  {
    "objectID": "Random Forests/random_forest.html#confidence-based-on-tree-variance",
    "href": "Random Forests/random_forest.html#confidence-based-on-tree-variance",
    "title": "Random Forest",
    "section": "Confidence based on tree variance",
    "text": "Confidence based on tree variance\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)\n\nWe saw how the model averages predictions across the trees to get an estimate - but how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions, instead of just the mean. This tells us the relative confidence of predictions - that is, for rows where the trees give very different results, you would want to be more cautious of using those results, compared to cases where they are more consistent. Using the same example as in the last lesson when we looked at bagging:\n\npreds = np.stack([t.predict(X_valid) for t in m.estimators_])\nnp.mean(preds[:, 0]), np.std(preds[:, 0])\n\nNameError: name 'm' is not defined\n\n\n\nx = raw_valid.copy()\nx[\"pred_std\"] = np.std(preds, axis=0)\nx[\"pred\"] = np.mean(preds, axis=0)\nx.Enclosure.value_counts().plot.barh()"
  },
  {
    "objectID": "01_General_Concepts/loss_functions.html",
    "href": "01_General_Concepts/loss_functions.html",
    "title": "Loss Functions",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0"
  },
  {
    "objectID": "01_General_Concepts/loss_functions.html#mean-absolute-error-l1-loss",
    "href": "01_General_Concepts/loss_functions.html#mean-absolute-error-l1-loss",
    "title": "Loss Functions",
    "section": "Mean Absolute Error (L1 Loss)",
    "text": "Mean Absolute Error (L1 Loss)\n\ndef calc_loss_one_pred(loss_function, actuals, pred):\n    preds_array = np.full(actuals.shape, pred)\n    loss = loss_function(actuals, preds_array)\n    return loss\n\n\ndef plot_loss(loss_function, actuals, possible_preds) -&gt; None:\n    loss = [calc_loss_one_pred(loss_function, actuals, pred) for pred in possible_preds]\n\n    fig, ax = plt.subplots(figsize=(5, 3))\n\n    ax.plot(possible_preds, loss)\n\n    ax.set_xlim(0, possible_preds.max())\n    ax.set_xticks(np.arange(0, possible_preds.max() + 1, 1))\n    ax.set_xlabel(\"Prediction\")\n\n    ax.set_ylim(0, max(loss) + 1)\n    ax.set_ylabel(\"Loss\")\n\n    ax.set_title(loss_function.__name__.replace(\"_\", \" \").title())\n    ax.grid(True)\n    plt.show()\n\n\nactuals = np.array([1, 8, 9])\npossible_preds = np.arange(0, 12, 0.1)\n\n\nplot_loss(mean_absolute_error, actuals, possible_preds)\n\n\n\n\n\ndef utility_function(inventory, demand):\n    if demand &gt;= inventory:\n        return -3 * (demand - inventory)\n    else:\n        return demand - inventory\n\n\nprint(f\"Loss from stocking 6 when demand is 1: {utility_function(6, 1)}\")\nprint(f\"Loss from stocking 6 when demand is 8: {utility_function(6, 8)}\")\nprint(f\"Loss from stocking 6 when demand is 9: {utility_function(6, 9)}\")\n\nLoss from stocking 6 when demand is 1: -5\nLoss from stocking 6 when demand is 8: -6\nLoss from stocking 6 when demand is 9: -9"
  },
  {
    "objectID": "01_General_Concepts/loss_functions.html#mean-squared-error",
    "href": "01_General_Concepts/loss_functions.html#mean-squared-error",
    "title": "Loss Functions",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nplot_loss(mean_squared_error, actuals, possible_preds)\n\n\n\n\nOne way in which mean absolute and mean squared error differ is that MAE leads your model to predict the median of the distribution while MSE leads your model to predict the mean of the distribution.\nTODO: proof of this"
  },
  {
    "objectID": "01_General_Concepts/metrics.html",
    "href": "01_General_Concepts/metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (\n    average_precision_score,\n    auc,\n    roc_auc_score,\n    roc_curve,\n    RocCurveDisplay,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    accuracy_score,\n    f1_score,\n    precision_recall_curve,\n    PrecisionRecallDisplay,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#prepare-example-model",
    "href": "01_General_Concepts/metrics.html#prepare-example-model",
    "title": "Metrics",
    "section": "Prepare Example Model",
    "text": "Prepare Example Model\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\")\n\n\ny = y.astype(int)\n\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n\ncategorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ntest_preds = rf.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#accuracy",
    "href": "01_General_Concepts/metrics.html#accuracy",
    "title": "Metrics",
    "section": "Accuracy",
    "text": "Accuracy\nFor accuracy to work you have to pick a threshold at which to consider a predicted probability to be a positive prediction. One value may result in a higher accuracy, but depending on the model you may want to pick a different threshold.\n\n(y_test.values == (test_preds &gt; 0.5)).mean()\n\n0.7865853658536586\n\n\n\naccuracy_score(y_test, test_preds &gt; 0.1)\n\n0.6067073170731707\n\n\n\naccuracy_score(y_test, test_preds &gt; 0.5)\n\n0.7865853658536586\n\n\n\naccuracy_score(y_test, test_preds &gt; 0.9)\n\n0.7530487804878049"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#recall",
    "href": "01_General_Concepts/metrics.html#recall",
    "title": "Metrics",
    "section": "Recall",
    "text": "Recall\nRecall measures what fraction of the true positives our model identified:\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\n\ntrue_positives = (y_test.values == 1) & (test_preds &gt; 0.5)\nfalse_negatives = (y_test.values == 1) & (test_preds &lt;= 0.5)\nnp.sum(true_positives) / np.sum(true_positives + false_negatives)\n\n0.72\n\n\n\nrecall_score(y_test, test_preds &gt; 0.1)\n\n0.928\n\n\n\nrecall_score(y_test, test_preds &gt; 0.5)\n\n0.72\n\n\n\nrecall_score(y_test, test_preds &gt; 0.9)\n\n0.376"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#precision",
    "href": "01_General_Concepts/metrics.html#precision",
    "title": "Metrics",
    "section": "Precision",
    "text": "Precision\nPrecision is also known as positive predictive value. It measures how accurate our positive predictions were:\n\\[\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\n\ntrue_positives = (y_test.values == 1) & (test_preds &gt; 0.5)\nfalse_positives = (y_test.values == 0) & (test_preds &gt; 0.5)\nnp.sum(true_positives) / np.sum(true_positives + false_positives)\n\n0.72\n\n\n\nprecision_score(y_test, test_preds &gt; 0.1)\n\n0.4915254237288136\n\n\n\nprecision_score(y_test, test_preds &gt; 0.5)\n\n0.72\n\n\n\nprecision_score(y_test, test_preds &gt; 0.9)\n\n0.94"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#f1-score",
    "href": "01_General_Concepts/metrics.html#f1-score",
    "title": "Metrics",
    "section": "F1 Score",
    "text": "F1 Score\nThe F1 score is the harmonic mean of the precision and recall. It thus symmetrically represents both precision and recall in one metric. The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero.\n\nf1_score(y_test, test_preds &gt; 0.5, average=\"binary\")\n\n0.72\n\n\n\nf1_score(y_test, test_preds &gt; 0.5, average=\"micro\")\n\n0.7865853658536586\n\n\n\nf1_score(y_test, test_preds &gt; 0.5, average=\"macro\")\n\n0.7737931034482759\n\n\n\nf1_score(y_test, test_preds &gt; 0.5, average=\"weighted\")\n\n0.7865853658536586"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#confusion-matrix",
    "href": "01_General_Concepts/metrics.html#confusion-matrix",
    "title": "Metrics",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nfig, ax = plt.subplots()\ncm = confusion_matrix(y_test, (test_preds &gt; 0.5).astype(int))\nConfusionMatrixDisplay(cm, display_labels=rf.classes_).plot(ax=ax)\nplt.show()"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#roc-curve",
    "href": "01_General_Concepts/metrics.html#roc-curve",
    "title": "Metrics",
    "section": "ROC Curve",
    "text": "ROC Curve\nBy varying the threshold we use to classify items in a sample as positive or negative, we can see how the true positive rate and false positive rate change. We can improve the recall of our model at the cost of precision, or vice versa. The Receiver Operating Characteristic (ROC) curve is a useful tool for visualizing this tradeoff. It gets its name from the fact that it was first used in WW2 to study the performance of radar receivers at detecting Japanese aircraft.\nThe ROC curve plots the TPR against FPR at various threshold values. In other words, it shows the performance of a model across classification thresholds.\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], \"--\", label=\"Random Guessing\")\nfpr, tpr, _ = roc_curve(y_test, test_preds)\nRocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax, label=\"Random Forest\")\nax.plot([0, 0, 1], [0, 1, 1], \"--\", label=\"Perfect Model\")\nax.set_xlim(-0.005, 1), ax.set_ylim(0, 1.005)\nplt.legend()\nplt.show()\n\n\n\n\nAs we can see above, with random guessing any attempt to increase our true positive rate will increase our false postitive rate by an equal amount. With a theoritically perfect model we have a 100% true positive rate and 0% false positive rate straight away. With our example model we can increase our true positive rate more quickly than the false positive rate up to a point before after which the false positive rate increases more quickly.\nWe would probably choose a threshold that puts our model as close to the upper left hand corner as possible. However there are times where we might not. For instance if a false postive would be more costly than a false negative we would want to choose a threshold that gives us a lower false positive rate at the cost of a higher false negative rate.\n\nROC AUC Score\nOf course we would prefer to not make that trade off and instead push the entire curve towards the upper left hand corner. To do that we would train a different model. To quantify how well different models are doing we can calculate the area underneath the ROC curve. This is called the ROC AUC score. The higher the score the better the model. A score of 0.5 means the model is no better than random guessing. A score of 1.0 means the model is perfect.\n\nprint(f\"ROC AUC score: {roc_auc_score(y_test, test_preds):.3f}\")\n\nROC AUC score: 0.847\n\n\nAn interesting feature of the ROC AUC Score is that the score for a set can be larger than the score for any subset of that set. We can see this by considering an alternative definition for the ROC AUC Score which is the probability that a randomly chosen row with a postive \\(y\\) value has a higher predicted probability than a randomly chosen row with a negative \\(y\\) value. Mathemically, the AUC formula can be written as follows:\n\\[\nAUC = \\frac{\\sum_{i=1}^{q}\\sum_{j=1}^{q}I[y_i &lt; y_j]I'[a_i &lt; a_j]}{\\sum_{i=1}^{q}\\sum_{j=1}^{q}I[y_i &lt; y_j]}\n\\]\nwhere\n\\[\nI'[a_i &lt; a_j] =\n\\begin{cases}\n    0 & a_i &gt; a_j \\\\\n    0.5 & a_i = a_j \\\\\n    1 & a_i &lt; a_j\n\\end{cases}\n\\]\nand\n\\[\nI[y_i &lt; y_j] =\n\\begin{cases}\n    0 & y_i \\geq y_j \\\\\n    1 & y_i &lt; y_j\n\\end{cases}\n\\]\nand\n\\(a_i\\) = the predicted probability for row \\(i\\)\n\\(y_i\\) = the true value for row \\(i\\)\n\\(q\\) = the number of rows in the set\nFor example, consider the following data, sorted by predicted probability:\n\n\n\npred\ny\n\n\n\n\n0.09\n0\n\n\n0.5\n1\n\n\n0.7\n0\n\n\n\nHere we have one correctly sorted pair of rows and one incorrectly sorted pair of rows. The probability that a randomly chosen row with a positive y value has a higher predicted probability than a randomly chosen row with a negative y value is therefore 0.5.\n\\[AUC = \\frac{1}{2}\\times\\frac{1}{2} + \\frac{1}{2}\\times\\frac{1}{2} = \\frac{1}{2}\\]\nThe AUC for this data will be 0.5.\nNow consider this data:\n\n\n\npred\ny\n\n\n\n\n0.095\n0\n\n\n0.41\n1\n\n\n0.42\n0\n\n\n\nAgain the AUC for this data will be 0.5.\nHowever let us now combine the two data sets:\n\n\n\npred\ny\n\n\n\n\n0.09\n0\n\n\n0.095\n0\n\n\n0.41\n1\n\n\n0.42\n0\n\n\n0.5\n1\n\n\n0.7\n0\n\n\n\n\\[AUC = \\frac{1}{2}\\times\\frac{1}{2} + \\frac{3}{4}\\times\\frac{1}{2} = 0.625\\]"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#gini",
    "href": "01_General_Concepts/metrics.html#gini",
    "title": "Metrics",
    "section": "Gini",
    "text": "Gini\nThe Gini Coefficient is the summary statistic of the Cumulative Accuracy Profile (CAP) chart. It is calculated as the quotient of the area which the CAP curve and diagonal enclose and the corresponding area in an ideal rating procedure.\n\nrandom_model_coords = [0, len(y_test)], [0, np.sum(y_test)]\nrf_model_coords = (\n    np.arange(0, len(y_test) + 1),\n    np.append([0], np.cumsum([y for _, y in sorted(zip(test_preds, y_test), reverse=True)])),\n)\nperfect_model_coords = ([0, np.sum(y_test), len(y_test)], [0, np.sum(y_test), np.sum(y_test)])\n\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax.plot(*random_model_coords, linestyle=\"--\", label=\"Random Model\")\nax.plot(*rf_model_coords, linestyle=\"-\", label=\"Random Forest Classifier\")\nax.plot(*perfect_model_coords, linestyle=(0, (3, 2)), label=\"Perfect Model\")\nax.set_xlim(0, len(y_test))\nax.set_ylim(0, np.sum(y_test) + 1)\nplt.legend()\nplt.show()\n\n\n\n\n\n# Area under Random Model\naR = auc(*random_model_coords)\n# Area under Random Forest Model\na = auc(*rf_model_coords)\n# Area under Perfect Model\naP = auc(*perfect_model_coords)\n\naR, a, aP\n\n(20500.0, 29343.5, 33187.5)\n\n\n\n# Area between Perfect and Random Model\naP - aR\n\n12687.5\n\n\n\n# Area between Random Forest and Random Model\na - aR\n\n8843.5\n\n\n\n# Accuracy rate\naccuracy_rate = (a - aR) / (aP - aR)\naccuracy_rate\n\n0.6970246305418719\n\n\nThe accuracy rate is the same as the Gini. The direct conversion between accuracy rate / Gini and AUC_ROC is:\n\\[Gini = 2AUC\\_ROC - 1\\]\nWe can derive the above formula like so:\nLet \\(aP\\) be the area under a perfect model curve\nLet \\(a\\) be the area under a trained model curve\nLet \\(aR\\) be the area under a random model curve\n\\[\n\\begin{align}\n\\text{Gini} &= \\frac{a - aR}{aP - aR} \\\\\naP - aR &= \\frac{1}{2} \\\\\n\\text{Gini} &= \\frac{a - aR}{\\frac{1}{2}} \\\\\n&= 2(a - aR) \\\\\nAUC &= (a - aR) + \\frac{1}{2} \\\\\na - aR &= AUC - \\frac{1}{2} \\\\\n\\text{Gini} &= 2(AUC - \\frac{1}{2}) \\\\\n&= 2AUC - 1 \\\\\n\\end{align}\n\\]\n\n2 * roc_auc_score(y_test, test_preds) - 1\n\n0.6938719211822659\n\n\nThe results are slightly different results because the AUC is calculated using the trapezium method which is only an approximation of the true area under the curve."
  },
  {
    "objectID": "01_General_Concepts/metrics.html#precision-recall-curve",
    "href": "01_General_Concepts/metrics.html#precision-recall-curve",
    "title": "Metrics",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\nThe precision-recall curve shows the tradeoff between precision and recall for different thresholds. As you increase the threshold, you increase the precision since the model is only returning positive predictions when it is very confident but you decrease the recall since fewer true positives will meet the threshold.\nHigh scores for both precision and recall show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). So we would like to maximise the area under this curve.\nThe precision-recall curve can be more informative than the ROC curve when dealing with imbalanced data. See more here. PR plots won’t let a model get away with just predicting the majority class.\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [y_test.mean(), y_test.mean()], \"--\", label=\"Random Guessing\")\nprecision, recall, thresholds = precision_recall_curve(y_test, test_preds)\nPrecisionRecallDisplay(precision=precision, recall=recall).plot(ax=ax, label=\"Random Forest\")\nax.plot([1, 1, 0], [0, 1, 1], \"--\", label=\"Perfect Model\")\nax.set_xlim(0, 1.005), ax.set_ylim(0, 1.005)\nplt.legend()\nplt.show()\n\n\n\n\nIn the above example a random model would produce a horizontal line at the mean the mean of the true values.\nWe can use the trapezium rule to find the area under the precision-recall curve:\n\n-np.sum(np.diff(recall) * np.array(precision)[:-1])\n\n0.8198678510173409\n\n\nIn scikit-learn this formula is known as the average_precision_score:\n\naverage_precision_score(y_test, test_preds)\n\n0.8198678510173409"
  },
  {
    "objectID": "01_General_Concepts/metrics.html#sources",
    "href": "01_General_Concepts/metrics.html#sources",
    "title": "Metrics",
    "section": "Sources",
    "text": "Sources\n\nhttps://www.toyota-ti.ac.jp/Lab/Denshi/COIN/people/yutaka.sasaki/F-measure-YS-26Oct07.pdf\nhttps://stats.stackexchange.com/questions/416901/can-the-roc-auc-of-a-total-test-set-be-larger-than-the-auc-for-any-subset-of-som\nhttps://dasha.ai/en-us/blog/auc-roc#::text=ROC%20AUC%20is%20the%20area,%3D%209.5%2F12%20%200.79.\nhttps://www.kaggle.com/code/rohandawar/cap-cumulative-accuracy-profile-analysis-1\nhttps://www.geeksforgeeks.org/python-cap-cumulative-accuracy-profile-analysis/\nhttps://www.listendata.com/2019/09/gini-cumulative-accuracy-profile-auc.html"
  },
  {
    "objectID": "07_Geospatial/geocoding_and_joining.html",
    "href": "07_Geospatial/geocoding_and_joining.html",
    "title": "Manipulating Geospatial Data",
    "section": "",
    "text": "import math\nfrom pathlib import Path\n\nimport folium\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom folium import Marker, GeoJson\nfrom folium.plugins import HeatMap\nfrom geopy.geocoders import Nominatim\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nPROJECT_ROOT = Path.cwd().parent.parent"
  },
  {
    "objectID": "07_Geospatial/geocoding_and_joining.html#geocoding",
    "href": "07_Geospatial/geocoding_and_joining.html#geocoding",
    "title": "Manipulating Geospatial Data",
    "section": "Geocoding",
    "text": "Geocoding\nGeocoding is the process of converting the name of a place or an address to a location on a map.\nIn the imports above, Nominatim refers to the geocoding software that will be used to generate locations.\nWe begin by instantiating the geocoder. Then, we need only apply the name or address as a Python string. (In this case, we supply “Pyramid of Khufu”, also known as the Great Pyramid of Giza.)\nIf the geocoding is successful, it returns a geopy.location.Location object with two important attributes:\n\nthe “point” attribute contains the (latitude, longitude) location, and\nthe “address” attribute contains the full address.\n\n\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\nlocation = geolocator.geocode(\"Pyramid of Khufu\")\n\nprint(location.point)\nprint(location.address)\n\n29 58m 44.976s N, 31 8m 3.17625s E\nهرم خوفو, شارع ابو الهول السياحي, نزلة البطران, الجيزة, 12125, مصر\n\n\nThe value for the “point” attribute is a geopy.point.Point object. We can get the latitude and longitude from the latitude and longitude attributes, respectively.\n\npoint = location.point\nprint(\"Latitude:\", point.latitude)\nprint(\"Longitude:\", point.longitude)\n\nLatitude: 29.97916\nLongitude: 31.134215625236113\n\n\nIt’s often the case that we’ll need to geocode many different addresses. For instance, say we want to obtain the locations of 100 top universities in Europe.\n\nuniversities = pd.read_csv(f\"{PROJECT_ROOT}/data/kaggle_geospatial/top_universities.csv\")\nuniversities.head()\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nUniversity of Oxford\n\n\n1\nUniversity of Cambridge\n\n\n2\nImperial College London\n\n\n3\nETH Zurich\n\n\n4\nUCL\n\n\n\n\n\n\n\nThen we can use a lambda function to apply the geocoder to every row in the DataFrame. (We use a try/except statement to account for the case that the geocoding is unsuccessful.)\n\ndef my_geocoder(row):\n    try:\n        point = geolocator.geocode(row).point\n        return pd.Series({\"Latitude\": point.latitude, \"Longitude\": point.longitude})\n    except:\n        return None\n\n\nuniversities[[\"Latitude\", \"Longitude\"]] = universities.apply(lambda x: my_geocoder(x[\"Name\"]), axis=1)\n\nprint(f\"{(1 - np.isnan(universities['Latitude']).mean()) * 100}% of addresses were geocoded\")\n\n95.0% of addresses were geocoded!\n\n\n\nuniversities = universities.loc[~np.isnan(universities[\"Latitude\"])]\nuniversities = gpd.GeoDataFrame(\n    universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude), crs=\"EPSG:4326\"\n)\nuniversities.head()\n\n\n\n\n\n\n\n\nName\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\nUniversity of Oxford\n51.759037\n-1.252430\nPOINT (-1.25243 51.75904)\n\n\n1\nUniversity of Cambridge\n52.210946\n0.092005\nPOINT (0.09200 52.21095)\n\n\n2\nImperial College London\n51.498959\n-0.175641\nPOINT (-0.17564 51.49896)\n\n\n3\nETH Zurich\n47.413218\n8.537491\nPOINT (8.53749 47.41322)\n\n\n4\nUCL\n51.521785\n-0.135151\nPOINT (-0.13515 51.52179)\n\n\n\n\n\n\n\nNext, we visualize all of the locations that were returned by the geocoder. Notice that a few of the locations are certainly inaccurate, as they’re not in Europe!\n\nm = folium.Map(location=[54, 15], tiles=\"openstreetmap\", zoom_start=2)\n\nfor idx, row in universities.iterrows():\n    Marker([row[\"Latitude\"], row[\"Longitude\"]], popup=row[\"Name\"]).add_to(m)\n\nm"
  },
  {
    "objectID": "07_Geospatial/geocoding_and_joining.html#table-joins",
    "href": "07_Geospatial/geocoding_and_joining.html#table-joins",
    "title": "Manipulating Geospatial Data",
    "section": "Table joins",
    "text": "Table joins\n\nAttribute join\nYou already know how to use pd.DataFrame.join() to combine information from multiple DataFrames with a shared index. We refer to this way of joining data (by sampling matching values in the index) as an attribute join.\nWhen performing an attribute join with a GeoDataFrame, it’s best to use gpd.GeoDataFrame.merge(). To illustrate this, we’ll work with a GeoDataFrame europe_boundaries containing the boundaries for every country in Europe. The first five rows of this GeoDataFrame are printed below.\n\nworld = gpd.read_file(f\"{PROJECT_ROOT}/data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\")\neurope = world.loc[world[\"CONTINENT\"] == \"Europe\"].reset_index(drop=True)\n\neurope_stats = europe[[\"NAME\", \"POP_EST\", \"GDP_MD\"]]\neurope_boundaries = europe[[\"NAME\", \"geometry\"]]\n\neurope_boundaries.head()\n\n\n\n\n\n\n\n\nNAME\ngeometry\n\n\n\n\n0\nRussia\nMULTIPOLYGON (((178.72530 71.09880, 180.00000 ...\n\n\n1\nNorway\nMULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n\n\n2\nFrance\nMULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n\n\n3\nSweden\nPOLYGON ((11.02737 58.85615, 11.46827 59.43239...\n\n\n4\nBelarus\nPOLYGON ((28.17671 56.16913, 29.22951 55.91834...\n\n\n\n\n\n\n\nWe’ll join it with a DataFrame europe_stats containing the estimated population and gross domestic product (GDP) for each country.\n\neurope_stats.head()\n\n\n\n\n\n\n\n\nNAME\nPOP_EST\nGDP_MD\n\n\n\n\n0\nRussia\n144373535.0\n1699876\n\n\n1\nNorway\n5347896.0\n403336\n\n\n2\nFrance\n67059887.0\n2715518\n\n\n3\nSweden\n10285453.0\n530883\n\n\n4\nBelarus\n9466856.0\n63080\n\n\n\n\n\n\n\nWe do the attribute join in the code cell below. The on argument is set to the column NAME that is used to match rows in europe_boundaries to rows in europe_stats.\n\neurope = europe_boundaries.merge(europe_stats, on=\"NAME\")\neurope.head()\n\n\n\n\n\n\n\n\nNAME\ngeometry\nPOP_EST\nGDP_MD\n\n\n\n\n0\nRussia\nMULTIPOLYGON (((178.72530 71.09880, 180.00000 ...\n144373535.0\n1699876\n\n\n1\nNorway\nMULTIPOLYGON (((15.14282 79.67431, 15.52255 80...\n5347896.0\n403336\n\n\n2\nFrance\nMULTIPOLYGON (((-51.65780 4.15623, -52.24934 3...\n67059887.0\n2715518\n\n\n3\nSweden\nPOLYGON ((11.02737 58.85615, 11.46827 59.43239...\n10285453.0\n530883\n\n\n4\nBelarus\nPOLYGON ((28.17671 56.16913, 29.22951 55.91834...\n9466856.0\n63080\n\n\n\n\n\n\n\n\n\nSpatial join\nAnother type of join is a spatial join. With a spatial join, we combine GeoDataFrames based on the spatial relationship between the objects in the geometry column. For instance, we already have a GeoDataFrame universities containing geocoded addresses of European universities.\nThen we can use a spatial join to match each university to its corresponding country. We do this with gpd.sjoin().\n\neuropean_universities = gpd.sjoin(universities, europe)\n\nprint(f\"We located {len(universities)} universities.\")\nprint(\n    f\"Only {len(european_universities)} of the universities were located in Europe (in {len(european_universities['NAME'].unique())} different countries).\"\n)\neuropean_universities.head()\n\nWe located 95 universities.\nOnly 90 of the universities were located in Europe (in 15 different countries).\n\n\n\n\n\n\n\n\n\nName\nLatitude\nLongitude\ngeometry\nindex_right\nNAME\nPOP_EST\nGDP_MD\n\n\n\n\n0\nUniversity of Oxford\n51.759037\n-1.252430\nPOINT (-1.25243 51.75904)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n1\nUniversity of Cambridge\n52.210946\n0.092005\nPOINT (0.09200 52.21095)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n2\nImperial College London\n51.498959\n-0.175641\nPOINT (-0.17564 51.49896)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n4\nUCL\n51.521785\n-0.135151\nPOINT (-0.13515 51.52179)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n5\nLondon School of Economics and Political Science\n51.514261\n-0.116734\nPOINT (-0.11673 51.51426)\n28\nUnited Kingdom\n66834405.0\n2829108\n\n\n\n\n\n\n\nThe spatial join above looks at the “geometry” columns in both GeoDataFrames. If a Point object from the universities GeoDataFrame intersects a Polygon object from the europe DataFrame, the corresponding rows are combined and added as a single row of the european_universities DataFrame. Otherwise, countries without a matching university (and universities without a matching country) are omitted from the results.\nThe gpd.sjoin() method is customizable for different types of joins, through the how and op arguments. For instance, you can do the equivalent of a SQL left (or right) join by setting how='left' or how='right'."
  },
  {
    "objectID": "07_Geospatial/geocoding_and_joining.html#proximity-analysis",
    "href": "07_Geospatial/geocoding_and_joining.html#proximity-analysis",
    "title": "Manipulating Geospatial Data",
    "section": "Proximity Analysis",
    "text": "Proximity Analysis\nWe have a dataset from the US Environmental Protection Agency (EPA) that tracks releases of toxic chemicals in Philadelphia.\n\nreleases = gpd.read_file(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/toxic_release_pennsylvania/toxic_release_pennsylvania/toxic_release_pennsylvania.shp\"\n)\nreleases.head()\n\n\n\n\n\n\n\n\nYEAR\nCITY\nCOUNTY\nST\nLATITUDE\nLONGITUDE\nCHEMICAL\nUNIT_OF_ME\nTOTAL_RELE\ngeometry\n\n\n\n\n0\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.005901\n-75.072103\nFORMIC ACID\nPounds\n0.160\nPOINT (2718560.227 256380.179)\n\n\n1\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.920120\n-75.146410\nETHYLENE GLYCOL\nPounds\n13353.480\nPOINT (2698674.606 224522.905)\n\n\n2\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n40.023880\n-75.220450\nCERTAIN GLYCOL ETHERS\nPounds\n104.135\nPOINT (2676833.394 261701.856)\n\n\n3\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nLEAD COMPOUNDS\nPounds\n1730.280\nPOINT (2684030.004 221697.388)\n\n\n4\n2016\nPHILADELPHIA\nPHILADELPHIA\nPA\n39.913540\n-75.198890\nBENZENE\nPounds\n39863.290\nPOINT (2684030.004 221697.388)\n\n\n\n\n\n\n\nWe also have a dataset that contains readings from air quality monitoring stations in Philadelphia.\n\nstations = gpd.read_file(\n    f\"{PROJECT_ROOT}/data/kaggle_geospatial/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations/PhillyHealth_Air_Monitoring_Stations.shp\"\n)\nstations.head()\n\n\n\n\n\n\n\n\nSITE_NAME\nADDRESS\nBLACK_CARB\nULTRAFINE_\nCO\nSO2\nOZONE\nNO2\nNOY_NO\nPM10\nPM2_5\nSPECIATED_\nPM_COURSE\nCARBONYLS\nPAMS_VOC\nTSP_11101\nTSP_METALS\nTSP_LEAD\nTOXICS_TO1\nMET\nCOMMUNITY_\nLATITUDE\nLONGITUDE\ngeometry\n\n\n\n\n0\nLAB\n1501 East Lycoming Avenue\nN\nN\nY\nN\nY\nY\nY\nN\nY\nN\nN\nY\nY\nN\nY\nN\ny\nN\nN\n40.008606\n-75.097624\nPOINT (2711384.641 257149.310)\n\n\n1\nROX\nEva and Dearnley Streets\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nY\nN\nN\nY\nN\nY\nN\nN\n40.050461\n-75.236966\nPOINT (2671934.290 271248.900)\n\n\n2\nNEA\nGrant Avenue and Ashton Street\nN\nN\nN\nN\nY\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nY\nN\n40.072073\n-75.013128\nPOINT (2734326.638 280980.247)\n\n\n3\nCHS\n500 South Broad Street\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nN\nY\nN\nN\nY\nN\nY\nN\nN\n39.944510\n-75.165442\nPOINT (2693078.580 233247.101)\n\n\n4\nNEW\n2861 Lewis Street\nN\nN\nY\nY\nY\nN\nY\nY\nY\nY\nY\nN\nN\nY\nN\nY\nN\nY\nN\n39.991688\n-75.080378\nPOINT (2716399.773 251134.976)\n\n\n\n\n\n\n\n\nMeasuring distance\nTo measure distances between points from two different GeoDataFrames, we first have to make sure that they use the same coordinate reference system (CRS). Thankfully, this is the case here, where both use EPSG 2272.\n\nprint(stations.crs)\nprint(releases.crs)\n\nEPSG:2272\nEPSG:2272\n\n\nWe also check the CRS to see which units it uses (meters, feet, or something else). In this case, EPSG 2272 has units of feet. (If you like, you can check this here.)\nIt’s relatively straightforward to compute distances in GeoPandas. The code cell below calculates the distance (in feet) between a relatively recent release incident in recent_release and every station in the stations GeoDataFrame.\n\nrecent_release = releases.iloc[360]\n\n# Measure distance from release to each station\ndistances = stations.geometry.distance(recent_release.geometry)\ndistances\n\n0     44778.509761\n1     51006.456589\n2     77744.509207\n3     14672.170878\n4     43753.554393\n5      4711.658655\n6     23197.430858\n7     12072.823097\n8     79081.825506\n9      3780.623591\n10    27577.474903\n11    19818.381002\ndtype: float64\n\n\nUsing the calculated distances, we can obtain statistics like the mean distance to each station.\n\nprint(f\"Mean distance to monitoring stations: {round(distances.mean(), 2)} feet\")\n\nMean distance to monitoring stations: 33516.28 feet\n\n\nOr, we can get the closest monitoring station.\n\nprint(f\"Closest monitoring station ({distances.min()} feet):\")\nprint(stations.iloc[distances.idxmin()][[\"ADDRESS\", \"LATITUDE\", \"LONGITUDE\"]])\n\nClosest monitoring station (3780.623590556444 feet):\nADDRESS      3100 Penrose Ferry Road\nLATITUDE                    39.91279\nLONGITUDE                 -75.185448\nName: 9, dtype: object\n\n\n\n\nCreating a buffer\nIf we want to understand all points on a map that are some radius away from a point, the simplest way is to create a buffer.\nThe code cell below creates a GeoSeries two_mile_buffer containing 12 different Polygon objects. Each polygon is a buffer of 2 miles (or, 2*5280 feet) around a different air monitoring station.\n\ntwo_mile_buffer = stations.geometry.buffer(2 * 5280)\ntwo_mile_buffer.head()\n\n0    POLYGON ((2721944.641 257149.310, 2721893.792 ...\n1    POLYGON ((2682494.290 271248.900, 2682443.441 ...\n2    POLYGON ((2744886.638 280980.247, 2744835.789 ...\n3    POLYGON ((2703638.580 233247.101, 2703587.731 ...\n4    POLYGON ((2726959.773 251134.976, 2726908.924 ...\ndtype: geometry\n\n\nWe use folium.GeoJson() to plot each polygon on a map. Note that since folium requires coordinates in latitude and longitude, we have to convert the CRS to EPSG 4326 before plotting.\n\nm = folium.Map(location=[39.9526, -75.1652], zoom_start=11)\n\nHeatMap(data=releases[[\"LATITUDE\", \"LONGITUDE\"]], radius=15).add_to(m)\n\nfor idx, row in stations.iterrows():\n    Marker([row[\"LATITUDE\"], row[\"LONGITUDE\"]]).add_to(m)\n\nGeoJson(two_mile_buffer.to_crs(epsg=4326)).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nNow, to test if a toxic release occurred within 2 miles of any monitoring station, we could run 12 different tests for each polygon (to check individually if it contains the point).\nBut a more efficient way is to first collapse all of the polygons into a MultiPolygon object. We do this with the unary_union attribute.\n\nmy_union = two_mile_buffer.geometry.unary_union\n\nmy_union\n\n\n\n\nWe use the contains() method to check if the multipolygon contains a point. We’ll use the release incident from earlier, which we know is roughly 3781 feet to the closest monitoring station.\n\nmy_union.contains(releases.iloc[360].geometry)\n\nTrue\n\n\nNot all releases occured within two miles of an air monitoring station\n\nmy_union.contains(releases.iloc[358].geometry)\n\nFalse"
  },
  {
    "objectID": "07_Geospatial/shapely.html",
    "href": "07_Geospatial/shapely.html",
    "title": "Shapely",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pyproj\nfrom shapely import Point, Polygon\nfrom shapely.ops import transform\n\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\npol1 = Polygon([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])\n\nprint(pol1.area)\nprint(pol1.length)\nprint(pol1.bounds)\nprint(pol1.centroid)\nprint(pol1.representative_point())\nprint(pol1.exterior)\nprint(pol1.exterior.xy)\n\n\nfig, ax = plt.subplots(figsize=(4, 4))\nax.plot(*pol1.exterior.xy)\nax.plot(*pol1.centroid.xy, \"o\")\n# ax.set_aspect('equal')\nplt.show()\n\n1.0\n4.0\n(0.0, 0.0, 1.0, 1.0)\nPOINT (0.5 0.5)\nPOINT (0.5 0.5)\nLINEARRING (0 0, 1 0, 1 1, 0 1, 0 0)\n(array('d', [0.0, 1.0, 1.0, 0.0, 0.0]), array('d', [0.0, 0.0, 1.0, 1.0, 0.0]))\ndef set_limits(ax, x0, xN, y0, yN):\n    ax.set_xlim(x0, xN)\n    ax.set_xticks(range(x0, xN + 1))\n    ax.set_ylim(y0, yN)\n    ax.set_yticks(range(y0, yN + 1))\n    ax.set_aspect(\"equal\")\nfrom shapely.plotting import plot_polygon, plot_points\nplot_polygon??\n\nSignature:\nplot_polygon(\n    polygon,\n    ax=None,\n    add_points=True,\n    color=None,\n    facecolor=None,\n    edgecolor=None,\n    linewidth=None,\n    **kwargs,\n)\nSource:   \ndef plot_polygon(\n    polygon,\n    ax=None,\n    add_points=True,\n    color=None,\n    facecolor=None,\n    edgecolor=None,\n    linewidth=None,\n    **kwargs\n):\n    \"\"\"\n    Plot a (Multi)Polygon.\n\n    Note: this function is experimental, and mainly targetting (interactive)\n    exploration, debugging and illustration purposes.\n\n    Parameters\n    ----------\n    polygon : shapely.Polygon or shapely.MultiPolygon\n    ax : matplotlib Axes, default None\n        The axes on which to draw the plot. If not specified, will get the\n        current active axes or create a new figure.\n    add_points : bool, default True\n        If True, also plot the coordinates (vertices) as points.\n    color : matplotlib color specification\n        Color for both the polygon fill (face) and boundary (edge). By default,\n        the fill is using an alpha of 0.3. You can specify `facecolor` and\n        `edgecolor` separately for greater control.\n    facecolor : matplotlib color specification\n        Color for the polygon fill.\n    edgecolor : matplotlib color specification\n        Color for the polygon boundary.\n    linewidth : float\n        The line width for the polygon boundary.\n    **kwargs\n        Additional keyword arguments passed to the matplotlib Patch.\n\n    Returns\n    -------\n    Matplotlib artist (PathPatch), if `add_points` is false.\n    A tuple of Matplotlib artists (PathPatch, Line2D), if `add_points` is true.\n    \"\"\"\n    from matplotlib import colors\n\n    if ax is None:\n        ax = _default_ax()\n\n    if color is None:\n        color = \"C0\"\n    color = colors.to_rgba(color)\n\n    if facecolor is None:\n        facecolor = list(color)\n        facecolor[-1] = 0.3\n        facecolor = tuple(facecolor)\n\n    if edgecolor is None:\n        edgecolor = color\n\n    patch = patch_from_polygon(\n        polygon, facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth, **kwargs\n    )\n    ax.add_patch(patch)\n    ax.autoscale_view()\n\n    if add_points:\n        line = plot_points(polygon, ax=ax, color=color)\n        return patch, line\n\n    return patch\nFile:      ~/.pyenv/versions/3.11.1/envs/py3111/lib/python3.11/site-packages/shapely/plotting.py\nType:      function\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import PathPatch\nfrom matplotlib.path import Path\nfrom shapely.geometry import Polygon\nimport numpy as np\n\n\ndef plot_shapely_polygon(polygon, ax=None, **kwargs):\n    if isinstance(polygon, Polygon):\n        polygons = [polygon]\n    else:\n        polygons = list(polygon.geoms)\n\n    for polygon in polygons:\n        exterior_coords = np.array(polygon.exterior.coords)\n        interior_coords = [np.array(ring.coords) for ring in polygon.interiors]\n\n        codes = [Path.MOVETO]\n        codes.extend([Path.LINETO] * (len(exterior_coords) - 2))\n        codes.append(Path.CLOSEPOLY)\n\n        for ring in interior_coords:\n            codes.append(Path.MOVETO)\n            codes.extend([Path.LINETO] * (len(ring) - 2))\n            codes.append(Path.CLOSEPOLY)\n\n        all_coords = np.vstack([exterior_coords] + interior_coords)\n\n        path = Path(all_coords, codes)\n        patch = PathPatch(path, **kwargs)\n        ax.add_patch(patch)\nfig, axes = plt.subplots(1, 4, figsize=(4, 16), dpi=90)\n\next = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\nint = [(1, 0), (0.5, 0.5), (1, 1), (1.5, 0.5), (1, 0)][::-1]\npolygon = Polygon(ext, [int])\n\n\nplot_polygon(polygon, ax=axes[0], add_points=False)\n# plot_points(polygon, ax=axes[0], alpha=0.7)\n\naxes[0].set_title(\"a) valid\")\n\nset_limits(axes[0], -1, 3, -1, 3)\n\next = [(0, 0), (0, 2), (2, 2), (2, 0), (0, 0)]\nint = [(1, 0), (0, 1), (0.5, 1.5), (1.5, 0.5), (1, 0)][::-1]\npolygon = Polygon(ext, [int])\n\nplot_polygon(polygon, ax=axes[1], add_points=False)\nplot_points(polygon, ax=axes[1], alpha=0.7)\n\naxes[1].set_title(\"b) invalid\")\n\nset_limits(axes[1], -1, 3, -1, 3)\n\n\nx, y = polygon.exterior.xy\n\naxes[2].fill(x, y, alpha=0.5, fc=\"r\")\naxes[2].plot(x, y, marker=\"o\", color=\"r\")\n\nfor interior in polygon.interiors:\n    x, y = interior.xy\n    axes[2].fill(x, y, alpha=0, fc=(1, 1, 1, 0))  # fill with white to create a hole effect\n    axes[2].plot(x, y, marker=\"o\", color=\"r\")\n\n# Customize plot\naxes[2].set_title(\"Shapely Polygon\")\n\nset_limits(axes[2], -1, 3, -1, 3)\n\nexterior_coords = np.array(polygon.exterior.coords)\ninterior_coords = [np.array(ring.coords) for ring in polygon.interiors]\n\ncodes = [Path.MOVETO]\ncodes.extend([Path.LINETO] * (len(exterior_coords) - 2))\ncodes.append(Path.CLOSEPOLY)\n\nfor ring in interior_coords:\n    codes.append(Path.MOVETO)\n    codes.extend([Path.LINETO] * (len(ring) - 2))\n    codes.append(Path.CLOSEPOLY)\n\nall_coords = np.vstack([exterior_coords] + interior_coords)\n\npath = Path(all_coords, codes)\npatch = PathPatch(path, facecolor=\"r\", edgecolor=\"r\", alpha=0.5)\naxes[3].add_patch(patch)\n\naxes[3].plot(x, y, marker=\"o\", color=\"r\")\n\naxes[3].set_title(\"Matplotlib PathPatch\")\n\nset_limits(axes[3], -1, 3, -1, 3)\n\nplt.show()\nall_coords\n\narray([[0. , 0. ],\n       [0. , 2. ],\n       [2. , 2. ],\n       [2. , 0. ],\n       [0. , 0. ],\n       [1. , 0. ],\n       [1.5, 0.5],\n       [0.5, 1.5],\n       [0. , 1. ],\n       [1. , 0. ]])"
  },
  {
    "objectID": "07_Geospatial/shapely.html#transforms",
    "href": "07_Geospatial/shapely.html#transforms",
    "title": "Shapely",
    "section": "Transforms",
    "text": "Transforms\n\nwgs84_pt = Point(-72.2495, 43.886)\n\nwgs84 = pyproj.CRS(\"EPSG:4326\")\nutm = pyproj.CRS(\"EPSG:32618\")\n\nproject = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True).transform\nutm_point = transform(project, wgs84_pt)\n\nutm_point.xy\n\n(array('d', [720944.1103566973]), array('d', [4862889.031679545]))"
  },
  {
    "objectID": "07_Geospatial/gdal.html",
    "href": "07_Geospatial/gdal.html",
    "title": "GDAL",
    "section": "",
    "text": "GDAL is a collection of tools for working with Geospatial data. It powers things like QGIS. You can install it using pip."
  },
  {
    "objectID": "07_Geospatial/gdal.html#gdal-cli-api",
    "href": "07_Geospatial/gdal.html#gdal-cli-api",
    "title": "GDAL",
    "section": "GDAL CLI API",
    "text": "GDAL CLI API\n\n!gdalinfo --version\n\nGDAL 3.6.4, released 2023/04/17"
  },
  {
    "objectID": "08_Decision_Optimisation/decision_opt_continuous_outcomes.html",
    "href": "08_Decision_Optimisation/decision_opt_continuous_outcomes.html",
    "title": "Decision Optimisation for Continuous Outcomes",
    "section": "",
    "text": "import math\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom quantile_forest import RandomForestQuantileRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\n\npd.set_option(\"display.max_columns\", None)\n\nPROJECT_ROOT = Path.cwd().parent.parent\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nThe data that we will use comes from the Grupo Bimbo Inventory Demand Kaggle competition.\nThe goal is to predict the demand of a product for a given week, at a particular store (column Demanda_uni_equil). The dataset consists of 9 weeks of sales transactions in Mexico. Each transaction consists of sales and returns. Returns are the products that are unsold and expired. The demand for a product in a certain week is defined as the sales this week subtracted by the return next week.\ndata = pd.read_csv(f\"{PROJECT_ROOT}/data/grupo-bimbo-inventory-demand/train.csv\", nrows=200000, low_memory=False)\nclientes = pd.read_csv(f\"{PROJECT_ROOT}/data/grupo-bimbo-inventory-demand/cliente_tabla.csv\", low_memory=False)\nproductos = pd.read_csv(f\"{PROJECT_ROOT}/data/grupo-bimbo-inventory-demand/producto_tabla.csv\", low_memory=False)\ntown_state = pd.read_csv(f\"{PROJECT_ROOT}/data/grupo-bimbo-inventory-demand/town_state.csv\", low_memory=False)\n\ndata = pd.merge(data, clientes, on=\"Cliente_ID\", how=\"left\")\ndata = pd.merge(data, productos, on=\"Producto_ID\", how=\"left\")\ndata = pd.merge(data, town_state, on=\"Agencia_ID\", how=\"left\")\ndata\n\n\n\n\n\n\n\n\nSemana\nAgencia_ID\nCanal_ID\nRuta_SAK\nCliente_ID\nProducto_ID\nVenta_uni_hoy\nVenta_hoy\nDev_uni_proxima\nDev_proxima\nDemanda_uni_equil\nNombreCliente\nNombreProducto\nTown\nState\n\n\n\n\n0\n3\n1110\n7\n3301\n15766\n1212\n3\n25.14\n0\n0.0\n3\nPUESTO DE PERIODICOS LAZARO\nRoles Canela 2p 120g BIM 1212\n2008 AG. LAGO FILT\nMÉXICO, D.F.\n\n\n1\n3\n1110\n7\n3301\n15766\n1216\n4\n33.52\n0\n0.0\n4\nPUESTO DE PERIODICOS LAZARO\nRoles Glass 2p 135g BIM 1216\n2008 AG. LAGO FILT\nMÉXICO, D.F.\n\n\n2\n3\n1110\n7\n3301\n15766\n1238\n4\n39.32\n0\n0.0\n4\nPUESTO DE PERIODICOS LAZARO\nPanquecito Gota Choc 2p 140g BIM 1238\n2008 AG. LAGO FILT\nMÉXICO, D.F.\n\n\n3\n3\n1110\n7\n3301\n15766\n1240\n4\n33.52\n0\n0.0\n4\nPUESTO DE PERIODICOS LAZARO\nMantecadas Vainilla 4p 125g BIM 1240\n2008 AG. LAGO FILT\nMÉXICO, D.F.\n\n\n4\n3\n1110\n7\n3301\n15766\n1242\n3\n22.92\n0\n0.0\n3\nPUESTO DE PERIODICOS LAZARO\nDonitas Espolvoreadas 6p 105g BIM 1242\n2008 AG. LAGO FILT\nMÉXICO, D.F.\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n200735\n3\n1116\n1\n1466\n2309869\n1238\n8\n78.64\n0\n0.0\n8\nUNION DEL VALLE 2\nPanquecito Gota Choc 2p 140g BIM 1238\n2011 AG. SAN ANTONIO\nMÉXICO, D.F.\n\n\n200736\n3\n1116\n1\n1466\n2309869\n1240\n8\n67.04\n0\n0.0\n8\nUNION DEL VALLE 2\nMantecadas Vainilla 4p 125g BIM 1240\n2011 AG. SAN ANTONIO\nMÉXICO, D.F.\n\n\n200737\n3\n1116\n1\n1466\n2309869\n1242\n6\n45.84\n0\n0.0\n6\nUNION DEL VALLE 2\nDonitas Espolvoreadas 6p 105g BIM 1242\n2011 AG. SAN ANTONIO\nMÉXICO, D.F.\n\n\n200738\n3\n1116\n1\n1466\n2309869\n1250\n27\n206.28\n0\n0.0\n27\nUNION DEL VALLE 2\nDonas Azucar 4p 105g BIM 1250\n2011 AG. SAN ANTONIO\nMÉXICO, D.F.\n\n\n200739\n3\n1116\n1\n1466\n2309869\n1278\n13\n58.50\n0\n0.0\n13\nUNION DEL VALLE 2\nNito 1p 62g BIM 1278\n2011 AG. SAN ANTONIO\nMÉXICO, D.F.\n\n\n\n\n200740 rows × 15 columns\ncategorical_cols = [\"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\", \"Producto_ID\"]\n\nlabel_encoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    le.fit(data[col])\n    data[col] = le.transform(data[col])\n    label_encoders[col] = le\nnum_unique_vals = {col: data[col].nunique() for col in categorical_cols}\nembedding_sizes = {col: min(50, num_unique_vals[col] // 2) for col in categorical_cols}\nnum_unique_vals\n\n{'Agencia_ID': 6,\n 'Canal_ID': 6,\n 'Ruta_SAK': 343,\n 'Cliente_ID': 10472,\n 'Producto_ID': 478}\nembedding_sizes\n\n{'Agencia_ID': 3,\n 'Canal_ID': 3,\n 'Ruta_SAK': 50,\n 'Cliente_ID': 50,\n 'Producto_ID': 50}\nX = data[categorical_cols].values\ny = data[\"Demanda_uni_equil\"].values\nX\n\narray([[   0,    3,  293,    3,   43],\n       [   0,    3,  293,    3,   44],\n       [   0,    3,  293,    3,   48],\n       ...,\n       [   5,    0,  235, 7722,   50],\n       [   5,    0,  235, 7722,   51],\n       [   5,    0,  235, 7722,   53]])\ny\n\narray([ 3,  4,  4, ...,  6, 27, 13])\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\nclass BimboDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = [torch.tensor(X[:, i], dtype=torch.long) for i in range(X.shape[1])]\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return [x[idx] for x in self.X], self.y[idx]\ntrain_dataset = BimboDataset(X_train, y_train)\nval_dataset = BimboDataset(X_val, y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\nclass SimpleModel(nn.Module):\n    def __init__(self, embedding_sizes, hidden_size=128):\n        super(SimpleModel, self).__init__()\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(num_unique_vals[col], embedding_sizes[col]) for col in categorical_cols]\n        )\n        self.fc1 = nn.Linear(sum(embedding_sizes.values()), hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        x = [embedding(x_i) for x_i, embedding in zip(x, self.embeddings)]\n        x = torch.cat(x, dim=-1)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x).squeeze(-1)\n        return x\ndef train_model(loss_fn, num_epochs=5):\n    model = SimpleModel(embedding_sizes)\n    optimizer = optim.Adam(model.parameters(), lr=0.005)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for inputs, targets in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        # Validation loop\n        model.eval()\n        val_loss = 0.0\n        val_preds = []\n        val_targets = []\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                outputs = model(inputs).squeeze()\n                loss = loss_fn(outputs, targets)\n                val_loss += loss.item()\n                val_preds.extend(outputs.tolist())\n                val_targets.extend(targets.tolist())\n\n        val_loss /= len(val_loader)\n        r2 = r2_score(val_targets, val_preds)\n        val_preds = np.clip(val_preds, 0, None)\n        rmsle = np.sqrt(mean_squared_log_error(val_targets, val_preds))\n        print(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": round(train_loss, 5),\n                \"val_loss\": round(val_loss, 5),\n                \"r_squared\": round(r2, 5),\n                \"rmsle\": round(rmsle, 5),\n            }\n        )\n    return model, np.array(val_preds), np.array(val_targets)\nbusiness_metrics = pd.DataFrame(\n    columns=[\n        \"Model Name\",\n        \"Understocked Fraction\",\n        \"Understocked Amount\",\n        \"Overstocked Fraction\",\n        \"Overstocked Amount\",\n        \"Utility\",\n        \"MAE\",\n        \"MSE\",\n        \"R2\",\n        \"RMSLE\",\n    ]\n)\ndef log_business_metrics(model_name: str, stocking_decisions, actual_demand):\n    frac_understocks = (stocking_decisions &lt; actual_demand).mean()\n    total_understocked_amt = (actual_demand - stocking_decisions).clip(0).sum()\n    frac_overstocks = (stocking_decisions &gt; actual_demand).mean()\n    total_overstocked_amt = (stocking_decisions - actual_demand).clip(0).sum()\n    utility = -3 * total_understocked_amt - total_overstocked_amt\n    mae = mean_absolute_error(actual_demand, stocking_decisions)\n    mse = mean_squared_error(actual_demand, stocking_decisions)\n    r2 = r2_score(actual_demand, stocking_decisions)\n    rmsle = np.sqrt(mean_squared_log_error(actual_demand, stocking_decisions))\n\n    df = pd.DataFrame(\n        data={\n            \"Model Name\": model_name,\n            \"Understocked Fraction\": frac_understocks,\n            \"Understocked Amount\": total_understocked_amt,\n            \"Overstocked Fraction\": frac_overstocks,\n            \"Overstocked Amount\": total_overstocked_amt,\n            \"Utility\": utility,\n            \"MAE\": mae,\n            \"MSE\": mse,\n            \"R2\": r2,\n            \"RMSLE\": rmsle,\n        },\n        index=[0],\n    )\n\n    return df\nloss = nn.MSELoss()\nmse_model, mse_val_preds, mse_val_targets = train_model(loss, num_epochs=5)\n\n{'epoch': 0, 'train_loss': 376.08925, 'val_loss': 227.47458, 'r_squared': 0.63116, 'rmsle': 0.67018}\n{'epoch': 1, 'train_loss': 246.72899, 'val_loss': 202.99381, 'r_squared': 0.67091, 'rmsle': 0.60655}\n{'epoch': 2, 'train_loss': 188.01208, 'val_loss': 157.39294, 'r_squared': 0.74483, 'rmsle': 0.59883}\n{'epoch': 3, 'train_loss': 154.99719, 'val_loss': 191.1642, 'r_squared': 0.69006, 'rmsle': 0.58823}\n{'epoch': 4, 'train_loss': 139.73279, 'val_loss': 172.35903, 'r_squared': 0.72053, 'rmsle': 0.58192}\nmse_val_stock = np.ceil(mse_val_preds)\nbm1 = log_business_metrics(\"MSE model\", mse_val_stock, mse_val_targets)\npd.concat([business_metrics, bm1], axis=0, ignore_index=True)\n\n\n\n\n\n\n\n\nModel Name\nUnderstocked Fraction\nUnderstocked Amount\nOverstocked Fraction\nOverstocked Amount\nUtility\nMAE\nMSE\nR2\nRMSLE\n\n\n\n\n0\nMSE model\n0.275381\n72458.0\n0.631015\n104514.0\n-321888.0\n4.40799\n173.166235\n0.719482\n0.627397\nWith the stocking rules above, we understock 27% of the time. This seems bad so let’s try a different approach to making the stocking decision.\nBelow we take the existing predictions and multiply them by 1.5 and round them up. In other words we are going to stock 50% above the model’s predictions and see what happens.\nalternative_stocking_rule = np.ceil(1.5 * mse_val_preds)\nbm2 = log_business_metrics(\"MSE model * 1.5\", alternative_stocking_rule, mse_val_targets)\npd.concat([business_metrics, bm1, bm2], axis=0, ignore_index=True)\n\n\n\n\n\n\n\n\nModel Name\nUnderstocked Fraction\nUnderstocked Amount\nOverstocked Fraction\nOverstocked Amount\nUtility\nMAE\nMSE\nR2\nRMSLE\n\n\n\n\n0\nMSE model\n0.275381\n72458.0\n0.631015\n104514.0\n-321888.0\n4.407990\n173.166235\n0.719482\n0.627397\n\n\n1\nMSE model * 1.5\n0.135075\n32762.0\n0.802904\n226306.0\n-324592.0\n6.452825\n322.720982\n0.477213\n0.775845\nWe now understock only 8% of the time. We paid for this by increasing the percentage of weeks we are overstocked from 59% to 80%.\nLooking at how the size of the overstocks we can se that we went from 89731 to 208034. In other words we’ve more than doubled how much unnecessary stuff we are buying. This suggests we would best off focusing improvements to our model that bring down this number."
  },
  {
    "objectID": "08_Decision_Optimisation/decision_opt_continuous_outcomes.html#predicting-full-distributions",
    "href": "08_Decision_Optimisation/decision_opt_continuous_outcomes.html#predicting-full-distributions",
    "title": "Decision Optimisation for Continuous Outcomes",
    "section": "Predicting Full Distributions",
    "text": "Predicting Full Distributions\nIn machine learning, it is common to train models to make point estimate predictions and not give too much thought to statistical distributions. However, when actually making decisions it is useful to have more fine grained control over what is coming out when you call model.predict(). It is especially useful for dynamic optimisation, where the prediction you make will affect the state you are in in the next period e.g. if our bread stays good for 2 weeks overordering one week might mean we expect to need to order less next week. This is very important in reinforcement learning.\nThere are many ways to build models that return statistical distributions, including deep learning methods or with Bayesian methods in libraries like PyMC. We for this example, we’ll use a Quantile Regression Forest.\nA typical random forest is composed of decision trees. Each decision will take the training data and split it many times until it reaches a leaf. When you want to make predictions at inference time you will take a row of data, run it through a tree, and get a prediction which will be the mean or median of the data in whichever leaf the row ends up in. The random forest’s final prediction is then the mean of each decision tree’s predictions.\nIn a quantile random forest, if you want to know the median of the data in a leaf you will take the median of the data in that leaf. If you want to know the 90th percentile of the data in a leaf you will take the 90th percentile of the data in that leaf.\nThe API for training the RandomForestQuantileRegressor is very similar to training a normal RandomForestRegressor in scikit-learn:\n\nqrf = RandomForestQuantileRegressor(n_estimators=100, min_samples_leaf=50, random_state=0)\nqrf.fit(X_train, y_train)\n\nRandomForestQuantileRegressor(min_samples_leaf=50, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestQuantileRegressorRandomForestQuantileRegressor(min_samples_leaf=50, random_state=0)\n\n\nAt prediction time, we need to choose which quantiles we want to make predictions for. Below we are saying that we would like to get the predictions for the 5th, 10th, 15th etc quantiles all the way up to the 95th quantile. In total that is 19 quantiles, so the array returned by the .precict() method is shape [n_rows, n_quantiles].\n\nquantiles = [i / 100 for i in range(5, 100, 5)]\nsample_preds = qrf.predict(X_val, quantiles=quantiles)\nprint(sample_preds.shape)\n\n(40148, 19)\n\n\nLet’s look at a single row of predictions. Say, the 6th:\n\none_demand_prediction = sample_preds[5]\none_demand_prediction\n\narray([1., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 4., 4., 5., 5., 6., 7.,\n       7., 9.])\n\n\nThe numbers returned may vary so suppose the result was:\n[1., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 4., 4., 5., 5., 6., 7., 7., 9.]\nWhat we are seeing here is that there is a \\(\\frac{1}{19}\\) chance that the actual amount of bread sold will be 1 unit. There is a \\(\\frac{3}{19}\\) chance that the actual amount of bread sold will be 2 units. And so on.\nLet’s now look at at how we could turn this into an actual stocking rule.\nSuppose we decided that we wanted to stock enough such that we are only understocked 10% of the time. In that case we would take the second last value from our prediction array (since that corresponds to the 90th percentile) and stock that amount.\nIt’s possible that your distribution may have some extreme jumps in it where e.g. it goes from predicting ~5 at the 80th percentile to ~50 at the 90th percentile. To smooth avoid taking too big a gamble on a particular quantile’s value you may want to add an outlier bound like below which caps how much you will stock at 3 times the mean of the quantile values.\n\ndef rarely_run_out_rule(prediction):\n    outlier_bound = 3 * np.mean(prediction)\n    to_stock = math.ceil(min(prediction[-2], outlier_bound))\n    return to_stock\n\n\nrarely_run_out_rule(one_demand_prediction)\n\n7\n\n\nWe can see how often this cap actually gets applied by running the cell below\n\nall_stocking_decisions = np.apply_along_axis(rarely_run_out_rule, 1, sample_preds)\n(all_stocking_decisions &lt; sample_preds[:, -2]).mean()\n\n0.009091361960745243\n\n\nLooking at the business metrics this rule generates, we can see that we do indeed understock ~10% of the time. We overstock a lot though as this is quite an aggressive rule which hurts the Utility score. There are various ways we could tune this e.g. by adjusting which percentiles or caps we use as well as the other ways discussed earlier.\n\nbm5 = log_business_metrics(\"Quantile regressor\", all_stocking_decisions, y_val)\npd.concat([business_metrics, bm1, bm2, bm3, bm4, bm5], axis=0, ignore_index=True)\n\n\n\n\n\n\n\n\nModel Name\nUnderstocked Fraction\nUnderstocked Amount\nOverstocked Fraction\nOverstocked Amount\nUtility\nMAE\nMSE\nR2\nRMSLE\n\n\n\n\n0\nMSE model\n0.275381\n72458.0\n0.631015\n104514.0\n-321888.0\n4.407990\n173.166235\n0.719482\n0.627397\n\n\n1\nMSE model * 1.5\n0.135075\n32762.0\n0.802904\n226306.0\n-324592.0\n6.452825\n322.720982\n0.477213\n0.775845\n\n\n2\nMAE model\n0.338921\n81489.0\n0.496687\n71728.0\n-316195.0\n3.816305\n185.795133\n0.699024\n0.520921\n\n\n3\nMAE model * 1.5\n0.156322\n37625.0\n0.758494\n184350.0\n-297225.0\n5.528918\n333.072955\n0.460443\n0.655141\n\n\n4\nQuantile regressor\n0.084313\n35904.0\n0.878624\n395286.0\n-502998.0\n10.740012\n835.487994\n-0.353437\n1.009825\n\n\n\n\n\n\n\n\nfor row in [0, 126, 295, 298, 557, 620, 882]:\n    print(f\"Stocked: {all_stocking_decisions[row]}\\nInput: {sample_preds[row, :]}\\n\")\n\nStocked: 8\nInput: [ 1.  1.  1.  1.  2.  2.  2.  2.  2.  3.  3.  3.  3.  4.  6.  6.  7.  8.\n 10.]\n\nStocked: 3\nInput: [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.45 2.   2.   2.\n 3.   3.   3.   3.   4.  ]\n\nStocked: 141\nInput: [  8.95  10.    12.    15.8   18.    29.4   33.3   36.    39.55  42.\n  47.25  57.    68.35  72.    78.5   84.2  138.3  141.   187.1 ]\n\nStocked: 25\nInput: [ 1.95  2.    2.    2.    2.75  3.7   4.65  5.    5.55  6.5   8.    8.\n  9.   10.   13.   15.   19.   25.   30.  ]\n\nStocked: 11\nInput: [ 1.    2.    2.    2.    2.    3.    3.    3.    4.    4.    4.    4.\n  4.35  5.    5.    6.2   8.   10.4  20.  ]\n\nStocked: 5\nInput: [1.   1.   1.   1.   1.75 2.   2.   2.   2.   2.   2.   3.   3.   3.\n 4.   4.2  5.   5.   7.05]\n\nStocked: 6\nInput: [1.   1.   1.   2.   2.   2.   2.   2.   2.   2.   2.45 3.   3.   3.\n 3.   4.   5.   5.1  7.05]"
  },
  {
    "objectID": "08_Decision_Optimisation/decision_opt_continuous_outcomes.html#references",
    "href": "08_Decision_Optimisation/decision_opt_continuous_outcomes.html#references",
    "title": "Decision Optimisation for Continuous Outcomes",
    "section": "References",
    "text": "References\n\nMachine Learning for Business Decision Optimization - Dan Becker\nZillow’s Quantile Forest Library\nScikit Garden"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html",
    "href": "08_Decision_Optimisation/drift.html",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "",
    "text": "from pathlib import Path\n\nimport graphviz\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport seaborn as sns\nimport shap\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.metrics import r2_score, roc_auc_score, log_loss\nfrom torch.utils.data import Dataset, DataLoader\nfrom xgboost import XGBClassifier\n\npd.set_option(\"display.max_columns\", None)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\nPROJECT_ROOT = Path.cwd().parent.parent"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#introduction",
    "href": "08_Decision_Optimisation/drift.html#introduction",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "Introduction",
    "text": "Introduction\nDrift is where the patterns that we see in our training data do not hold after the model goes into production. There are two main types of drift.\n\nConcept Drfit: The relationship between the input and target changes\nCovariate Shift: The distribution of the input feautures changes\n\nA more mathmatical way of thinking about these two types of drift is that given a “data generating function” \\(y = f(x)\\), concept drift is a change in \\(f\\) and covariate shift is a change in the distribution of \\(x\\). We can also visualise this difference:\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nax = ax.flatten()\n\nxs = np.arange(0, 13, 1)\nys_1 = xs\nys_2 = xs * 1.2\n\nax[0].scatter(xs, ys_1, label=\"Training Data\")\nax[0].scatter(xs, ys_2, label=\"Production Data\")\nax[0].set_title(\"Concept Drift\")\n\nxs_1 = np.arange(0, 10, 1)\nxs_2 = np.arange(5.5, 14.5, 1)\nys_1 = xs_1\nys_2 = xs_2\n\nax[1].scatter(xs_1, ys_1, label=\"Training Data\")\nax[1].scatter(xs_2, ys_2, label=\"Production Data\")\nax[1].set_title(\"Covariate Shift\")\n\nfor a in ax:\n    a.set_xlim(0, 15)\n    a.set_ylim(0, 15)\n    a.set_xticks(np.arange(0, 16, 1))\n    a.set_yticks(np.arange(0, 16, 1))\n    a.set_xlabel(\"Predictive Feature\")\n    a.set_ylabel(\"Target\")\n    a.legend()\n    a.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#detecting-drift",
    "href": "08_Decision_Optimisation/drift.html#detecting-drift",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "Detecting Drift",
    "text": "Detecting Drift\nTwo ways to detect drift are:\n\nMonitoring the descriptive statistics in you data over time\nBacktesting, where you train you model on a certain time period and then validate it on a later time period\nUsing your intuition and domain knowledge to be aware of when things might be different\n\nWe’ll use the Grupo Bimbo dataset to illustrate how to detect drift\n\nall_data = pd.read_csv(f\"{PROJECT_ROOT}/data/grupo-bimbo-inventory-demand/train.csv\", low_memory=True)\nprint(all_data.shape)\nall_data.head()\n\n(74180464, 11)\n\n\n\n\n\n\n\n\n\nSemana\nAgencia_ID\nCanal_ID\nRuta_SAK\nCliente_ID\nProducto_ID\nVenta_uni_hoy\nVenta_hoy\nDev_uni_proxima\nDev_proxima\nDemanda_uni_equil\n\n\n\n\n0\n3\n1110\n7\n3301\n15766\n1212\n3\n25.14\n0\n0.0\n3\n\n\n1\n3\n1110\n7\n3301\n15766\n1216\n4\n33.52\n0\n0.0\n4\n\n\n2\n3\n1110\n7\n3301\n15766\n1238\n4\n39.32\n0\n0.0\n4\n\n\n3\n3\n1110\n7\n3301\n15766\n1240\n4\n33.52\n0\n0.0\n4\n\n\n4\n3\n1110\n7\n3301\n15766\n1242\n3\n22.92\n0\n0.0\n3\n\n\n\n\n\n\n\nOne way in which this data changes over time in the presence of absence of gaps. This can lead to compositional changes which may affect the performance of your model or the patterns it identifies when train. You would want to investigate how these gaps occur to see if they contain a useful signal, are merely noise, or if there is a way to mitigate their effect.\nFor this demo we will mitigate some of their effect by just looking at a single product ID.\n\nproduct_data = all_data.query(\"Producto_ID == 1238\")\nprint(product_data.shape)\nproduct_data.head()\n\n(1191873, 11)\n\n\n\n\n\n\n\n\n\nSemana\nAgencia_ID\nCanal_ID\nRuta_SAK\nCliente_ID\nProducto_ID\nVenta_uni_hoy\nVenta_hoy\nDev_uni_proxima\nDev_proxima\nDemanda_uni_equil\n\n\n\n\n2\n3\n1110\n7\n3301\n15766\n1238\n4\n39.32\n0\n0.0\n4\n\n\n95\n3\n1110\n7\n3301\n198780\n1238\n9\n88.47\n0\n0.0\n9\n\n\n153\n3\n1110\n7\n3301\n988589\n1238\n1\n9.83\n0\n0.0\n1\n\n\n169\n3\n1110\n7\n3301\n1159580\n1238\n6\n58.98\n0\n0.0\n6\n\n\n189\n3\n1110\n7\n3301\n1163700\n1238\n3\n29.49\n0\n0.0\n3\n\n\n\n\n\n\n\nThis doesn’t get rid of the compositional shift entirely. We can see that the number of locations where this product is appearing seems to be decreasing week on week.\n\nproduct_data[\"Semana\"].value_counts().sort_index()\n\nSemana\n3    194927\n4    181975\n5    171431\n6    166625\n7    164743\n8    158032\n9    154140\nName: count, dtype: int64\n\n\nLooking further at the data we can see that most locations do not have a record of this product in every week. 75% have 5 weeks of fewer worth of records. To further reduce the compositional shift we will just look at locations that have a record of the full 7 weeks.\n\nstore_product_group_cols = [\"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\"]\nstore_product_value_counts = product_data.groupby(store_product_group_cols).size()\nstore_product_value_counts.describe()\n\ncount    372501.000000\nmean          3.199650\nstd           1.983802\nmin           1.000000\n25%           1.000000\n50%           3.000000\n75%           5.000000\nmax           7.000000\ndtype: float64\n\n\n\nfull_filled_data = product_data.set_index(store_product_group_cols).loc[(store_product_value_counts == 7)]\nfull_filled_data.reset_index(inplace=True)\n\nprint(full_filled_data.shape)\nfull_filled_data.head()\n\n(288617, 11)\n\n\n\n\n\n\n\n\n\nAgencia_ID\nCanal_ID\nRuta_SAK\nCliente_ID\nSemana\nProducto_ID\nVenta_uni_hoy\nVenta_hoy\nDev_uni_proxima\nDev_proxima\nDemanda_uni_equil\n\n\n\n\n0\n1110\n7\n3301\n15766\n3\n1238\n4\n39.32\n0\n0.0\n4\n\n\n1\n1110\n7\n3301\n1159580\n3\n1238\n6\n58.98\n0\n0.0\n6\n\n\n2\n1110\n7\n3301\n1163700\n3\n1238\n3\n29.49\n0\n0.0\n3\n\n\n3\n1110\n7\n3301\n1307034\n3\n1238\n3\n29.49\n0\n0.0\n3\n\n\n4\n1110\n7\n3301\n1682456\n3\n1238\n3\n29.49\n0\n0.0\n3\n\n\n\n\n\n\n\nAfter all that filtering, we can see that the average demand has declined over time from ~5.4 to ~4.6.\n\nfig, ax = plt.subplots(figsize=(4, 4))\nfull_filled_data.groupby([\"Semana\"])[\"Demanda_uni_equil\"].mean().plot(kind=\"bar\", ax=ax)\nax.set_ylim(0, 6)\nax.grid(axis=\"y\")\nax.set_xlabel(\"Week\")\nax.set_ylabel(\"Average demand\")\nplt.show()"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#backtesting",
    "href": "08_Decision_Optimisation/drift.html#backtesting",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "Backtesting",
    "text": "Backtesting\nThe next thing we are going to do is train a set of models where each model is trained on one week and validated on data from the next week.\nIf each model performs similarly well that would indicate less drift. If however the performance varies a lot that would suggest that the validation score we have for a model trained on the full dataset may not be a good indicator of how it will perform in the future.\n\nModelling\n\ncategorical_cols = [\"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\", \"Producto_ID\"]\nnumerical_cols = [\"Venta_uni_hoy\", \"Venta_hoy\"]\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, num_unique_vals, hidden_size=128, num_features=2):\n        super(SimpleModel, self).__init__()\n        embedding_size = 50\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(num_unique_vals[col], embedding_size) for col in categorical_cols]\n        )\n        self.num_layer = nn.Linear(num_features, embedding_size)\n        self.fc1 = nn.Linear(embedding_size * len(num_unique_vals) + embedding_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n\n    def forward(self, x_cat, x_num):\n        x_cat = [embedding(x_i.clip(0)) for x_i, embedding in zip(x_cat, self.embeddings)]\n        x_cat = torch.cat(x_cat, dim=-1)\n        x_num = torch.relu(self.num_layer(x_num))\n        x = torch.cat([x_cat, x_num], dim=-1)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x).squeeze(-1)\n        return x\n\n\ndef train(train_dataset, val_dataset, num_unique_vals):\n    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n\n    num_epochs = 5\n    loss_fn = nn.MSELoss()\n    model = SimpleModel(num_unique_vals)\n    optimizer = optim.Adam(model.parameters(), lr=0.005)\n\n    for _ in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for (inputs_cat, inputs_num), targets in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs_cat, inputs_num).squeeze()\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n\n    model.eval()\n    val_loss, val_preds, val_targets = 0.0, [], []\n    with torch.no_grad():\n        for (inputs_cat, inputs_num), targets in val_loader:\n            outputs = model(inputs_cat, inputs_num).squeeze()\n            loss = loss_fn(outputs, targets)\n            val_loss += loss.item()\n            val_preds.extend(outputs.tolist())\n            val_targets.extend(targets.tolist())\n    val_loss /= len(val_loader)\n    return model, val_loss\n\n\nclass BimboDataset(Dataset):\n    def __init__(self, X_cat, X_num, y):\n        self.X_cat = [torch.tensor(X_cat[:, i], dtype=torch.long) for i in range(X_cat.shape[1])]\n        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        x_cat = [x[idx] for x in self.X_cat]\n        x_num = self.X_num[idx]\n        y = self.y[idx]\n        return (x_cat, x_num), y\n\n\ndef make_models(data):\n    num_unique_vals = {col: data[col].nunique() for col in categorical_cols}\n    data = data.sort_values(\"Semana\")\n\n    X_categorical = data[categorical_cols].values\n    X_numerical = data[numerical_cols].values\n    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n    X_categorical = encoder.fit_transform(X_categorical)\n    y = data[\"Demanda_uni_equil\"].values\n\n    weeks = sorted(data[\"Semana\"].unique())\n\n    model_list, metric_list = [], []\n    for training_week in weeks[:-1]:\n        val_week = training_week + 1\n        train_index = data[data[\"Semana\"] == training_week].index\n        val_index = data[data[\"Semana\"] == val_week].index\n\n        X_train_cat, X_val_cat = X_categorical[train_index], X_categorical[val_index]\n        X_train_num, X_val_num = X_numerical[train_index], X_numerical[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n        train_dataset = BimboDataset(X_train_cat, X_train_num, y_train)\n        val_dataset = BimboDataset(X_val_cat, X_val_num, y_val)\n\n        model, val_loss = train(train_dataset, val_dataset, num_unique_vals)\n        metrics = {\"training_data_week\": training_week, \"validation_data_week\": val_week, \"validation_loss\": val_loss}\n        model_list.append(model)\n        metric_list.append(metrics)\n    return model_list, encoder, metric_list\n\n\nmodels, encoder, metrics = make_models(full_filled_data)\n\nWe can see here a week where the a model performs much worse. This is the sort of thing you want to investigate in search of drift.\n\nfig, ax = plt.subplots(figsize=(4, 4))\npd.DataFrame(metrics).plot(kind=\"bar\", x=\"training_data_week\", y=\"validation_loss\", ax=ax)\nax.set_xlabel(\"Training Data Week\")\nax.set_ylabel(\"Validation Loss\")\nplt.show()"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#understanding-covariate-shift",
    "href": "08_Decision_Optimisation/drift.html#understanding-covariate-shift",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "Understanding covariate shift",
    "text": "Understanding covariate shift\nAlthough covariate shift does not mean the relationship between the input and target has change, it can still cause problems as your model will now being seeing data in a different area from where it learned that relationship.\nIf you think you have covariate shift in your data it is worth thinking about how you may need to update your simulation as a result. For example, if you have more high income people going forward and they are known to purchase things more frequently, it may be worth thinking about ways to “hard code” that in your simulation."
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#understanding-concept-drift",
    "href": "08_Decision_Optimisation/drift.html#understanding-concept-drift",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "Understanding concept drift",
    "text": "Understanding concept drift\nWe’ll use the telco customer churn dataset to illustrate this.\n\ndata = pd.read_csv(f\"{PROJECT_ROOT}/data/WA_Fn-UseC_-Telco-Customer-Churn.csv\", low_memory=False)\nprint(data.shape)\ndata.head()\n\n(7043, 21)\n\n\n\n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\nOnlineBackup\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n0\n7590-VHVEG\nFemale\n0\nYes\nNo\n1\nNo\nNo phone service\nDSL\nNo\nYes\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.85\n29.85\nNo\n\n\n1\n5575-GNVDE\nMale\n0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\nNo\nYes\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\n\n\n2\n3668-QPYBK\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\nYes\nYes\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n108.15\nYes\n\n\n3\n7795-CFOCW\nMale\n0\nNo\nNo\n45\nNo\nNo phone service\nDSL\nYes\nNo\nYes\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.30\n1840.75\nNo\n\n\n4\n9237-HQITU\nFemale\n0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\n\n\n\n\n\n\n\nWe can start by training the model normally\n\ntarget = \"Churn\"\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(target, axis=1), data[target] == \"Yes\", test_size=0.2, random_state=0\n)\ncols_to_use = [\n    \"tenure\",\n    \"PhoneService\",\n    \"MultipleLines\",\n    \"InternetService\",\n    \"OnlineSecurity\",\n    \"OnlineBackup\",\n    \"DeviceProtection\",\n    \"TechSupport\",\n    \"StreamingTV\",\n    \"StreamingMovies\",\n    \"Contract\",\n    \"PaperlessBilling\",\n    \"PaymentMethod\",\n    \"MonthlyCharges\",\n]\n\npreprocessor = ColumnTransformer(\n    transformers=[(\"one_hot\", OneHotEncoder(), make_column_selector(dtype_include=\"object\"))],\n    remainder=\"passthrough\",  # Leave numerical variables unchanged\n)\n\npipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", XGBClassifier())])\npipeline.fit(X_train[cols_to_use], y_train)\ny_pred = pipeline.predict_proba(X_test[cols_to_use])[:, 1]\nroc_auc = roc_auc_score(y_test, y_pred)\nlog_loss_val = log_loss(y_test, y_pred)\n\nWe’ll now write a baseline prediction function which simply returns the predicted probability of churn from the model.\n\ndef baseline_prediction(data):\n    return pipeline.predict_proba(data[cols_to_use])[:, 1]\n\nNow we might have some intuition that DSL customers are more likely to churn than the model would predict based on the historic training data. May there has been a recent development in the news or the telco market which we would like to take into account.\nIn this case we could do something like update our prediction function to add a 10% increase in the probability of churn for DSL customers. 10% is a very arbitrary number here. It may be possible to come up with a strong justification for a particular number. However, even if you can’t it is always worth considering at least adjusting somewhat in the direction you think your business is about to move.\n\ndef prediction_adjust_DSL(data):\n    baseline = baseline_prediction(data)\n    has_DSL = data[\"InternetService\"] == \"DSL\"\n    out = baseline + has_DSL * 0.1\n    out = np.clip(out, 0, 1)\n    return out"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#adjusting-concept-drift-with-shap",
    "href": "08_Decision_Optimisation/drift.html#adjusting-concept-drift-with-shap",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "Adjusting concept drift with Shap",
    "text": "Adjusting concept drift with Shap\nAs well as adjusting the model’s predictions for subsets of the population, it is also possible to adjust the contribution a feature makes to the prediction in the first place. We can do this by using the Shap library to calculate the Shapley values for each feature and then adjusting them based on our intuition.\nFor instance, in this example we have fitted a TreeExplainer from the Shap library. We have then created a custom prediction function which will reduce the effect of customer’s tenure on the prediction probability that they will churn by half.\n\nencoded_value = pipeline.named_steps[\"preprocessor\"].transform(X_train[cols_to_use])\nexplainer = shap.TreeExplainer(pipeline.named_steps[\"classifier\"], data=encoded_value)\n\n\ndef prediction_with_less_effect_for_tenure(data):\n    tenure_column = 0\n    effect_reduction_size = 0.5\n\n    prediction = baseline_prediction(data)\n    encoded_data = pipeline.named_steps[\"preprocessor\"].transform(data[cols_to_use])\n    shap_values = explainer.shap_values(encoded_data)\n    effect_for_tenure = shap_values[:, tenure_column]\n    adjusted_predictions = prediction - effect_for_tenure * effect_reduction_size\n    adjusted_predictions = np.clip(adjusted_predictions, 0, 1)\n    return adjusted_predictions\n\nWe can get a high level visual impression of how these adjustments affect our predictions. You would probably want to do further analysis and monitor how these predictions would interact with your business decisions before putting them into production.\n\nfig, ax = plt.subplots(figsize=(4, 4))\nsns.kdeplot(baseline_prediction(X_test), label=\"Baseline\", ax=ax, clip=(0.0, 1.0))\nsns.kdeplot(prediction_adjust_DSL(X_test), label=\"DSL\", ax=ax, clip=(0.0, 1.0))\nsns.kdeplot(prediction_with_less_effect_for_tenure(X_test), label=\"Tenure\", ax=ax, clip=(0.0, 1.0))\n\nax.set_xlim(0, 1)\nax.set_xlabel(\"Predicted Probability\")\nax.set_ylabel(\"Density\")\nax.legend()\nplt.show()"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#the-sim2real-problem",
    "href": "08_Decision_Optimisation/drift.html#the-sim2real-problem",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "The Sim2Real Problem",
    "text": "The Sim2Real Problem\nHow should you make sure your simulation is useful in the real world?\n\nMake it better!\nThink of it like overfitting. Simpler decision frameworks are likely more robust\nValidate it with people close to the existing process\nTest it on a small scale first e.g. AB Testing\n\nWhen designing a test it is important to randomise but also to minimise “contamination”.\nFor instance, you might give a random set of people across all geographies a discount. However, if people who don’t get a discount see this going on they may react negatively which will affect your results. You could instead give a whole geography a discount and not another. However now you have reduced your randomisation.\nYou could randomise a store’s decision rule week by week. However, if the rule they followed last week will affect their behaviour this week you have again created contamination in your results.\nTrading these forces off against each other is a key part of designing a good test."
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#simulate---optimise---test",
    "href": "08_Decision_Optimisation/drift.html#simulate---optimise---test",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "Simulate - Optimise - Test",
    "text": "Simulate - Optimise - Test\n\ns = \"\"\"\nSimulate[shape=box label=\"Simulate\"]\nDecision[shape=box label=\"Choose decision rule\"]\nTest[shape=box label=\"Test\"]\nSimulate-&gt;Decision\nDecision-&gt;Test\nTest-&gt;Simulate\n\"\"\"\ngraphviz.Source(f'digraph G{{rankdir=\"LR\"; margin=0; pad=0; bgcolor=\"transparent\"; {s}}}')"
  },
  {
    "objectID": "08_Decision_Optimisation/drift.html#references",
    "href": "08_Decision_Optimisation/drift.html#references",
    "title": "Concept Drift and How Things Go Wrong",
    "section": "References",
    "text": "References\n\nMachine Learning for Business Decision Optimization - Dan Becker"
  },
  {
    "objectID": "Data Science From Scratch/gradient_descent.html",
    "href": "Data Science From Scratch/gradient_descent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "import random\nfrom typing import Callable, Iterator, List, Tuple\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nFrequently in data science, we try to the find the best model for a certain situation. Usually “best” means something like “minimises the error of its predictions” or “maximizes the likelihood of the data”. In other words, it will represent the solution of an optimisation problem. That might mean finding the input the produces the largest or smallest possible output, known as maximising or minimising a function.\nThe gradient is the vector of partial derivatives of a function. It gives the input direction in which the function most quickly increases. So one approach to maximising a function is to pick a random starting point, compute the gradient, take a small step in the direction of the gradient (i.e., the direction that causes the function to increase the most), and repeat with the new starting point.\ndef f(x: float, y: float) -&gt; float:\n    return x**2 + y**2\n\n\ndef f_derivative(x: float) -&gt; float:\n    return 2 * x\n\n\ndef difference_quotient(f: Callable[[float], float], x: float, y: float, h: float) -&gt; float:\n    return (f(x + h, y + h) - f(x, y)) / h\n\n\ndef gradient_descent(learning_rate: float, iterations: float, initial_point: Tuple[float, float]) -&gt; np.ndarray:\n    x, y = initial_point\n    history = [(x, y)]\n    for _ in range(iterations):\n        gradient_x = f_derivative(x)\n        gradient_y = f_derivative(y)\n\n        # gradient_x = difference_quotient(f, x, y, h=0.00001)\n        # gradient_y = difference_quotient(f, x, y, h=0.00001)\n\n        x -= learning_rate * gradient_x\n        y -= learning_rate * gradient_y\n        history.append((x, y))\n    return np.array(history)\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\nx_values = np.linspace(-10, 10, 100)\ny_values = np.linspace(-10, 10, 100)\nX, Y = np.meshgrid(x_values, y_values)\nZ = f(X, Y)\nax.plot_surface(X, Y, Z, alpha=0.5)\n\npath = gradient_descent(learning_rate=0.1, iterations=50, initial_point=(9, 9))\npath_z = f(path[:, 0], path[:, 1])\nax.plot(path[:, 0], path[:, 1], path_z, \"-r\", marker=\"o\")\n\nax.set_xmargin(0)\nax.set_ymargin(0)\nax.set_zmargin(0)\nax.set_xticks(np.arange(-10, 11, 5))\nax.set_yticks(np.arange(-10, 11, 5))\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nax.set_title(\"3D Gradient Descent Visualization\")\nplt.tight_layout()\nplt.show()\nIn the diagram above, the minimum is the global minimum. In a real example, the descent may get stuck in a local minimum. Lots of techniques like random restarts and momentum can help with this.\nIf \\(f\\) is a function of one variable, its derivative at a point \\(x\\) measures how \\(f(x)\\) changes when we make a very small change to \\(x\\). The derivative is defined as the limit of the difference quotients as \\(h\\) approaches \\(0\\). The derivative is the slope of the tangent to the graph of \\(f\\) at the point \\((x, f(x))\\). The difference quotient is the slope of the secant line through the points \\((x, f(x))\\) and \\((x + h, f(x + h))\\). We can see that as \\(h\\) approaches \\(0\\), the secant line approaches the tangent line, and the slope of the secant line approaches the slope of the tangent line, which is the derivative.\ndef f(x):\n    return x**2\n\n\ndef f_derivative(x):\n    return 2 * x\n\n\ndef difference_quotient(f, x, h):\n    return (f(x + h) - f(x)) / h\nx_values = np.linspace(-10, 10, 100)\nh = 0.5\n\nderivative_values = [f_derivative(x) for x in x_values]\ndiff_quot_values = [difference_quotient(f, x, h) for x in x_values]\nfig, ax = plt.subplots(figsize=(5, 5))\n\nax.plot(x_values, [f(x) for x in x_values], label=\"f(x) = x^2\")\nax.plot(x_values, derivative_values, label=\"f'(x) = 2x (True Derivative)\", linestyle=\"--\")\nax.plot(x_values, diff_quot_values, label=\"f'(x) ≈ Difference Quotient\", linestyle=\":\")\n\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Approximating a Derivative with a Difference Quotient\")\nax.legend()\nax.grid()\nax.set_xmargin(0)\nax.set_ymargin(0)\nplt.show()\nWhen \\(f\\) is a function of many variables, it has multiple partial derivatives, each indicating how \\(f\\) changes when we make small changes in just one of the input variables. We can calculate the \\(i\\)th partial derivative by treating it as a function of just its \\(i\\)-th variable, holding the other variables fixed.\ndef f(x, y):\n    return x**2 + y**2\ndef partial_derivative(f, point, var_idx, h):\n    point_plus_h = point.copy()\n    point_plus_h[var_idx] += h\n    return (f(*point_plus_h) - f(*point)) / h\nx_values = np.linspace(-10, 10, 25)\ny_values = np.linspace(-10, 10, 25)\nh = 0.01\n\nX, Y = np.meshgrid(x_values, y_values)\n\npartial_x = np.vectorize(lambda x, y: partial_derivative(f, [x, y], 0, h))(X, Y)\npartial_y = np.vectorize(lambda x, y: partial_derivative(f, [x, y], 1, h))(X, Y)\nZ = f(X, Y)\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111)\n\nax.contour(X, Y, Z, levels=20, alpha=0.5)\nax.quiver(X, Y, partial_x, partial_y, color=\"r\", headwidth=4, headlength=5, scale=50)\n\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Partial Derivatives of f(x, y) = x^2 + y^2\")\nax.grid()\nplt.show()\nQuiver plots display vector fields by drawing arrows to represent the vectors at various points in the 2D space. In the context of our example, the quiver plot is used to visualise the partial derivatives of the function \\(f(x, y) = x^2 + y^2\\) at various points in the \\(xy\\) plane.\nHere’s how you should interpret the quiver plot:\nIn the example given, the quiver plot shows the partial derivatives of \\(f(x, y) = x^2 + y^2\\). The arrows point away from the origin \\((0, 0)\\), because that is where the function has its minimum value, and they point in the direction of the steepest increase. The lengths of the arrows increase as they get further away from the origin, indicating that the rate of change of the function is higher at those points."
  },
  {
    "objectID": "Data Science From Scratch/gradient_descent.html#choosing-the-step-size",
    "href": "Data Science From Scratch/gradient_descent.html#choosing-the-step-size",
    "title": "Gradient Descent",
    "section": "Choosing the step size",
    "text": "Choosing the step size\nIt is easy to see in which direction we should move but choosing the size of the steps to take is less obvious. Too small, and gradient descent will take forever; too big, and we could take giant steps that overshoot the minimum and could make the function get larger or even be undefined. Options include:\n\nUsing a fixed step size\nGradually shrinking the step size over time\nAt each step, choosing the step size that minimizes the value of the objective function. This sounds great but is a costly computation."
  },
  {
    "objectID": "Data Science From Scratch/gradient_descent.html#using-gradient-descent-to-fit-models",
    "href": "Data Science From Scratch/gradient_descent.html#using-gradient-descent-to-fit-models",
    "title": "Gradient Descent",
    "section": "Using Gradient Descent to Fit Models",
    "text": "Using Gradient Descent to Fit Models\nParameterised Models\nA parameterised model is a mathematical or computational representation of a system or process that relies on parameters to describe its behavior, relationships, or properties. In the context of machine learning, statistics, and data science, these models are used to represent complex relationships between variables and predict outcomes based on input data.\nParameters are variables in the model that can be adjusted or learned from data to optimise the model’s performance. The process of adjusting these parameters to minimise the error between the model’s predictions and the actual observed data is called model fitting or training.\nFor example, in a linear regression model, the parameters are the coefficients of the independent variables (slope) and the intercept. These parameters are adjusted to find the best-fitting line that minimises the difference between the predicted and observed values.\nParameterised models can be found in various forms, such as linear models, neural networks, decision trees, and support vector machines, to name a few. The choice of model and the number of parameters usually depend on the problem at hand, the complexity of the relationships between variables, and the available data.\nIn this example we have some input data xs between -50 and 50.\nWe’ll be fitting a simple linear regression model to this data. It takes the form of the equation\n\\[y = mx + b\\]\nwhere parameter \\(m\\) is the slope of our linear relationship and parameter \\(b\\) is the intercept.\nThe ys are also synthetically generated so we can easily see when the model is perfectly fitted.\n\nslope = 30\nintercept = 7\n\nxs = np.array([x for x in range(-50, 50)])\nys = np.array([slope * x + intercept for x in xs])\n\nWe will now use gradient descent to learn what the values of \\(m\\) and \\(b\\) parameters should be. It does this by adjusting the parameters in a direction the will minimise the mean squared error of the model. The formula for mean squared error is\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y_i})^2\\]\nwhere:\n\n\\(y_i\\) is the actual value of the \\(i\\)-th data point\n\\(\\hat{y_i}\\) is the predicted value of the \\(i\\)-th data point, or \\(mx_i + b\\)\n\nWe can rewrite this equation as\n\\[f(m, b) = \\frac{1}{n}\\sum_{i=1}^n(y_i - (mx_i + b))^2\\]\nThe MSE can tell us how good our model with its current parameters is. But in order to know how to adjust the parameters to improve the model, we need to take the partial derivatives of the MSE with respect to those parameters. This will tell us the MSE’s gradient. The gradient is a vector that points in the direction of greatest increase of the function. So if we take a small step in the direction of the gradient, we should be able to slightly improve the model. We can repeat this process until the gradient is zero, which means we have found the minimum of the function.\nWe need to use the chain rule to find the partial derivatives. The formula for the chain rule is:\n\\[f(g(x))' = f'(g(x))g'(x)\\]\nwhere f is the outer function and g is the inner function.\nIn our case, the outer function \\(()^2\\) and the inner function is \\(y - mx + b\\). Therefore:\n\\[f'(m) = 2x\\]\nTo take the partial derivate of \\(g(x)\\) with respect to \\(m\\), we treat everything that is not \\(m\\) as a constant equal to 0. We derive the resulting \\(0 - (mx + 0)\\) normally. The derivative of \\(mx\\) is just \\(x\\). Therefore:\n\\[g'(m) = -x\\]\nPlugging these values into the chain rule, we get:\n\\[f(g(m))' = -2x(y - mx + b)\\]\nWe can repeat the process with respect to \\(b\\). Setting everything that is not \\(b\\) to 0. So \\(y - mx + b\\) becomes \\((0 - (0 + b))\\). The derivative of \\(b\\) is 1. Therefore:\n\\[f'(b) = 2x\\]\n\\[g'(b) = -1\\]\nPlugging these values into the chain rule, we get:\n\\[f(g(b))' = -2(y - mx + b)\\]\nWe can now see how this would get implemented in code. \\(y - mx + b\\) is effectively the error of the model. So we will calculate that each epoch and plug it into the partial derivative formulae above.\n\ndef linear_gradient(xs: np.ndarray, ys: np.ndarray, params: List[float]) -&gt; List[float]:\n    slope, intercept = params\n    predicted = slope * xs + intercept\n    error = predicted - ys\n    grad = [2 * error * xs, 2 * error]\n    return grad\n\nWe begin by randomly initialising the parameters \\(m\\) and \\(b\\).\nIn each epoch we then:\n\nCalculate the error of the model\nUse the error to find the gradients\nAdjust the parameters in the direction of the gradients, mediated by the learning rate\n\n\nlearning_rate = 0.001\nparams = random.uniform(-1, 1), random.uniform(-1, 1)\nfor epoch in range(5001):\n    grads = np.array([linear_gradient(x, y, params) for x, y in zip(xs, ys)])\n    grads_mean = np.mean(grads, axis=0)\n    params = -learning_rate * grads_mean + params\n    if epoch % 500 == 0:\n        print(epoch, params)\n\n0 [49.79222779 -0.12864659]\n500 [29.99842866  4.38372465]\n1000 [29.99942234  6.0382    ]\n1500 [29.99978764  6.6464213 ]\n2000 [29.99992193  6.87001674]\n2500 [29.9999713   6.95221531]\n3000 [29.99998945  6.9824333 ]\n3500 [29.99999612  6.9935421 ]\n4000 [29.99999857  6.99762593]\n4500 [29.99999948  6.99912724]\n5000 [29.99999981  6.99967915]\n\n\n\nMinibatch Gradient Descent\nIf our dataset was bigger, calculating the average gradient across the entire dataset would be too computationally expensive. We can instead calculate the average gradient across a random sample of the dataset. This is called minibatch gradient descent.\n\ndef minibatches(dataset: np.ndarray, batch_size: int, shuffle: bool = True) -&gt; Iterator[np.ndarray]:\n    batch_starts = [start for start in range(0, dataset.shape[0], batch_size)]\n    if shuffle:\n        random.shuffle(batch_starts)\n    for start in batch_starts:\n        end = start + batch_size\n        yield dataset[start:end]\n\n\nlearning_rate = 0.001\nparams = random.uniform(-1, 1), random.uniform(-1, 1)\nfor epoch in range(1001):\n    for batch in minibatches(np.array(list(zip(xs, ys))), batch_size=10):\n        grads = np.array([linear_gradient(x, y, params) for x, y in batch])\n        grads_mean = np.mean(grads, axis=0)\n        params = -learning_rate * grads_mean + params\n    if epoch % 100 == 0:\n        print(epoch, params)\n\n0 [20.37835254 -2.84812216]\n100 [30.02735882  6.66564635]\n200 [30.00638674  6.97173028]\n300 [30.00075672  6.99673374]\n400 [29.99990046  6.99961018]\n500 [29.99999952  6.99999153]\n600 [30.00000002  6.99999986]\n700 [29.99999999  6.99999996]\n800 [30.  7.]\n900 [30.  7.]\n1000 [30.  7.]\n\n\n\n\nStochastic Gradient Descent\nIf the batch size was 1, we would would be taking gradient steps based on one training example at a time.\n\nlearning_rate = 0.001\nparams = random.uniform(-1, 1), random.uniform(-1, 1)\nfor epoch in range(1001):\n    for x, y in zip(xs, ys):\n        grads = np.array([linear_gradient(x, y, params)])\n        grads_mean = np.mean(grads, axis=0)\n        params = -learning_rate * grads_mean + params\n    if epoch % 100 == 0:\n        print(epoch, params)\n\n0 [30.1538657  -0.65822025]\n100 [30.00191734  6.90456916]\n200 [30.00002389  6.99881082]\n300 [30.0000003   6.99998518]\n400 [30.00000001  6.99999982]\n500 [30.  7.]\n600 [30.00000001  7.        ]\n700 [30.00000001  7.        ]\n800 [30.  7.]\n900 [30.00000002  7.        ]\n1000 [30.00000001  7.        ]\n\n\nThis is known as stochastic gradient descent (although somtimes minibatch gradient descent is also called this). For our extremely simple example it finds the optimal parameters very quickly. In more complicated examples the gradient for a single example might not be quite different to the gradient for the entire dataset. So we might not find the optimal parameters as quickly. We also miss out on the computational efficiency vectorisation our computations."
  },
  {
    "objectID": "Data Science From Scratch/gradient_descent.html#references",
    "href": "Data Science From Scratch/gradient_descent.html#references",
    "title": "Gradient Descent",
    "section": "References",
    "text": "References\n\nGradient Descent From Scratch - Arseny Turin\nData Science from Scratch 2nd Edition (Chapter 8) - Joel Grus"
  },
  {
    "objectID": "05_Interpretability/ale_plots.html",
    "href": "05_Interpretability/ale_plots.html",
    "title": "ALE PLot",
    "section": "",
    "text": "Accumulated local effects describe how features influence the prediction of a machine learning model on average. ALE plots are a faster and unbiased alternative to partial dependence plots (PDPs).\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml, load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom PyALE import ale\nimport matplotlib.pyplot as plt\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\")\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n\nX = X[categorical_columns + numerical_columns]\ny = y.replace({\"0\": 0, \"1\": 1})\n\n# Remove rows with missing values in X and y\nmissing_mask = X.isna().any(axis=1)\n\nX = X[~missing_mask]\ny = y[~missing_mask]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n\nX\n\n\n\n\n\n\n\n\npclass\nsex\nembarked\nage\nsibsp\nparch\nfare\n\n\n\n\n0\n1\nfemale\nS\n29.0000\n0\n0\n211.3375\n\n\n1\n1\nmale\nS\n0.9167\n1\n2\n151.5500\n\n\n2\n1\nfemale\nS\n2.0000\n1\n2\n151.5500\n\n\n3\n1\nmale\nS\n30.0000\n1\n2\n151.5500\n\n\n4\n1\nfemale\nS\n25.0000\n1\n2\n151.5500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1301\n3\nmale\nC\n45.5000\n0\n0\n7.2250\n\n\n1304\n3\nfemale\nC\n14.5000\n1\n0\n14.4542\n\n\n1306\n3\nmale\nC\n26.5000\n0\n0\n7.2250\n\n\n1307\n3\nmale\nC\n27.0000\n0\n0\n7.2250\n\n\n1308\n3\nmale\nS\n29.0000\n0\n0\n7.8750\n\n\n\n\n1043 rows × 7 columns\n\n\n\n\ncategorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nrf.set_output(transform=\"pandas\")\nrf.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex',\n                                                   'embarked']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nfrom sklearn.metrics import classification_report\n\ntest_preds = rf.predict_proba(X_test)[:, 1]\n\nprint(classification_report(y_test, test_preds &gt; 0.5))\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.81      0.80       155\n           1       0.71      0.71      0.71       106\n\n    accuracy                           0.77       261\n   macro avg       0.76      0.76      0.76       261\nweighted avg       0.77      0.77      0.77       261\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.histplot(test_preds, bins=20, kde=True, ax=ax)\nax.set_xlabel(\"Predicted probability of survival\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Distribution of predicted probabilities\")\nax.set_xmargin(0)\nplt.show()\n\n\n\n\n\nclass clf_dummy:\n    def predict(df):\n        return rf.named_steps[\"classifier\"].predict_proba(df)[:, 1]\n\n\nX_test_transformed = pd.DataFrame(\n    rf.named_steps[\"preprocess\"].transform(X_test),\n    columns=rf.named_steps[\"preprocess\"].get_feature_names_out(),\n)\n\n\nfig, axes = plt.subplots(3, 3, figsize=(20, 12))\naxes = axes.flatten()\n\nfor i, col in enumerate(X_test_transformed.columns):\n    if col in numerical_columns:\n        ale(X=X_test_transformed, model=clf_dummy, feature=[col], feature_type=\"continuous\", fig=fig, ax=axes[i])\n    elif col in categorical_columns:\n        ale(X=X_test_transformed, model=clf_dummy, feature=[col], feature_type=\"discrete\", fig=fig, ax=axes[i])\n        category_names = rf.named_steps[\"preprocess\"].transformers_[0][1].categories_[i]\n        axes[i].set_xticks(axes[i].get_xticks())\n        axes[i].set_xticklabels(category_names)\n    else:\n        continue\n\nfor ax in fig.axes:\n    ax.set_title(\"\")\n    ax.tick_params(axis=\"y\", labelcolor=\"black\")\n    ax.set_ylabel(\"\")\n    ax.set_xmargin(0)\n\nfor col, ax in zip(X_test_transformed.columns, fig.axes):\n    ax.set_title(col)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "05_Interpretability/feature_importance.html",
    "href": "05_Interpretability/feature_importance.html",
    "title": "Feature Importance",
    "section": "",
    "text": "In this example, we will compare the impurity-based feature importance of RandomForestClassifier with the permutation importance on the titanic dataset using permutation_importance. We will show that the impurity-based feature importance can inflate the importance of numerical features.\nFurthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.\nThis example shows how to use Permutation Importances as an alternative that can mitigate those limitations.\n\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import spearmanr\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import squareform, pdist\nfrom sklearn.datasets import fetch_openml, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.inspection import permutation_importance\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\n\n\nLet’s use pandas to load a copy of the titanic dataset. The following shows how to apply separate preprocessing on numerical and categorical features.\nWe further include two random variables that are not correlated in any way with the target variable (survived):\nrandom_num is a high cardinality numerical variable (as many unique values as records).\nrandom_cat is a low cardinality categorical variable (3 possible values).\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\")\nrng = np.random.RandomState(seed=42)\nX[\"random_cat\"] = rng.randint(3, size=X.shape[0])\nX[\"random_num\"] = rng.randn(X.shape[0])\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\", \"random_cat\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\", \"random_num\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\nWe define a predictive model based on a random forest. Therefore, we will make the following preprocessing steps:\n\nuse OrdinalEncoder to encode the categorical features;\nuse SimpleImputer to fill missing values for numerical features using a mean strategy.\n\n\ncategorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked', 'random_cat']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare',\n                                  'random_num'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked', 'random_cat']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare', 'random_num']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\n\n\nPrior to inspecting the feature importances, it is important to check that the model predictive performance is high enough. Indeed there would be little interest of inspecting the important features of a non-predictive model.\nHere one can observe that the train accuracy is very high (the forest model has enough capacity to completely memorize the training set) but it can still generalize well enough to the test set thanks to the built-in bagging of random forests.\nIt might be possible to trade some accuracy on the training set for a slightly better accuracy on the test set by limiting the capacity of the trees (for instance by setting min_samples_leaf=5 or min_samples_leaf=10) so as to limit overfitting while not introducing too much underfitting.\nHowever let’s keep our high capacity random forest model for now so as to illustrate some pitfalls with feature importance on variables with many unique values.\n\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\n\nRF train accuracy: 1.000\nRF test accuracy: 0.814\n\n\n\n\n\nThe impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive random_num variable is ranked as one of the most important features!\nThis problem stems from two limitations of impurity-based feature importances:\nimpurity-based importances are biased towards high cardinality features;\nimpurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).\nThe bias towards high cardinality features explains why random_num has a really large importance in comparison with random_cat while we would expect both random features to have a null importance.\nThe fact that we use training set statistics explains why both the random_num and random_cat features have a non-null importance.\n\nfeature_names = rf[:-1].get_feature_names_out()\nmdi_importances = pd.Series(rf[-1].feature_importances_, index=feature_names).sort_values(ascending=True)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.set_title(\"Random Forest Feature Importances (MDI)\")\nmdi_importances.plot.barh(ax=ax)\nplt.show()\n\n\n\n\nAs an alternative, the permutation importances of rf are computed on a held out test set. This shows that the low cardinality categorical features sex and pclass are the most important features. Indeed, permuting the values of these features will lead to most decrease in accuracy score of the model on the test set.\nAlso note that both random features have very low importances (close to 0) as expected.\n\nresult = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.set_title(\"Permutation Importances (test set)\")\nimportances.plot.box(vert=False, whis=10, ax=ax)\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nplt.show()\n\n\n\n\nIt is also possible to compute the permutation importances on the training set. This reveals that random_num and random_cat get a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the RF model has enough capacity to use those random numerical and categorical features to overfit.\n\nresult = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.set_title(\"Permutation Importances (train set)\")\nax = importances.plot.box(vert=False, whis=10, ax=ax)\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nplt.show()\n\n\n\n\nWe can retry the experiment by limiting the capacity of the trees to overfit by setting min_samples_leaf to 20 data points.\n\nrf.set_params(classifier__min_samples_leaf=20).fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestClassifier(min_samples_leaf=20, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestClassifier(min_samples_leaf=20, random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked', 'random_cat']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare',\n                                  'random_num'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked', 'random_cat']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare', 'random_num']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(min_samples_leaf=20, random_state=42)\n\n\nObserving the accuracy score on the training and testing set, we observe that the two metrics are very similar now. Therefore, our model is not overfitting anymore. We can then check the permutation importances with this new model.\n\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\n\nRF train accuracy: 0.810\nRF test accuracy: 0.832\n\n\n\ntrain_result = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)\ntest_results = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\nsorted_importances_idx = train_result.importances_mean.argsort()\n\n\ntrain_importances = pd.DataFrame(\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = pd.DataFrame(\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True, sharex=True)\nfig.tight_layout()\nfor name, importances, ax in zip([\"train\", \"test\"], [train_importances, test_importances], axes.flatten()):\n    ax = importances.plot.box(vert=False, whis=10, ax=ax)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\nplt.show()\n\n\n\n\nNow, we can observe that on both sets, the random_num and random_cat features have a lower importance compared to the overfitting random forest. However, the conclusions regarding the importance of the other features are still valid."
  },
  {
    "objectID": "05_Interpretability/feature_importance.html#permutation-importance-vs-random-forest-feature-importance-mdi",
    "href": "05_Interpretability/feature_importance.html#permutation-importance-vs-random-forest-feature-importance-mdi",
    "title": "Feature Importance",
    "section": "",
    "text": "In this example, we will compare the impurity-based feature importance of RandomForestClassifier with the permutation importance on the titanic dataset using permutation_importance. We will show that the impurity-based feature importance can inflate the importance of numerical features.\nFurthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.\nThis example shows how to use Permutation Importances as an alternative that can mitigate those limitations.\n\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import spearmanr\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import squareform, pdist\nfrom sklearn.datasets import fetch_openml, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.inspection import permutation_importance\n\npd.set_option(\"display.max_columns\", None)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\n\n\n\nLet’s use pandas to load a copy of the titanic dataset. The following shows how to apply separate preprocessing on numerical and categorical features.\nWe further include two random variables that are not correlated in any way with the target variable (survived):\nrandom_num is a high cardinality numerical variable (as many unique values as records).\nrandom_cat is a low cardinality categorical variable (3 possible values).\n\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\")\nrng = np.random.RandomState(seed=42)\nX[\"random_cat\"] = rng.randint(3, size=X.shape[0])\nX[\"random_num\"] = rng.randn(X.shape[0])\n\ncategorical_columns = [\"pclass\", \"sex\", \"embarked\", \"random_cat\"]\nnumerical_columns = [\"age\", \"sibsp\", \"parch\", \"fare\", \"random_num\"]\n\nX = X[categorical_columns + numerical_columns]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\nWe define a predictive model based on a random forest. Therefore, we will make the following preprocessing steps:\n\nuse OrdinalEncoder to encode the categorical features;\nuse SimpleImputer to fill missing values for numerical features using a mean strategy.\n\n\ncategorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\nnumerical_pipe = SimpleImputer(strategy=\"mean\")\n\npreprocessing = ColumnTransformer(\n    [\n        (\"cat\", categorical_encoder, categorical_columns),\n        (\"num\", numerical_pipe, numerical_columns),\n    ],\n    verbose_feature_names_out=False,\n)\n\nrf = Pipeline(\n    [\n        (\"preprocess\", preprocessing),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nrf.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier', RandomForestClassifier(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked', 'random_cat']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare',\n                                  'random_num'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked', 'random_cat']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare', 'random_num']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\n\n\nPrior to inspecting the feature importances, it is important to check that the model predictive performance is high enough. Indeed there would be little interest of inspecting the important features of a non-predictive model.\nHere one can observe that the train accuracy is very high (the forest model has enough capacity to completely memorize the training set) but it can still generalize well enough to the test set thanks to the built-in bagging of random forests.\nIt might be possible to trade some accuracy on the training set for a slightly better accuracy on the test set by limiting the capacity of the trees (for instance by setting min_samples_leaf=5 or min_samples_leaf=10) so as to limit overfitting while not introducing too much underfitting.\nHowever let’s keep our high capacity random forest model for now so as to illustrate some pitfalls with feature importance on variables with many unique values.\n\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\n\nRF train accuracy: 1.000\nRF test accuracy: 0.814\n\n\n\n\n\nThe impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive random_num variable is ranked as one of the most important features!\nThis problem stems from two limitations of impurity-based feature importances:\nimpurity-based importances are biased towards high cardinality features;\nimpurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).\nThe bias towards high cardinality features explains why random_num has a really large importance in comparison with random_cat while we would expect both random features to have a null importance.\nThe fact that we use training set statistics explains why both the random_num and random_cat features have a non-null importance.\n\nfeature_names = rf[:-1].get_feature_names_out()\nmdi_importances = pd.Series(rf[-1].feature_importances_, index=feature_names).sort_values(ascending=True)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.set_title(\"Random Forest Feature Importances (MDI)\")\nmdi_importances.plot.barh(ax=ax)\nplt.show()\n\n\n\n\nAs an alternative, the permutation importances of rf are computed on a held out test set. This shows that the low cardinality categorical features sex and pclass are the most important features. Indeed, permuting the values of these features will lead to most decrease in accuracy score of the model on the test set.\nAlso note that both random features have very low importances (close to 0) as expected.\n\nresult = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.set_title(\"Permutation Importances (test set)\")\nimportances.plot.box(vert=False, whis=10, ax=ax)\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nplt.show()\n\n\n\n\nIt is also possible to compute the permutation importances on the training set. This reveals that random_num and random_cat get a significantly higher importance ranking than when computed on the test set. The difference between those two plots is a confirmation that the RF model has enough capacity to use those random numerical and categorical features to overfit.\n\nresult = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax.set_title(\"Permutation Importances (train set)\")\nax = importances.plot.box(vert=False, whis=10, ax=ax)\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nplt.show()\n\n\n\n\nWe can retry the experiment by limiting the capacity of the trees to overfit by setting min_samples_leaf to 20 data points.\n\nrf.set_params(classifier__min_samples_leaf=20).fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestClassifier(min_samples_leaf=20, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OrdinalEncoder(encoded_missing_value=-1,\n                                                                 handle_unknown='use_encoded_value',\n                                                                 unknown_value=-1),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'random_cat']),\n                                                 ('num', SimpleImputer(),\n                                                  ['age', 'sibsp', 'parch',\n                                                   'fare', 'random_num'])],\n                                   verbose_feature_names_out=False)),\n                ('classifier',\n                 RandomForestClassifier(min_samples_leaf=20, random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OrdinalEncoder(encoded_missing_value=-1,\n                                                handle_unknown='use_encoded_value',\n                                                unknown_value=-1),\n                                 ['pclass', 'sex', 'embarked', 'random_cat']),\n                                ('num', SimpleImputer(),\n                                 ['age', 'sibsp', 'parch', 'fare',\n                                  'random_num'])],\n                  verbose_feature_names_out=False)cat['pclass', 'sex', 'embarked', 'random_cat']OrdinalEncoderOrdinalEncoder(encoded_missing_value=-1, handle_unknown='use_encoded_value',\n               unknown_value=-1)num['age', 'sibsp', 'parch', 'fare', 'random_num']SimpleImputerSimpleImputer()RandomForestClassifierRandomForestClassifier(min_samples_leaf=20, random_state=42)\n\n\nObserving the accuracy score on the training and testing set, we observe that the two metrics are very similar now. Therefore, our model is not overfitting anymore. We can then check the permutation importances with this new model.\n\nprint(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")\n\nRF train accuracy: 0.810\nRF test accuracy: 0.832\n\n\n\ntrain_result = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)\ntest_results = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\nsorted_importances_idx = train_result.importances_mean.argsort()\n\n\ntrain_importances = pd.DataFrame(\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = pd.DataFrame(\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True, sharex=True)\nfig.tight_layout()\nfor name, importances, ax in zip([\"train\", \"test\"], [train_importances, test_importances], axes.flatten()):\n    ax = importances.plot.box(vert=False, whis=10, ax=ax)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\nplt.show()\n\n\n\n\nNow, we can observe that on both sets, the random_num and random_cat features have a lower importance compared to the overfitting random forest. However, the conclusions regarding the importance of the other features are still valid."
  },
  {
    "objectID": "05_Interpretability/feature_importance.html#permutation-importance-with-multicollinear-or-correlated-features",
    "href": "05_Interpretability/feature_importance.html#permutation-importance-with-multicollinear-or-correlated-features",
    "title": "Feature Importance",
    "section": "Permutation Importance with Multicollinear or Correlated Features",
    "text": "Permutation Importance with Multicollinear or Correlated Features\nWhen features are collinear, permuting one feature will have little effect on the model’s performance because it can get the same information from other correlated features. One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.\n\nHandling Multicollinear Features\nThis is quite a fiddly process so we’ll break it down step by step:\n\nSpearman’s Rank-Order Correlation\nThe spearmanr function from the scipy.stats module calculates Spearman rank-order correlation coefficient between two data arrays, or across the rows or columns of a 2-D array. This is a measure of the strength and direction of association between two ranked variables.\nHere’s the basic idea:\nRanking the Data: Spearman’s correlation is based on the ranks of the data, rather than their raw values. Each value in each variable is replaced by its rank, including dealing with tied ranks.\nCalculating the Correlation: Once the data are ranked, the same formula as Pearson’s correlation coefficient is used, with the ranks replacing the raw data.\nThe formula to compute the Spearman’s correlation coefficient \\(\\rho\\) is:\n\\[\n\\rho = 1 - \\frac{6 \\sum{d_i^2}}{n(n² - 1)}\n\\]\nwhere \\(d_i\\) are the differences between the ranks of corresponding variables, and \\(n\\) is the number of observations.\nThe output is a correlation matrix if the input is a 2D array, or a correlation coefficient and p-value if the input consists of two 1D arrays.\nThe correlation matrix \\(\\rho\\) is a square symmetric matrix with ones on the diagonal (as each variable is perfectly correlated with itself), and the off-diagonal elements are the Spearman correlation coefficients between different variables. The corresponding p-values matrix can also be returned, which tells you the significance of the correlations.\n\nnp.random.seed(0)\nX = np.random.rand(10)\nY = np.random.rand(10)\n\nrho, pval = spearmanr(X, Y)\n\nprint(f\"Spearman's correlation coefficient: {rho}\")\nprint(f\"P-value: {pval}\")\n\nSpearman's correlation coefficient: 0.04242424242424241\nP-value: 0.907363817812816\n\n\nWe can pass a 2-D matrix to spearmanr and calculate the correlation coefficient across the rows or columns of it.\n\nnp.random.seed(0)\nX = np.random.rand(10, 5)\n\nrho, pval = spearmanr(X)\n\nEach element in the rho matrix represents the correlation coefficient between two variables (i.e., columns of the input matrix). The rho matrix is symmetric, with a diagonal of ones, because each variable is perfectly correlated with itself.\n\nprint(rho.shape)\nrho\n\n(5, 5)\n\n\narray([[ 1.     ,  0.24848, -0.27273,  0.28485, -0.51515],\n       [ 0.24848,  1.     , -0.53939,  0.33333, -0.50303],\n       [-0.27273, -0.53939,  1.     ,  0.13939,  0.61212],\n       [ 0.28485,  0.33333,  0.13939,  1.     , -0.09091],\n       [-0.51515, -0.50303,  0.61212, -0.09091,  1.     ]])\n\n\nThe pval matrix represents the p-values associated with each correlation coefficient. A small p-value indicates that the correlation coefficient is statistically significant.\n\nnp.set_printoptions(suppress=True, precision=5)\nprint(pval)\nnp.set_printoptions()\n\n[[0.      0.48878 0.44584 0.42504 0.12755]\n [0.48878 0.      0.10759 0.34659 0.13833]\n [0.44584 0.10759 0.      0.70093 0.05997]\n [0.42504 0.34659 0.70093 0.      0.80277]\n [0.12755 0.13833 0.05997 0.80277 0.     ]]\n\n\n\n\nScipy’s Squareform Function\nThe scipy.spatial.distance.squareform function is used for converting between a square, symmetric 2-D array (a full distance matrix) and a 1-D condensed distance matrix, and vice versa.\nIn the context of a full distance matrix, we can consider a 2-D array where the [i, j] entry represents the distance between the i-th and j-th elements. Since the distance from i to j is the same as from j to i, this matrix is symmetric. Additionally, the diagonal elements of the matrix (distance of elements to themselves) are always 0.\nIn a condensed distance matrix, we flatten the full matrix into a 1-D array. Here, we only keep one of the i, j or j, i pairs (since the matrix is symmetric, they’re redundant), and we don’t include the diagonal elements (since they’re always 0). This saves space and can be more efficient for certain computations.\nIn general, the squareform function works as follows:\n\nFrom a full matrix to a condensed matrix:\nWhen the input is a 2-D square symmetric matrix, it returns a 1-D array containing the elements below the main diagonal. This is useful for saving space when you have a full distance matrix and you want to eliminate redundant information.\nFrom a condensed matrix to a full matrix:\nWhen the input is a 1-D condensed distance matrix, it returns a square symmetric 2-D distance matrix. This is useful when you have a condensed distance matrix and you want to visualize it or use it in computations that require a full matrix.\n\n\narr = np.array([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\narr\n\narray([[0, 1, 2],\n       [1, 0, 3],\n       [2, 3, 0]])\n\n\n\ncondensed = squareform(arr)\ncondensed\n\narray([1, 2, 3])\n\n\n\nfull = squareform(condensed)\nfull\n\narray([[0, 1, 2],\n       [1, 0, 3],\n       [2, 3, 0]])\n\n\n\n\npdist\nThe pdist function from scipy.spatial.distance is used to compute the pairwise distances between observations in the input array X.\npdist stands for “pairwise distance”. It takes a 2-D array where each row is an observation and each column is a feature of that observation, and it computes the distance between every pair of observations. This is also known as a condensed distance matrix.\nBy default, pdist uses the Euclidean distance (also known as the L2 norm or 2-norm distance), but it can also use other distance metrics. The distance metric can be specified via the metric parameter.\n\nX = np.array([[1, 2], [3, 4], [5, 6]])\nX\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\nFor this small matrix we can calculate the pairwise distance by hand to see what pdist does for us:\n\\[\n\\begin{align}\nd_1 &= \\sqrt{(3-1)^2 + (4-2)^2} \\\\\n&= \\sqrt{8} \\\\\n&= 2.828 \\\\\nd_2 &= \\sqrt{(5-1)^2 + (6-2)^2} \\\\\n&= \\sqrt{32} \\\\\n&= 5.657 \\\\\nd_3 &= \\sqrt{(5-3)^2 + (6-4)^2} \\\\\n&= \\sqrt{8} \\\\\n&= 2.828\n\\end{align}\n\\]\nThe number of pairwise distances is determined by the by the number of pairs of observations (rows) in X. If X has n rows there will be \\(\\frac{n(n-1)}{2}\\) pairs of observations. So there the output of pdist will be an array d of size \\(\\frac{n(n-1)}{2}\\). The size of d grows quadratically with n.\n\nD = pdist(X)\nD\n\narray([3.46410162, 6.92820323, 3.46410162])\n\n\n\n\nWard’s Method\n\n# Create a random sample data\nnp.random.seed(0)\nX = np.random.random((10, 3))\nX\n\narray([[0.5488135 , 0.71518937, 0.60276338],\n       [0.54488318, 0.4236548 , 0.64589411],\n       [0.43758721, 0.891773  , 0.96366276],\n       [0.38344152, 0.79172504, 0.52889492],\n       [0.56804456, 0.92559664, 0.07103606],\n       [0.0871293 , 0.0202184 , 0.83261985],\n       [0.77815675, 0.87001215, 0.97861834],\n       [0.79915856, 0.46147936, 0.78052918],\n       [0.11827443, 0.63992102, 0.14335329],\n       [0.94466892, 0.52184832, 0.41466194]])\n\n\n\n# Calculate the pairwise distance of X\ndistance_matrix = pdist(X)\ndistance_matrix\n\narray([0.29473397, 0.41689499, 0.19662693, 0.57216693, 0.86543108,\n       0.46672837, 0.398299  , 0.63410319, 0.47902444, 0.57586803,\n       0.41860234, 0.76350759, 0.63809564, 0.60361979, 0.29019522,\n       0.69376753, 0.47216408, 0.44940452, 0.90274337, 0.94847268,\n       0.34159159, 0.59112128, 0.91558599, 0.83389329, 0.51150232,\n       0.88049546, 0.6034734 , 0.5875395 , 0.49193536, 0.63313412,\n       1.27710576, 0.93324293, 0.87874855, 0.53771137, 0.65033634,\n       1.10498141, 0.83929158, 0.92741024, 1.07781983, 0.4545101 ,\n       1.08906121, 0.68336753, 0.94944076, 0.3983422 , 0.87776842])\n\n\nThe linkage matrix generated by the ward function (or any other linkage method in scipy.cluster.hierarchy) is an array with the shape (n-1, 4) where n is the number of observations in your input data. Each row in this matrix represents a single agglomeration (merge) event between two clusters during the hierarchical clustering process.\nHere’s how to interpret each column in the linkage matrix:\n\nColumn 0 and Column 1: These columns represent the indices of the two clusters being merged in each agglomeration step. For the first n steps, where n is the number of input observations, the indices correspond to the input observations themselves. After the first n steps, the indices correspond to the newly-formed clusters, with indices starting from n and incrementing for each new cluster.\nColumn 2: This column represents the distance between the two clusters being merged. In the case of Ward’s method, this distance is calculated based on the increase in the total within-cluster variance after merging the two clusters.\nColumn 3: This column contains the number of observations in the newly-formed cluster after the merge.\n\n\n# Perform Ward's method clustering\nlinkage_matrix = hierarchy.ward(distance_matrix)\nlinkage_matrix\n\narray([[ 0.        ,  3.        ,  0.19662693,  2.        ],\n       [ 1.        ,  7.        ,  0.29019522,  2.        ],\n       [ 2.        ,  6.        ,  0.34159159,  2.        ],\n       [ 9.        , 11.        ,  0.47575119,  3.        ],\n       [ 4.        ,  8.        ,  0.53771137,  2.        ],\n       [10.        , 12.        ,  0.63341709,  4.        ],\n       [13.        , 15.        ,  0.82022065,  7.        ],\n       [ 5.        , 16.        ,  1.13447788,  8.        ],\n       [14.        , 17.        ,  1.21509521, 10.        ]])\n\n\nThe dendrogram method plots these merge events as a binary tree. Each leaf of the tree corresponds to one of the original observations. Each internal node of the tree corresponds to a merge event, with its two children being the clusters that were merged.\nThe y-coordinate of each internal node is the distance at which the corresponding merge event occurred (the third column of the corresponding row in the linkage matrix). This means that the height of each internal node in the dendrogram corresponds to the dissimilarity between the two clusters that it merged.\nThe x-coordinates of the nodes are chosen to place all leaves at the bottom of the plot and to avoid overlapping as much as possible.\n\n# Plot the hierarchical clustering as a dendrogram.\nhierarchy.dendrogram(linkage_matrix)\nplt.show()\n\n\n\n\n\n\n\nExample\nIn this example, we compute the permutation importance on the Wisconsin breast cancer dataset using permutation_importance. The RandomForestClassifier can easily get about 97% accuracy on a test dataset.\n\ndata = load_breast_cancer()\n\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target)\n\n\nX\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n1.0950\n0.9053\n8.589\n153.40\n0.006399\n0.04904\n0.05373\n0.01587\n0.03003\n0.006193\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n0.5435\n0.7339\n3.398\n74.08\n0.005225\n0.01308\n0.01860\n0.01340\n0.01389\n0.003532\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n0.7456\n0.7869\n4.585\n94.03\n0.006150\n0.04006\n0.03832\n0.02058\n0.02250\n0.004571\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n0.4956\n1.1560\n3.445\n27.23\n0.009110\n0.07458\n0.05661\n0.01867\n0.05963\n0.009208\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n0.7572\n0.7813\n5.438\n94.44\n0.011490\n0.02461\n0.05688\n0.01885\n0.01756\n0.005115\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n1.1760\n1.2560\n7.673\n158.70\n0.010300\n0.02891\n0.05198\n0.02454\n0.01114\n0.004239\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n0.7655\n2.4630\n5.203\n99.04\n0.005769\n0.02423\n0.03950\n0.01678\n0.01898\n0.002498\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n0.4564\n1.0750\n3.425\n48.55\n0.005903\n0.03731\n0.04730\n0.01557\n0.01318\n0.003892\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n0.7260\n1.5950\n5.772\n86.22\n0.006522\n0.06158\n0.07117\n0.01664\n0.02324\n0.006185\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n0.3857\n1.4280\n2.548\n19.15\n0.007189\n0.00466\n0.00000\n0.00000\n0.02676\n0.002783\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows × 30 columns\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\nprint(f\"Accuracy on test data: {clf.score(X_test, y_test):.2f}\")\n\nAccuracy on test data: 0.97\n\n\nNext, we plot the tree based feature importance and the permutation importance.\nBecause this dataset contains multicollinear features, the permutation importance will show that none of the features are important. In this case, permuting a feature drops the accuracy by at most 0.012, which would suggest that none of the features are important. This is in contradiction with the high test accuracy computed above: some feature must be important.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n\ntree_importance_sorted_idx = np.argsort(clf.feature_importances_)\ntree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n\nax1.barh(tree_indices, clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\nax1.set_yticks(tree_indices)\nax1.set_yticklabels(X.columns[tree_importance_sorted_idx])\nax1.set_ylim((0, len(clf.feature_importances_)))\nax1.set_title(\"MDI Feature Importance\")\nax1.set_xlim((0, 1))\n\nresult = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42)\nperm_sorted_idx = result.importances_mean.argsort()\n\nax2.boxplot(result.importances[perm_sorted_idx].T, vert=False, labels=X.columns[perm_sorted_idx])\nax2.set_title(\"Permutation Importance (training set)\")\nax2.set_xlim((0, 1))\nfig.tight_layout()\nplt.show()\n\n\n\n\nOne approach to handling multicollinearity is by performing hierarchical clustering on the features’ Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster.\nWe can visualise how correlated our features are by plotting a dendrogram of the correlations as well as a heatmap. These suggest that all the columns relating to “size” are highly correlated with each other, as we would expect.\n\ncorr = spearmanr(X).correlation\n\n# Ensure the correlation matrix is symmetric\ncorr = (corr + corr.T) / 2\nnp.fill_diagonal(corr, 1)\n\n# Convert correlation matrix to distance matrix before performing hierarchical clustering using Ward's linkage.\ndistance_matrix = 1 - np.abs(corr)\ndist_linkage = hierarchy.ward(squareform(distance_matrix))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 8))\n\ndendro = hierarchy.dendrogram(dist_linkage, labels=X.columns, ax=ax1, leaf_rotation=90)\ndendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n\nax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\nax2.set_xticks(dendro_idx)\nax2.set_yticks(dendro_idx)\nax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\nax2.set_yticklabels(dendro[\"ivl\"])\nfig.tight_layout()\nplt.show()\n\n\n\n\nNext, we manually pick a threshold by visual inspection of the dendrogram to group our features into clusters and choose a feature from each cluster to keep, select those features from our dataset, and train a new random forest. The test accuracy of the new random forest did not change much compared to the random forest trained on the complete dataset.\n\ncluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = defaultdict(list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nselected_features = [X.columns[i] for i in selected_features]\n\nX_train_sel = X_train[selected_features]\nX_test_sel = X_test[selected_features]\n\nclf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(f\"Accuracy on test data with features removed: {clf_sel.score(X_test_sel, y_test):.2f}\")\n\nAccuracy on test data with features removed: 0.97\n\n\nReplotting the feature importances now gives us a much clearer picture of what is important. In this case, the radius is clearly the most important feature. This suggests that the various “size” features were cancelling each other out with their collinearity when we previously looked at the feature importance.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\ntree_importance_sorted_idx = np.argsort(clf_sel.feature_importances_)\ntree_indices = np.arange(0, len(clf_sel.feature_importances_)) + 0.5\n\nax1.barh(tree_indices, clf_sel.feature_importances_[tree_importance_sorted_idx], height=0.7)\nax1.set_yticks(tree_indices)\nax1.set_yticklabels(X_train_sel.columns[tree_importance_sorted_idx])\nax1.set_ylim((0, len(clf_sel.feature_importances_)))\nax1.set_title(\"MDI Feature Importance\")\nax1.set_xlim((0, 1))\n\nresult = permutation_importance(clf_sel, X_train_sel, y_train, n_repeats=10, random_state=42)\nperm_sorted_idx = result.importances_mean.argsort()\n\nax2.boxplot(result.importances[perm_sorted_idx].T, vert=False, labels=X_train_sel.columns[perm_sorted_idx])\nax2.set_title(\"Permutation Importance (training set)\")\nax2.set_xlim((0, 1))\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "05_Interpretability/feature_importance.html#references-and-links",
    "href": "05_Interpretability/feature_importance.html#references-and-links",
    "title": "Feature Importance",
    "section": "References and Links",
    "text": "References and Links\n\nPermutation Importance vs Random Forest Feature Importance (MDI)\nPermutation Importance with Multicollinear or Correlated Features"
  },
  {
    "objectID": "Calculus/chain_rule.html",
    "href": "Calculus/chain_rule.html",
    "title": "Chain Rule",
    "section": "",
    "text": "import sympy\nThe chain rule is a fundamental concept in calculus that allows you to compute the derivative of a composite function. In other words, if you have a function that is formed by combining two or more functions, the chain rule helps you find the derivative of the resulting function with respect to its input variable(s).\nMathematically, the chain rule states that for two functions, \\(f(x)\\) and \\(g(x)\\), the derivative of their composition \\[h(x) = f(g(x))\\] is\n\\[h'(x) = f'(g(x)) * g'(x)\\]\nwhere \\(h'(x)\\) is the derivative of \\(h(x)\\) with respect to \\(x\\), \\(f'(g(x))\\) is the derivative of \\(f\\) with respect to \\(g(x)\\), and \\(g'(x)\\) is the derivative of \\(g\\) with respect to \\(x\\)."
  },
  {
    "objectID": "Calculus/chain_rule.html#examples",
    "href": "Calculus/chain_rule.html#examples",
    "title": "Chain Rule",
    "section": "Examples",
    "text": "Examples\nFind the derivative of the composite function \\(h(x) = (2x^2 + 3)^5\\) with respect to \\(x\\).\n\nDefine functions \\(f(x)\\) and \\(g(x)\\)\n\n\\[\n\\begin{aligned}\nf(x) &= x^5 \\\\\ng(x) &= 2x^2 + 3\n\\end{aligned}\n\\]\n\nCompute the derivatives\n\n\\[\n\\begin{aligned}\nf'(x) &= 5x^4 \\\\\ng'(x) &= 4x\n\\end{aligned}\n\\]\n\nApply the chain rule\n\n\\[\n\\begin{aligned}\nh'(x) &= f'(g(x)) * g'(x) \\\\\n&= 5(2x^2 + 3)^4 * 4x \\\\\n&= 20x(2x^2 + 3)^4\n\\end{aligned}\n\\]\nIt’s possible to check this using the sympy library\n\nx = sympy.symbols(\"x\")\n\nf_x = x**5\ng_x = 2 * x**2 + 3\n\nf_prime = sympy.diff(f_x, x)\ng_prime = sympy.diff(g_x, x)\n\nh_prime = f_prime.subs(x, g_x) * g_prime\n\nsympy.simplify(h_prime)\n\n\\(\\displaystyle 20 x \\left(2 x^{2} + 3\\right)^{4}\\)"
  },
  {
    "objectID": "Calculus/quotient_rule.html",
    "href": "Calculus/quotient_rule.html",
    "title": "Quotient Rule",
    "section": "",
    "text": "import sympy\nThe quotient rule is an important concept in calculus used to compute the derivative of the quotient (division) of two functions. If you have two functions, \\(f(x)\\) and \\(g(x)\\), the quotient rule helps you find the derivative of their quotient \\[h(x) = \\frac{f(x)}{g(x}\\] with respect to \\(x\\). Mathematically, the quotient rule states that:\n\\[h'(x) = \\frac{f'(x)g(x) - f(x)g'(x)}{f(x)^2}\\]\nwhere \\(h'(x)\\) is the derivative of \\(h(x)\\) with respect to \\(x\\), \\(f'(x)\\) is the derivative of \\(f(x)\\) with respect to \\(x\\), and \\(g'(x)\\) is the derivative of \\(g(x)\\) with respect to \\(x\\)."
  },
  {
    "objectID": "Calculus/quotient_rule.html#example",
    "href": "Calculus/quotient_rule.html#example",
    "title": "Quotient Rule",
    "section": "Example",
    "text": "Example\nExample: Find the derivative of the quotient function \\(h(x) = \\frac{x^2 + 1}{3x - 4}\\) with respect to \\(x\\).\n\nDefine the functions \\(f(x)\\) and \\(g(x)\\):\n\n\\[\n\\begin{aligned}\nf(x) &= x^2 + 1 \\\\\ng(x) &= 3x - 4\n\\end{aligned}\n\\]\n\nCompute the derivatives:\n\n\\[\n\\begin{aligned}\nf'(x) &= 2x \\\\\ng'(x) &= 3\n\\end{aligned}\n\\]\n\nApply the quotient rule:\n\n\\[\n\\begin{aligned}\nh'(x) &= \\frac{f'(x)g(x) - f(x)g'(x)}{f(x)^2} \\\\\n&= \\frac{2x(3x - 4) - 3(x^2 + 1)}{(3x - 4)^2} \\\\\n&= \\frac{3x^2 - 8x - 3}{9x^2 - 24x + 16}\n\\end{aligned}\n\\]\nIn Sympy this looks like:\n\nx = sympy.symbols(\"x\")\n\nu_x = x**2 + 1\nv_x = 3 * x - 4\n\nu_prime = sympy.diff(u_x, x)\nv_prime = sympy.diff(v_x, x)\n\nw_prime = (u_prime * v_x - u_x * v_prime) / v_x**2\n\nsympy.simplify(w_prime)\n\n\\(\\displaystyle \\frac{3 x^{2} - 8 x - 3}{9 x^{2} - 24 x + 16}\\)"
  },
  {
    "objectID": "Calculus/product_rule.html",
    "href": "Calculus/product_rule.html",
    "title": "Product Rule",
    "section": "",
    "text": "import sympy\nThe product rule is a fundamental concept in calculus used to compute the derivative of the product of two functions. If you have two functions, \\(f(x)\\) and \\(g(x)\\), the product rule helps you find the derivative of their product \\[h(x) = f(x)g(x)\\] with respect to \\(x\\). Mathematically, the product rule states that:\n\\[\nh'(x) = f'(x)g(x) + f(x)g'(x)\n\\]\nwhere \\(h'(x)\\) is the derivative of \\(h(x)\\) with respect to \\(x\\), \\(f'(x)\\) is the derivative of \\(f(x)\\) with respect to \\(x\\), and \\(g'(x)\\) is the derivative of \\(g(x)\\) with respect to \\(x\\)."
  },
  {
    "objectID": "Calculus/product_rule.html#example",
    "href": "Calculus/product_rule.html#example",
    "title": "Product Rule",
    "section": "Example",
    "text": "Example\nFind the derivative of the product function \\(h(x) = (x^2 + 1)(3x - 4)\\) with respect to \\(x\\)\n\nDefine the functions \\(f(x)\\) and \\(g(x)\\):\n\n\\[\n\\begin{aligned}\nf(x) &= x^2 + 1 \\\\\ng(x) &= 3x - 4\n\\end{aligned}\n\\]\n\nCompute the derivatives:\n\n\\[\n\\begin{aligned}\nf'(x) &= 2x \\\\\ng'(x) &= 3\n\\end{aligned}\n\\]\n\nApply the product rule:\n\n\\[\n\\begin{aligned}\nh'(x) &= f'(x)g(x) + f(x)g'(x) \\\\\n&= 2x(3x - 4) + (x^2 + 1)(3) \\\\\n&= 9x^2 - 8x + 3 \\\\\n\\end{aligned}\n\\]\nIn Sympy this looks like\n\nx = sympy.symbols(\"x\")\n\nu_x = x**2 + 1\nv_x = 3 * x - 4\n\nu_prime = sympy.diff(u_x, x)\nv_prime = sympy.diff(v_x, x)\n\nw_prime = u_prime * v_x + u_x * v_prime\n\nsympy.simplify(w_prime)\n\n\\(\\displaystyle 9 x^{2} - 8 x + 3\\)"
  },
  {
    "objectID": "06_Computer_Vision/opencv.html",
    "href": "06_Computer_Vision/opencv.html",
    "title": "OpenCV",
    "section": "",
    "text": "from pathlib import Path\n\nimport numpy as np\nimport cv2\nfrom PIL import Image\n\nPROJECT_ROOT = Path.cwd().parent.parent"
  },
  {
    "objectID": "06_Computer_Vision/opencv.html#contours",
    "href": "06_Computer_Vision/opencv.html#contours",
    "title": "OpenCV",
    "section": "Contours",
    "text": "Contours\nContours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition.\n\nFor better accuracy, use binary images. Before finding contours, apply threshold or canny edge detection.\nIn OpenCV, finding contours is like finding a white object on black background. So the object to be found should be white and the background should be black.\n\n\ndef show(img: np.ndarray, scale: float):\n    return Image.fromarray(img).resize((int(img.shape[1] * 0.25), int(img.shape[0] * 0.25)))\n\n\nimg = cv2.imread(f\"{PROJECT_ROOT}/data/brackley_map.png\")\nshow(img, 0.25)\n\n\n\n\n\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nshow(img_gray, 0.25)\n\n\n\n\n\nret, thresh = cv2.threshold(img_gray, 127, 255, 0)\n\n\nret\n\n127.0\n\n\n\nshow(thresh, 0.25)\n\n\n\n\n\ncontours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\nThere are 3 arguments in cv.findContours():\n\nThe source image\n\n\nThe contour retrieval mode\n\n\nContour Approximation Method\nContours are the boundaries of a shape with same intensity. OpenCV stores the (x,y) coordinates of the boundary of a shape. Whether it stores all the coordinates is specified by the contour approximation method.\nIf you pass cv.CHAIN_APPROX_NONE, all the boundary points will be stored. But do we need all the points? e.g. if you found the contour of a straight line. Do you need all the points on the line to represent that line? No, we just need the two end points of that line. This is what cv.CHAIN_APPROX_SIMPLE does. It removes all redundant points and compresses the contour, thereby saving memory.\nIt outputs:\n\nContours, a list of all the contours in the image. Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.\n\nTo draw contours, cv.drawContours() is used. It can be used to draw any shape provided you have its boundary points. Its arguments are:\n\nThe source image\na list of contours,\nThe index of contours (useful when drawing an individual contour. To draw all contours, pass -1)\nColor, thickness etc\n\nTo draw all the contours in an image:\n\ncv2.drawContours(img, contours, -1, (0, 255, 0), 3)\n\narray([[[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       ...,\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]]], dtype=uint8)\n\n\n\nshow(img, 0.25)\n\n\n\n\nTo draw an individual contour, say 4th contour:\n\ncv2.drawContours(img, contours, 3, (0, 255, 0), 3)\n\narray([[[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       ...,\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]]], dtype=uint8)\n\n\nBut most of the time, the below method will be useful:\n\ncnt = contours[4]\ncv2.drawContours(img, [cnt], 0, (0, 255, 0), 3)\n\narray([[[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       ...,\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0],\n        ...,\n        [  0, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]]], dtype=uint8)"
  },
  {
    "objectID": "06_Computer_Vision/opencv.html#contour-features",
    "href": "06_Computer_Vision/opencv.html#contour-features",
    "title": "OpenCV",
    "section": "Contour Features",
    "text": "Contour Features\n\nimg = cv2.imread(f\"{PROJECT_ROOT}/data/brackley_map.png\")\nthresh = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nret, thresh = cv2.threshold(thresh, 127, 255, 0)\ncontours, hierarchy = cv2.findContours(thresh, 1, 2)\ncnt = contours[0]\n\n\nImage Moment\nImage moments help you to calculate some features like center of mass or area of the object. The function cv.moments() gives a dictionary of all moment values calculated.\n\nM = cv2.moments(cnt)\n\nprint(M)\n\n{'m00': 18.0, 'm10': 26030.0, 'm01': 25102.0, 'm20': 37642337.0, 'm11': 36300272.666666664, 'm02': 35006145.33333333, 'm30': 54435189123.0, 'm21': 52494305766.333336, 'm12': 50622752172.833336, 'm03': 48818047302.0, 'mu20': 64.7777777784354, 'mu11': -8.444444443639895, 'mu02': 11.777777775981576, 'mu30': -12.61728898538739, 'mu21': 2.0987655809396197, 'mu12': 1.6234642072223873, 'mu03': -0.7160334160613768, 'nu20': 0.19993141289640554, 'nu11': -0.026063100134691035, 'nu02': 0.03635116597525178, 'nu30': -0.00917877633925695, 'nu21': 0.001526801825517828, 'nu12': 0.0011810314299800333, 'nu03': -0.0005208972058160166}\n\n\nFrom this, you can extract useful data like area, centroid etc. The centroid is given by the relations, \\(C_x = \\frac{M_{10}}{M_{00}}\\) and \\(C_y=\\frac{M_{01}}{M_{00}}\\).\n\ncx = int(M[\"m10\"] / M[\"m00\"])\ncy = int(M[\"m01\"] / M[\"m00\"])\n\nprint(cx, cy)\n\n1446 1394\n\n\nThe contour area is given by the function cv.contourArea() or from moments, M['m00'].\n\narea = cv2.contourArea(cnt)\n\narea\n\n18.0\n\n\nThe contour perimetre is also called the arc length. It can be found out using cv.arcLength(). The second argument specifies whether the shape is a closed contour or just a curve.\n\nperimeter = cv2.arcLength(cnt, True)\n\nperimeter\n\n18.485281229019165\n\n\n\n\nContour Approximation\nIt approximates a contour shape to another shape with less number of vertices depending upon the precision we specify. It is an implementation of Douglas-Peucker algorithm. Check the wikipedia page for algorithm and demonstration.\nTo understand this, suppose you are trying to find a square in an image, but due to some problems in the image, you didn’t get a perfect square, but a “bad shape” (As shown in first image below). Now you can use this function to approximate the shape. In this, second argument is called epsilon, which is maximum distance from contour to approximated contour. It is an accuracy parameter. A wise selection of epsilon is needed to get the correct output.\n\nepsilon = 0.1 * cv2.arcLength(cnt, True)\napprox = cv2.approxPolyDP(cnt, epsilon, True)\n\n\nepsilon\n\n1.8485281229019166\n\n\n\napprox\n\narray([[[1442, 1395]],\n\n       [[1450, 1394]]], dtype=int32)\n\n\n\ncv2.drawContours(thresh, [approx], -1, (0, 255, 0), 3)\n\narray([[255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       ...,\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255],\n       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n\n\n\nshow(thresh, 0.25)"
  },
  {
    "objectID": "04_Logistic_Regression/logistic_regression.html",
    "href": "04_Logistic_Regression/logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\nnp.random.seed(42)\n\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)  # RGBA tuple with alpha=0\nRegression models predict a continuous variable, such as rainfall amount or sunlight intensity. They can also predict probabilities, such as the probability that an image contains a cat. A probability-predicting regression model can be used as part of a classifier by imposing a decision rule - for example, if the probability is 50% or more, decide it’s a cat.\nLogistic regression predicts probabilities, and is therefore a regression algorithm. However, it is commonly described as a classification method in the machine learning literature, because it can be (and is often) used to make classifiers. There are also “true” classification algorithms, such as SVMs, which only predict an outcome and do not provide a probability."
  },
  {
    "objectID": "04_Logistic_Regression/logistic_regression.html#logistic-regression-from-scratch",
    "href": "04_Logistic_Regression/logistic_regression.html#logistic-regression-from-scratch",
    "title": "Logistic Regression",
    "section": "Logistic regression from scratch",
    "text": "Logistic regression from scratch\n\nOdds and Odds Ratio\nUnderstanding logsitic regression begins with probability.\nThe probability of an event is defined as:\n\\[\nP(A) = \\frac{Number\\ of\\ ways\\ event\\ A\\ can\\ occur}{Total\\ number\\ of\\ possible\\ outcomes}\n\\]\nFor example:\n\\[\n\\begin{align*}\nP(Heads) &= \\frac{1}{2} \\\\\n\\\\\nP(1\\ or\\ 2) &= \\frac{2}{6} = \\frac{1}{3} \\\\\n\\\\\nP(diamond\\ card) &= \\frac{13}{52} = \\frac{1}{4}\n\\end{align*}\n\\]\nThe odds of an event is defined as:\n\\[\n\\begin{align*}\nodds &= \\frac{P(occurring)}{P(not\\ occurring)} = \\frac{p}{1 - p}\n\\end{align*}\n\\]\nFor example:\n\\[\n\\begin{align*}\nodds(Heads) &= \\frac{P(Heads)}{P(Tails)} &= \\frac{\\frac{1}{2}}{\\frac{1}{2}} &= 1\\ or\\ 1:1 \\\\\n\\\\\nodds(1\\ or\\ 2) &= \\frac{P(1\\ or\\ 2)}{P(3\\ or\\ 4\\ or\\ 5\\ or\\ 6)} &= \\frac{\\frac{1}{3}}{\\frac{2}{3}} &= \\frac{1}{2}\\ or\\ 1:2 \\\\\n\\\\\nodds(diamond\\ card) &= \\frac{P(diamond\\ card)}{P(not\\ diamond\\ card)} &= \\frac{\\frac{1}{4}}{\\frac{3}{4}}& = \\frac{1}{3}\\ or\\ 1:3\n\\end{align*}\n\\]\n\n\nBernoulli Distribution\nThe dependent variable in logistic regression follows the Bernoulli distribution having an unknown probability \\(p\\).\nThe Bernoulli distribution is a special case of the Binomial distribution where \\(n = 1\\) (in other words where there is just 1 trial).\nSuccess is 1 and failure is 0.\nThe probability of success is \\(p\\) and the probability of failure is \\(q\\). So \\(q = 1 - p\\).\nIn logistic regression we are estimating an unknown \\(p\\) for any given linear combination of the independent variables.\nWe therefore need to link our inpedendent variables to the Bernoulli distribution. That link is called the logit.\nIn logistic regression we do not know \\(p\\) like we do in Bernoulli (and more broadly Binomial) distribution problems. The goal of logistic regression is to estimate \\(p\\) for any given linear combination of the independent variables. The estimate of \\(p\\) is called \\(\\hat{p}\\).\nTo tie our linear combination of variables to the Bernoulli distribution we need a function that maps the linear combination of variables that could result in any given value on to the Bernoulli probability distribution with a domain from 0 to 1. The natural log of the odds, the logit, is that function. This can be written a few ways:\n\\[\n\\begin{align*}\nln(odds) &= ln(\\frac{p}{1 - p})\\ \\text{or}\\ ln(p) - ln(1 - p)\\ \\text{or}\\ logit(p)\n\\end{align*}\n\\]\nNote also that \\(ln(x) = log_e(x)\\).\nOn a graph this looks like:\n\nfig, ax = plt.subplots(figsize=(9, 6))\n\np = np.linspace(0.001, 0.999, 100)\ny = np.log(p / (1 - p))\n\nax.plot(p, y, label=\"Logit function\")\n\nax.set_xlim(-2.25, 2.25)\nax.set_xticks(np.arange(-2, 2.1, 0.25))\nax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.2f}\".rstrip(\"0\").rstrip(\".\")))\n\nax.set_yticks(np.arange(-6, 6.1, 1))\n\nax.spines[\"left\"].set_position(\"zero\")\nax.spines[\"bottom\"].set_position(\"zero\")\nax.spines[\"right\"].set_color(\"none\")\nax.spines[\"top\"].set_color(\"none\")\n\nax.legend()\nax.grid()\nplt.show()\n\n\n\n\nThis is close to what we want. However, the probabilities are currently on the x-axis. We want the probabilities to be on the y-axis. We can do this by inverting the function. We begin by remembering that the logit function is equivalent to a linear combination of the independent variables and their coefficients. We define this linear combination as:\n\\[\nz = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k\n\\]\nwhere \\(x_1\\) to \\(x_k\\) are the independent variables, and \\(\\beta_0\\) to \\(\\beta_k\\) are the coefficients.\n\\[\n\\begin{aligned}\nln(\\frac{p}{1 - p}) &= z \\\\\n\\frac{p}{1 - p} &= e^z \\\\\np &= (1 - p)e^z \\\\\np &= e^z - pe^z \\\\\np + pe^z &= e^z \\\\\np(1 + e^z) &= e^z \\\\\np &= \\frac{e^z}{1 + e^z}\n\\end{aligned}\n\\]\nwhere \\(p\\) is between 0 and 1.\nOn a graph it looks like this:\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\na = np.linspace(-5.01, 5.01, 100)\ny = np.exp(a) / (1 + np.exp(a))\n\nax.plot(a, y, label=\"Inverse Logit Function\")\n\nax.set_ylim(-2.25, 2.25)\nax.set_yticks(np.arange(-2, 2.1, 0.5))\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f\"{y:.2f}\".rstrip(\"0\").rstrip(\".\")))\n\nax.set_xticks(np.arange(-5, 6, 1))\n\nax.spines[\"left\"].set_position(\"zero\")\nax.spines[\"bottom\"].set_position(\"zero\")\nax.spines[\"right\"].set_color(\"none\")\nax.spines[\"top\"].set_color(\"none\")\n\nax.legend()\nax.grid()\nplt.show()\n\n\n\n\nThis is known as the sigmoid or logistic function (\\(\\sigma\\)), or the estimated regression equation. It is a non-linear function that maps real-valued inputs to the range \\([0, 1]\\). It is used in logistic regression to model the probability that an instance belongs to a particular class.\n\n\nFinding the coefficients with Maximum Likelihood Estimation (MLE)\nFor our model to be useful, we still need to find a good set off coefficents to substitute into our regression equation. The coefficients for logistic regression are often calculated using maximum likelihood estimation (MLE).\nThe likelihood function is a product of individual probabilities for each observation in the data set.\n\\[\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(\\mathbf{x}_i)^{y_i} (1 - p(\\mathbf{x}_i))^{1 - y_i}\n\\]\nwhere:\n\n\\(L(\\boldsymbol{\\beta})\\) is the vector of coefficients in the logistic regression model.\n\\(\\mathbf{x}_i\\) is the vector of features for observation \\(i\\).\n\\(y_i\\) is the outcome for observation \\(i\\).\n\\(p(\\mathbf{x}_i)\\) is the predicted probability of the positive class for observation \\(i\\), as given by the logistic function applied to \\(\\mathbf{x}_i \\cdot \\boldsymbol{\\beta}\\).\n\nEach of these probabilities is computed using the logistic function.\nThe goal of MLE in logistic regression is to find the set of parameters that maximises this likelihood function. This is typically done using iterative optimization algorithms such as gradient ascent (since we’re maximizing a function) or its variants.\nHowever, for reasons of numerical stability we generally work with the log-likelihood function instead of the likelihood function. The log-likelihood equation is:\n\\[\n\\log L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\log p(\\mathbf{x}_i) + (1 - y_i) \\log (1 - p(\\mathbf{x}_i)) \\right]\n\\]\nThe derivative of the logistic function is:\n\\[\\frac{d}{dz} \\sigma(z) = \\sigma(z) (1 - \\sigma(z))\\]"
  },
  {
    "objectID": "04_Logistic_Regression/logistic_regression.html#example",
    "href": "04_Logistic_Regression/logistic_regression.html#example",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\nIt’s also very easy to train our own logistic regression model. The process is as follows:\n\nInitialise the weights. This could be done randomly, or with zeros to make the model deterministic, or with some other method.\nCalculate the output of the current version model, given the inputs and the weights.\nThe line scores = np.dot(X, weights) performs the dot product of the input features X with the corresponding weights. In the context of logistic regression, this dot product is the weighted sum of the features for each sample in the dataset. The result of this operation is a score for each sample that indicates how strongly it belongs to the positive class.\nIf X is a matrix where rows represent different samples and columns represent different features, and weights is a 1D array where each element corresponds to the weight of a feature, then np.dot(X, weights) will be a 1D array where each element is the weighted sum for a sample. That is, it represents the linear combination of the features for each sample.\nFor instance:\n\\[\n\\begin{align}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n0.5 \\\\\n0.6\n\\end{bmatrix}\n&= \\begin{bmatrix}\n1 \\cdot 0.5 + 2 \\cdot 0.6 \\\\\n3 \\cdot 0.5 + 4 \\cdot 0.6 \\\\\n5 \\cdot 0.5 + 6 \\cdot 0.6\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1.7 \\\\\n3.9 \\\\\n6.1\n\\end{bmatrix}\n\\end{align}\n\\]\nThis essentially projects the feature vectors onto the weights vector, resulting in a single scalar value (the “score”) for each sample.\nThese scores are then passed through the sigmoid function to produce the predicted probability of the positive class for each sample.\nWe then update the weights using maximum likelihood estimation. This means:\n\nThe predicted probabilities are subtracted from the actual labels to produce the error.\nWe then calculate the gradient of the log-likelihood with respect to the weights.\nIn order to perform this operation correctly, we need to multiply the n x m matrix X with the n x 1 matrix error (where n is the number of observations and m is the number of features). However, the standard rules of matrix multiplication don’t allow us to multiply these two matrices in this order, because the number of columns of the first matrix doesn’t match the number of rows of the second matrix. So we transpose X into an m x n matrix X.T where the number of columns in X.T matches the number of rows in the error matrix. The result is an m x 1 matrix, which represents the gradient of the log-likelihood with respect to each feature’s weight.\nOr to put it another way, we have calculated the contribution of each feature to the error for each observation, and then summed these contributions over all observations.\nThe weights are then updated by adding the gradient multiplied by the learning rate. The learning rate controls how much the weights are changed in response to the error. If the learning rate is too high, the weights will change too much and the model will not converge. If the learning rate is too low, the model will take a long time to converge.\n\n\nAlong the way we also calculate the negative log-likelihood to illustrate how well our model is converging.\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef predict_proba(x, weights):\n    scores = np.dot(x, weights)\n    return sigmoid(scores)\n\n\ndef negative_log_likelihood(X, y, weights):\n    scores = np.dot(X, weights)\n    log_likelihood = np.sum(y * scores - np.log(1 + np.exp(scores)))\n    return -log_likelihood\n\n\ndef logistic_regression(X, y, num_steps: int, learning_rate: float, silent: bool = False):\n    weights = np.zeros(X.shape[1])\n\n    for step in range(num_steps):\n        predicted_probabilities = predict_proba(X, weights)\n\n        error = y - predicted_probabilities\n        gradient = np.dot(X.T, error)\n        weights += learning_rate * gradient\n\n        if step % 100000 == 0:\n            print(f\"Negative log-likelihood at step {step}: {negative_log_likelihood(X, y, weights)}\")\n\n    return weights\n\nWe can then fit it like so:\n\nweights = logistic_regression(X, y, num_steps=1000000, learning_rate=5e-5)\nprint(\"Weights:\", weights)\n\nNegative log-likelihood at step 0: 3.465610934830976\nNegative log-likelihood at step 100000: 1.6612460527415533\nNegative log-likelihood at step 200000: 1.2058437634360355\nNegative log-likelihood at step 300000: 0.9896429709538741\nNegative log-likelihood at step 400000: 0.8576781489870338\nNegative log-likelihood at step 500000: 0.766062385341013\nNegative log-likelihood at step 600000: 0.697364426602992\nNegative log-likelihood at step 700000: 0.6431658475892499\nNegative log-likelihood at step 800000: 0.5988480247519807\nNegative log-likelihood at step 900000: 0.5616385926438348\nWeights: [-8.92141553  2.604058  ]\n\n\n\npredict_proba(X, weights)\n\narray([0.00180145, 0.02381586, 0.24801232, 0.8168001 , 0.98367944])\n\n\nFinally we can see that our from scratch model makes similar predictions to the sklearn model and that both sets of predictions are very close to the actual labels.\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax.scatter(X[:, 1], y, color=\"C0\", label=\"Data\")\nax.scatter(X[:, 1], predict_proba(X, weights), color=\"C1\", label=\"Our model predictions\", marker=\"x\")\nax.scatter(X[:, 1], model.predict_proba(X)[:, 1], color=\"C2\", label=\"Sklearn model predictions\", marker=\"x\")\nax.legend()\nax.set_xlabel(\"X\")\nax.set_ylabel(\"y\")\nax.set_title(\"Logistic regression predictions\")\nplt.show()"
  },
  {
    "objectID": "04_Logistic_Regression/logistic_regression.html#why-not-use-linear-regression",
    "href": "04_Logistic_Regression/logistic_regression.html#why-not-use-linear-regression",
    "title": "Logistic Regression",
    "section": "Why not use linear regression?",
    "text": "Why not use linear regression?\nA linear model does not output probabilities. It treats the classes as numbers (0 and 1) and fits the best hyperplane (for a single feature, it is a line) that minimises the distances between the points and the hyperplane. Tt simply interpolates between the points, and you cannot interpret it as probabilities.\nWe can see it going wrong when it predicts values outside the range of 0 and 1. We can also how interpolation breaks when the x values are very large or very small. In contrast the logistic regression model remains reliable at each extreme.\n\nn_samples = 100\nX = np.random.normal(size=n_samples)\ny = (X &gt; 0).astype(float)\n\nX[X &gt; 0] *= 4\nX += 0.3 * np.random.normal(size=n_samples)\nX = X[:, np.newaxis]\n\nX_test = np.linspace(-5, 10, 300)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\n\nlog_reg = LogisticRegression(C=1e5)\nlog_reg.fit(X, y)\n\nfig, ax = plt.subplots(figsize=(4, 3))\nax.scatter(X.ravel(), y, color=\"C0\", label=\"Data\", marker=\".\")\nax.plot(X_test, X_test * lin_reg.coef_ + lin_reg.intercept_, color=\"C1\", label=\"Linear Regression\")\nax.plot(X_test, sigmoid(X_test * log_reg.coef_ + log_reg.intercept_).ravel(), color=\"C2\", label=\"Logistic Regression\")\n\nax.set_yticks([0, 0.25, 0.5, 0.75, 1])\nax.set_ylim(-0.1, 1.1)\nax.set_ylabel(\"y\")\nax.set_xticks(range(-5, 10))\nax.set_xlim(-4, 10)\nax.set_xlabel(\"x\")\nax.legend(loc=\"best\")\nax.grid()\nplt.axhline(0.5, color=\"0.5\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "04_Logistic_Regression/logistic_regression.html#interpreting-how-a-change-in-a-feature-affects-the-probability-of-the-target-being-1",
    "href": "04_Logistic_Regression/logistic_regression.html#interpreting-how-a-change-in-a-feature-affects-the-probability-of-the-target-being-1",
    "title": "Logistic Regression",
    "section": "Interpreting how a change in a feature affects the probability of the target being 1",
    "text": "Interpreting how a change in a feature affects the probability of the target being 1\n\nOdds Ratio\nThe odds ratio is a ratio of 2 odds.:\n\\[\n\\begin{align*}\nodds\\ ratio &= \\frac{odds\\ of\\ event\\ A}{odds\\ of\\ event\\ B} \\\\\n\\end{align*}\n\\]\nFor example, suppose we have a loaded coin such that:\n\\[\n\\begin{align*}\nP(Heads) &= \\frac{7}{10} = 0.7 \\\\\nodds(Heads) &= \\frac{0.7}{1 - 0.7} ~= 2.333 \\\\\n\\end{align*}\n\\]\nThe odds ratio of this coin compared to a fair coin is:\n\\[\n\\begin{align*}\nodds\\ ratio = \\frac{odds_1}{odds_0} = \\frac{\\frac{p_1}{1 - p_1}}{\\frac{p_0}{1 - p_0}} = \\frac{\\frac{0.7}{1 - 0.7}}{\\frac{0.5}{1 - 0.5}} = \\frac{0.7}{0.3} \\times \\frac{0.5}{0.5} = \\frac{0.35}{0.15} = 2.333\n\\end{align*}\n\\]\nThis is means the odds of getting heads with the loaded coin is 2.333 times greater than the odds of getting heads with a fair coin. (The odds and odds ratio are the same for tails in this example because the odds of heads with a fair coin is 1).\nThe odds ratio for a variable in logistic regression represents how the odds change for a 1 unit increase in that variable holding all other variables constant. To take a fictious example:\n\nBody weight and sleep apnea (2 categories: apnea / no apnea).\nWeight variable has an odds ratio of 1.07.\nThis means that a 1 unit increase in weight increases the odds of having sleep apnea by 1.07\nA 10 pound increase in weight increases the odds of having sleep apnea to \\(1.07^{10} = 1.967\\). It almost doubles a person’s odds of having sleep apnea.\nThis holds true at any point on the weight spectrum."
  },
  {
    "objectID": "Statistical Rethinking/chapter1.html",
    "href": "Statistical Rethinking/chapter1.html",
    "title": "Baysian Data Analysis",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom IPython import display\nfrom IPython.display import HTML, Image\nimport scipy.stats as stats\n\nBayesian data analysis takes a question in the form of a model and uses logic to produce an answer in the form of probability distributions.\nIn Baysian data analysis:\n\nFor each possible explanation of the data, count all the ways the data can happen\nExplanations with more ways to produce the data are more plausible\n\nImagine a bag that contains 4 marbles. Some are blue and some are white. We don’t know in what proportion. A table of all the possible combinations looks like this. These possible combinations are our conjectures.\n\n\n\nB\nB\nB\nB\n\n\nB\nB\nB\nW\n\n\nB\nB\nW\nW\n\n\nB\nW\nW\nW\n\n\nW\nW\nW\nW\n\n\n\nNow suppose that we draw 3 marbles from the bag with replacement. The results from that will be our data.\n\\[\nBWB\n\\]\nHow could we think about what the 4th marble is likely to be?\nNotice that the number of ways to produce the data, for each conjecture, can be computed by first counting the number of paths in each “ring” of the garden and then by multiplying these counts together.\n\n\n\nConjecture\nNumber of ways to produce the \\[BWB\\]\nPlausibility\n\n\n\n\n\\[WWWW\\]\n\\[0 \\times 4 \\times 0 = 0\\]\n0\n\n\n\\[BWWW\\]\n\\[1 \\times 3 \\times 1 = 3\\]\n0.15\n\n\n\\[BBWW\\]\n\\[2 \\times 2 \\times 2 = 8\\]\n0.4\n\n\n\\[BBBW\\]\n\\[3 \\times 1 \\times 3  = 9\\]\n0.45\n\n\n\\[BBBB\\]\n\\[4 \\times 0 \\times 4  = 0\\]\n0\n\n\n\nThe rules for Baysian updating:\n\nState a causal model for how observations arise, given each possible explanation\nCount ways data could arise for each explanation\nRelative plausibility is relative value from (2)\n\nLet’s suppose we draw another \\[B\\] from the bag. We can update our previous (prior) counts and update them in light of the new observation becuase the new observation is logically independent of the previous observations. First count the numbers of ways each conjecture could produce the new observation. Then multiply each of these new counts by the prior numbers of ways for each conjecture. In table form:\n\n\n\n\n\n\n\n\n\n\nConjecture\nWays to produce \\[B\\]\nPrevious counts\nNew count\nNew plausibility\n\n\n\n\n\\[WWWW\\]\n0\n0\n\\[0 \\times 0 = 0\\]\n0\n\n\n\\[BWWW\\]\n1\n3\n\\[3 \\times 1 = 3\\]\n\\[0.07\\]\n\n\n\\[BBWW\\]\n2\n8\n\\[8 \\times 2 = 16\\]\n\\[0.35\\]\n\n\n\\[BBBW\\]\n3\n9\n\\[9 \\times 3 = 27\\]\n\\[0.57\\]\n\n\n\\[BBBB\\]\n4\n0\n\\[0 \\times 4 = 0\\]\n0\n\n\n\nObviously you want to normalise these values which is why we have a plausibilty colum. You can can converts the counts to probabilities with the following formula:\n\\[\nplausibility\\ of\\ p\\ after\\ D_{new} = \\frac {ways\\ p\\ can\\ produce\\ D_{new} \\times prior\\ plausibility\\ p} {sum\\ of\\ products}\n\\]\n\nresults = np.random.binomial(n=1, p=0.7, size=[10])\n\nyes_line = np.linspace(10, 0, 20)\nno_line = np.linspace(0, 10, 20)\n\nresult_lines = []\nfor r in results:\n    if r == 1:\n        result_lines.append(yes_line)\n    else:\n        result_lines.append(no_line)\n\n\nprint(results)\n\n[1 0 1 0 1 0 1 0 1 0]\n\n\n\nplt.figure()\nfor i in range(0, results[:7].shape[0]):\n    plt.plot(np.prod(result_lines[:i], axis=0))\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots()\n\n\ndef init():\n    ax.clear()\n    (line,) = ax.plot(result_lines[0])\n    return (line,)\n\n\ndef animate(i):\n    ax.clear()\n    index = i % 20\n    new_data = np.prod(result_lines[:index], axis=0)\n    (line,) = ax.plot(new_data)\n    ax.set_yticks([])\n    ax.set_xticks([])\n    ax.set_yticks(range(0, int(np.ceil(np.max(yes_line)))))\n    return (line,)\n\n\nani = FuncAnimation(fig=fig, func=animate, init_func=init, frames=10, interval=1000, blit=True, save_count=20)\nvideo = ani.to_html5_video()\nhtml = display.HTML(video)\ndisplay.display(html)\nplt.close()\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\nplt.figure()\nplt.margins(x=0, y=0)\n\nyes_line = np.linspace(10, 0)\nno_line = np.linspace(0, 10)\ny = yes_line * yes_line * yes_line * no_line\nx = no_line\nplt.plot(x, y)\n\n# mode = x[np.argmax(y)]\n# plt.axvline(x=mode, label=\"mode\")\n\n# mean = np.mean(y)\n# plt.axvline(x=mean, label=\"mean\")\n\n\n\n\n\ndef posterior_grid_approx(grid_points: int = 5, success: int = 6, tosses: int = 9):\n    p_grid = np.linspace(0, 1, grid_points)\n    print(p_grid)\n    prior = np.repeat(5, grid_points)  # uniform\n    print(prior)\n    # prior = (p_grid &gt;= 0.5).astype(int)  # truncated\n    # prior = np.exp(- 5 * abs(p_grid - 0.5))  # double exp\n    # compute likelihood at each point in the grid\n    likelihood = stats.binom.pmf(success, tosses, p_grid)\n    print(likelihood)\n    # compute product of likelihood and prior\n    unstd_posterior = likelihood * prior\n    print(unstd_posterior)\n    # standardize the posterior, so it sums to 1\n    posterior = unstd_posterior / unstd_posterior.sum()\n    print(posterior)\n    print()\n    return p_grid, posterior\n\n\nw, n = 6, 9\n_, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nfor idx, ps in enumerate((5, 20)):\n    p_grid, posterior = posterior_grid_approx(ps, w, n)\n    ax[idx].plot(p_grid, posterior, \"o-\", label=f\"successes = {w}\\ntosses = {n}\")\n    ax[idx].set_xlabel(\"probability of water\")\n    ax[idx].set_ylabel(\"posterior probability\")\n    ax[idx].set_title(f\"{ps} points\")\n    ax[idx].legend(loc=0)\n\n[0.   0.25 0.5  0.75 1.  ]\n[5 5 5 5 5]\n[0.         0.00865173 0.1640625  0.2335968  0.        ]\n[0.         0.04325867 0.8203125  1.16798401 0.        ]\n[0.         0.02129338 0.40378549 0.57492114 0.        ]\n\n[0.         0.05263158 0.10526316 0.15789474 0.21052632 0.26315789\n 0.31578947 0.36842105 0.42105263 0.47368421 0.52631579 0.57894737\n 0.63157895 0.68421053 0.73684211 0.78947368 0.84210526 0.89473684\n 0.94736842 1.        ]\n[5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n[0.00000000e+00 1.51814898e-06 8.18509295e-05 7.77292279e-04\n 3.59857537e-03 1.11609461e-02 2.66829865e-02 5.29210991e-02\n 9.08269760e-02 1.38341326e-01 1.89768623e-01 2.36114658e-01\n 2.66611252e-01 2.71400562e-01 2.45005089e-01 1.89768623e-01\n 1.17918118e-01 5.02667021e-02 8.85384487e-03 0.00000000e+00]\n[0.00000000e+00 7.59074491e-06 4.09254647e-04 3.88646139e-03\n 1.79928768e-02 5.58047305e-02 1.33414933e-01 2.64605496e-01\n 4.54134880e-01 6.91706630e-01 9.48843114e-01 1.18057329e+00\n 1.33305626e+00 1.35700281e+00 1.22502544e+00 9.48843114e-01\n 5.89590588e-01 2.51333510e-01 4.42692243e-02 0.00000000e+00]\n[0.00000000e+00 7.98983711e-07 4.30771684e-05 4.09079660e-04\n 1.89388732e-03 5.87387288e-03 1.40429377e-02 2.78517436e-02\n 4.78011547e-02 7.28073907e-02 9.98729639e-02 1.24264330e-01\n 1.40314323e-01 1.42834880e-01 1.28943257e-01 9.98729639e-02\n 6.20588995e-02 2.64547661e-02 4.65967300e-03 0.00000000e+00]\n\n\n\n\n\n\nEvery updated set of plausibilities becomes the initial plausibilities for the next observation. Every conclusion is the starting point for future inference. This updating process works backwards as well as forwards. Given a final set of plausibilities and knowing the final observation \\[W\\] it is possible to mathematically divide out the observation to infer the previous plausibility curve. So the data could be presented to your model in any order, or all at once even. In most cases you will present the data all at once for convenience. This represents an abbreviation of an iterated learning process.\nSome points about Baysian Inference:\n\nThere is no such thing as a minimum sample size. The curve is likely to be flat and wide when you have little data but that’s fine. It might even be enough information to be useful in some sense.\nThe shape of the curve embodies the sample size. As you get more data the curve gets narrower and taller.\nPoint estimates don’t have a big role in Baysian data analyis because the the estimate is the curve/distribution. Always use the entire posterior distribution because to do otherwise is to throw away uncertainty and generate overconfident answers. Summary should always be the last step.\nIntervals don’t have a strong role in Baysian inference. How wide to make an interval or even where to centre it is an arbitrary descision.\n\nWhat proportion of the Earth is covered by water? We aren’t going to visit literally every point on Earth. So how can we work this out? We can take a sample. Suppose we visit ten points of earth at random and record whether it is covered by land or water. Suppose these are our results :\n\\[\nW, W, W, L, W, W, W, W, W, L\n\\]\nThe counts of “water” \\[W\\] and “land’ \\[L\\] are distributed binomially, with probability p of “water” on each toss.\n\\[\nPr(W,L|p) = \\frac {(W + L)!} {W!L!} p^W(1 − p)^L\n\\]\nA short way of writing our model\nW ~ Binomial(N, p)\nWhere N = W + L\np ~ Uniform(0,1)"
  },
  {
    "objectID": "00_Statistics/statistics.html",
    "href": "00_Statistics/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "\\[\n\\bar x = \\frac 1 n \\sum_ {i=1}^n  x_i\n\\]\n\n\n\nThe median is the middle value in an ordered set of data. If there are an even number of values it is the average of the middle 2 values in the set.\n\n\n\nThe mode is the most common value (or values) in a set of data.\n\n\n\nDispersion refers to measures of how spread out our data is. The simplest measure of dispersion is the range which is the difference between the largest and smallest elements in a set of data.\n\n\n\nA more sophisticated measure of dispersion is the variance. You can think of this as the average squared deviation from the mean. However we modify the sum by dividing by \\(n-1\\) because when dealing with a sample from a larger population, \\(\\bar{x}\\) is only an estimate of the actual mean. On average this means \\(x_i - \\bar{x}\\) is an underestimate of the true deviation which is why we divide by a smaller denominator.\n[QUESTION: Why only subtract 1?]\n[QUESTION: What if you actually do have the entire population?]\n\\[\nS^2 = \\frac {\\sum {(x_i - \\bar{x})}^2} {n - 1}\n\\]\n\\(S^2\\) = sample variance\n\\(x_i\\) = value of the one observation\n\\(\\bar{x}\\) = mean value of all observations\n\\(n\\) = number of observations\n\n\n\nThe standard deviation is the square root of the variance which means it is in the same units as the range of the dataset.\n\\[\nS = \\sqrt {\\frac {\\sum {(x_i - \\bar{x})}^2} {n - 1}}\n\\]\n\\(S^2\\) = sample variance\n\\(x_i\\) = value of the one observation\n\\(\\bar{x}\\) = mean value of all observations\n\\(n\\) = number of observations\n\n\n\nThe interquartile range is the difference between the 75th and 25th percentile value. This is a simple measure but useful as it is not affected by outliers like the range and standard deviation are.\n\n\n\nWhereas variance measures how a single variable deviates from its mean, covariance measures how two variables vary in tandem from their means.\n\\[\ncov_{x,y} = \\frac {\\sum {(x_i - \\bar{x})(y_i - \\bar{y})}} {N - 1}\n\\]\n\\(cov_{x,y}\\) = covariance between variable x and y\n\\(x_i\\) = data value of x\n\\(y_i\\) = data value of y\n\\(\\bar{x}\\) = mean of x\n\\(\\bar{y}\\) = mean of y\n\\(N\\) = number of data values\nA “large” positive covariance means that \\(x\\) tends to be large when \\(y\\) is large and small when y is small. A “large” negative covariance means the opposite — that \\(x\\) tends to be small when \\(y\\) is large and vice versa. A covariance close to zero means that no such relationship exists.\nCovariance can be hard to interpret because:\n\nIts units are the product of the inputs’ units which can be hard to make sense of.\nIf each every value of \\(x\\) was doubled (with \\(y\\) staying the same) the covariance would be twice as large. But the variables would be just as interrelated. So it’s hard to say what counts as a “large” covariance.\n\n\n\n\nPearson’s correlation coefficient is the covariance of the two variables divided by the product of their standard deviations:\n\\[\n\\rho(X,Y) = \\frac {cov(X,Y)}{\\sigma(X)\\sigma(Y)}\n\\]\n\\(cov\\) = the covariance\n\\(\\sigma(X)\\) = the standard deviation of X\n\\(\\sigma(Y)\\) = the standard deviation of Y\n\nCorrelation shows us relationships in which knowing how \\(x_i\\) compares to the mean of \\(x\\) gives us information about how \\(y_i\\) compares to the mean of \\(y\\). Other types of relationship may not show up. It also doesn’t tell us how large the relationship is. It is also the case that if \\(E\\) and \\(F\\) are independent if the probability of them both happening is equal to the product of the probability of each one happening."
  },
  {
    "objectID": "00_Statistics/statistics.html#descriptive-statistics",
    "href": "00_Statistics/statistics.html#descriptive-statistics",
    "title": "Statistics",
    "section": "",
    "text": "\\[\n\\bar x = \\frac 1 n \\sum_ {i=1}^n  x_i\n\\]\n\n\n\nThe median is the middle value in an ordered set of data. If there are an even number of values it is the average of the middle 2 values in the set.\n\n\n\nThe mode is the most common value (or values) in a set of data.\n\n\n\nDispersion refers to measures of how spread out our data is. The simplest measure of dispersion is the range which is the difference between the largest and smallest elements in a set of data.\n\n\n\nA more sophisticated measure of dispersion is the variance. You can think of this as the average squared deviation from the mean. However we modify the sum by dividing by \\(n-1\\) because when dealing with a sample from a larger population, \\(\\bar{x}\\) is only an estimate of the actual mean. On average this means \\(x_i - \\bar{x}\\) is an underestimate of the true deviation which is why we divide by a smaller denominator.\n[QUESTION: Why only subtract 1?]\n[QUESTION: What if you actually do have the entire population?]\n\\[\nS^2 = \\frac {\\sum {(x_i - \\bar{x})}^2} {n - 1}\n\\]\n\\(S^2\\) = sample variance\n\\(x_i\\) = value of the one observation\n\\(\\bar{x}\\) = mean value of all observations\n\\(n\\) = number of observations\n\n\n\nThe standard deviation is the square root of the variance which means it is in the same units as the range of the dataset.\n\\[\nS = \\sqrt {\\frac {\\sum {(x_i - \\bar{x})}^2} {n - 1}}\n\\]\n\\(S^2\\) = sample variance\n\\(x_i\\) = value of the one observation\n\\(\\bar{x}\\) = mean value of all observations\n\\(n\\) = number of observations\n\n\n\nThe interquartile range is the difference between the 75th and 25th percentile value. This is a simple measure but useful as it is not affected by outliers like the range and standard deviation are.\n\n\n\nWhereas variance measures how a single variable deviates from its mean, covariance measures how two variables vary in tandem from their means.\n\\[\ncov_{x,y} = \\frac {\\sum {(x_i - \\bar{x})(y_i - \\bar{y})}} {N - 1}\n\\]\n\\(cov_{x,y}\\) = covariance between variable x and y\n\\(x_i\\) = data value of x\n\\(y_i\\) = data value of y\n\\(\\bar{x}\\) = mean of x\n\\(\\bar{y}\\) = mean of y\n\\(N\\) = number of data values\nA “large” positive covariance means that \\(x\\) tends to be large when \\(y\\) is large and small when y is small. A “large” negative covariance means the opposite — that \\(x\\) tends to be small when \\(y\\) is large and vice versa. A covariance close to zero means that no such relationship exists.\nCovariance can be hard to interpret because:\n\nIts units are the product of the inputs’ units which can be hard to make sense of.\nIf each every value of \\(x\\) was doubled (with \\(y\\) staying the same) the covariance would be twice as large. But the variables would be just as interrelated. So it’s hard to say what counts as a “large” covariance.\n\n\n\n\nPearson’s correlation coefficient is the covariance of the two variables divided by the product of their standard deviations:\n\\[\n\\rho(X,Y) = \\frac {cov(X,Y)}{\\sigma(X)\\sigma(Y)}\n\\]\n\\(cov\\) = the covariance\n\\(\\sigma(X)\\) = the standard deviation of X\n\\(\\sigma(Y)\\) = the standard deviation of Y\n\nCorrelation shows us relationships in which knowing how \\(x_i\\) compares to the mean of \\(x\\) gives us information about how \\(y_i\\) compares to the mean of \\(y\\). Other types of relationship may not show up. It also doesn’t tell us how large the relationship is. It is also the case that if \\(E\\) and \\(F\\) are independent if the probability of them both happening is equal to the product of the probability of each one happening."
  },
  {
    "objectID": "00_Statistics/statistics.html#probability",
    "href": "00_Statistics/statistics.html#probability",
    "title": "Statistics",
    "section": "Probability",
    "text": "Probability\nNotationally, \\(P(E)\\) means the probability of the event \\(E\\).\nEvents \\(E\\) and \\(F\\) are dependent if inforamation about whether \\(E\\) happens gives us information about whether \\(F\\) happens or vice versa. Otherwise the two events are independent of each other.\n\\[ P(E,F) = P(E)P(F) \\]\nIf the two events are not necessarily independent (and the probability of \\(F\\) is not 0) we define probability of \\(E\\) given \\(F\\) as:\n\\[ P(E|F) = \\frac {P(E,F)}{P(F)} \\]\nThis is often written as:\n\\[ P(E,F) = P(E|F)P(F) \\]\nWhen \\(E\\) and \\(F\\) are independent this means:\n\\[ P(E|F) = P(E) \\]\nLet’s ask what is the probability “both children are girls” conditional on the event “the older child is a girl”?\nWe use the definition of conditional probability and the fact that the event \\(Both\\) and \\(Girl\\) (“both children are girls and the older child is a girl”) is the same as the event \\(Both\\).\n\\[\n\\begin{align}\nP(Both|Girl) &= \\frac {P(Both, Girl)}{P(Girl)} \\notag \\\\\n&= \\frac {P(Both)}{P(Girl)} \\notag \\\\\n&= \\frac {1}{2} \\notag\n\\end{align}\n\\]\nWe can also ask about the probability of the event “both children are girls” conditional on the event “at least one of the children is a girl” (\\(At\\ Least\\ One\\)). Surprisingly, the answer is different from before. If all you know is that at least one of the children is a girl, then it is twice as likely that the family has one boy and one girl than that it has both girls.\n\\[\n\\begin{align}\nP(Both | At\\ Least\\ One) &= \\frac {P(Both, At\\ Least\\ One)} {P(At\\ Least\\ One)} \\notag \\\\\n&= \\frac {P(Both)} {P(At\\ Least\\ One)} \\notag \\\\\n&= \\frac {1}{3} \\notag\n\\end{align}\n\\]\n\nBayes’ Theorem\nBayes’s theorem is a way of “reversing” conditional probabilities. If we need to know the probability of some event \\(E\\) conditional on some other event \\(F\\) occurring but we only have information about the probability of \\(F\\) conditional on \\(E\\) occurring, using the definition of conditional probability twice tells us that:\n\\[\n\\begin{align}\nP(E | F) &= \\frac {P(E, F)} {P(F)} \\notag \\\\\n&= \\frac {P(F | E)P(E)} {P(F)} \\notag \\\\\n\\end{align}\n\\]\nThe event \\(F\\) can be split into the two mutually exclusive events “\\(F\\) and \\(E\\)” and “\\(F\\) and not \\(E\\)”. We write \\(\\neg\\) for “not”.\n\\[\n\\begin{align}\nP(F) &= P(F, E) + P(F, \\neg E) \\notag \\\\\nP(E | F) &= \\frac {P(F | E)P(E)} {P(F, E)P(E) + P(F, \\neg E)P(\\neg E)} \\notag \\\\\n\\end{align}\n\\]\nIf a disease has a 1 in 10000 chance of occurring (\\(P(Disease) = 0.0001\\)) and a test has a 99% chance of giving a positive result when someone has the disease (\\(P(Positive|Disease) = 0.99\\)) we would apply Bayes Theorum like so:\n\\[\n\\begin{align}\nP(Disease | Positive) &= \\frac {P(Positive | Disease)P(Disease)} {P(Positive | Disease)P(Disease) + P(Positive | \\neg Disease)P(\\neg Disease)} \\notag \\\\\n&= \\frac {0.99 \\times 0.0001}{0.99 \\times 0.0001 + 0.01 \\times 0.9999} \\notag \\\\\n&\\approx 0.0098 \\notag \\\\\n&= 0.98\\% \\notag\n\\end{align}\n\\]\nNote that this assumes people take the test roughly at random. If only people with certain symptoms take the test, we would have to condition on the event “positive result and symptoms” and the number would likely be a lot higher.\nWe can also view this problem using natural frequencies. In a population of 1 million people you’d expect 100 of them to have the disease, and 99 of those 100 to test positive. On the other hand, you’d expect 999,900 of them not to have the disease, and 9,999 of those to test positive. That means you’d expect only 99 out of 10098 (99 + 9999) positive testers to actually have the disease which is roughly 0.98%.\n\n\nRandom Variables\nA random variable is a variable whose possible values have an associated probability distribution. The expected value of a random variable is the average of its values weighted by their probabilities.\nA very simple random variable equals 1 if a coin flip turns up heads and 0 if the flip turns up tails. The variable equals 0 with probability 0.5 and 1 with probability 0.5. It has an expected value of \\(\\frac{1}{2} = (0 * \\frac{1}{2} + 1 * \\frac{1}{2})\\)\nA more complicated random variable might measure the number of heads you observe when flipping a coin 10 times or a value picked from range(10) where each number is equally likely. The associated distribution gives the probabilities that the variable realizes each of its possible values. In this case the variable has a distribution that assigns probability 0.1 to each of the numbers from 0 to 9. Flipping a coin 10 times has an expected value of 5.5. The range(10) variable has an expected value of 4.5.\n\\[\nE(X) = \\sum {X_iP(X_i)}\n\\]\n\\(X_i\\) = The values that X can take\n\\(P(X_i)\\) = The probability that X takes the value \\(X_i\\)\nRandom variables can be conditioned on events just as other events can. Going back to the two-child example, if \\(X\\) is the random variable representing the number of girls, \\(X\\) equals 0 with probability \\(1 \\over 2\\) \\(\\frac{1}{4}\\), 1 with probability \\(\\frac{1}{2}\\), and 2 with probability \\(\\frac{1}{4}\\). We can define a new random variable \\(Y\\) that gives the number of girls conditional on at least one of the children being a girl. Then \\(Y\\) equals 1 with probability \\(\\frac{2}{3}\\) and 2 with probability \\(\\frac{1}{3}\\). And a variable \\(Z\\) that’s the number of girls conditional on the older child being a girl equals 1 with probability \\(\\frac{1}{2}\\) and 2 with probability \\(\\frac{1}{2}\\).\n\n\nContinuous Distributions\nA coin flip corresponds to a discrete distribution — one that associates positive probability with discrete outcomes. Often we’ll want to model distributions across a continuum of outcomes. Because there are infinitely many numbers between 0 and 1, this means that the weight it assigns to individual points is be zero (for our purposes these outcomes will always be real numbers although that’s not always the case in real life). We represent a continuous distribution with a probability density function (PDF) such that the probability of seeing a value in a certain interval equals the integral of the density function over the interval. The cumulative distribution function (CDF) gives the probability that a random variable is less than or equal to a certain value.\n\nUniform Distribution\nThe uniform distribution puts equal weight on all the numbers between 0 and 1.\n\nProbability density function\n\\[\nf(x) =\n\\begin{cases}\n    \\frac{1}{b-a} & a \\leq x \\leq b \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\n\nplot_lower_bound, plot_upper_bound = -1, 6\nxs = np.arange(plot_lower_bound, plot_upper_bound, 0.1)\n\n\ndef uniform_pdf(x: float, lower: float, upper: float) -&gt; float:\n    if lower &lt;= x &lt;= upper:\n        return 1 / (upper - lower)\n    else:\n        return 0\n\n\nys = [uniform_pdf(x, 0, 5) for x in xs]\n\nfig, ax = plt.subplots(1, 1, tight_layout=True)\nax.plot(xs, ys)\nax.margins(x=0, y=0)\n# ax.set_yticks(np.arange(0, max(ys)*1.1, 0.02))\nax.set_xticks(np.arange(plot_lower_bound, plot_upper_bound + 1))\nax.set_ylim(0, max(ys) * 1.25)\nplt.title(\"Uniform PDF\")\nplt.show()\n\n\n\n\n\n\nCumulative distribution function\n\\[\nF(x) =\n\\begin{cases}\n    0 & x &lt; a \\\\\n    \\frac{x-a}{b-a} & a \\leq x \\leq b \\\\\n    1 & x &gt; b\n\\end{cases}\n\\]\n\ndef uniform_cdf(x: float, lower: float, upper: float) -&gt; float:\n    \"\"\"Returns the probability that a uniform random variable is &lt;= x\"\"\"\n    if x &lt; lower:\n        return 0  # uniform random is never less than 0\n    elif x &lt; upper:\n        return (x - lower) / (upper - lower)  # e.g. P(X &lt;= 0.4) = 0.4\n    else:\n        return 1  # uniform random is always less than 1\n\n\nys = [uniform_cdf(x, 0, 5) for x in xs]\n\nfig, ax = plt.subplots(1, 1, tight_layout=True)\nax.plot(xs, ys)\nax.margins(x=0, y=0)\nax.set_ylim(0, max(ys) * 1.25)\nax.set_xticks(np.arange(plot_lower_bound, plot_upper_bound + 1))\nplt.title(\"Uniform CDF\")\nplt.show()\n\n\n\n\n\n\n\nNormal Distribution\nThe normal distribution is a continuous distribution that is symmetric about its mean. It is defined by two parameters: the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). The normal distribution is the most important continuous distribution in statistics. It is often used to model random variables that are the sum of many independent random variables. It is also the basis for the t-distribution, which is used to model the distribution of sample means.\n\nProbability density function\n\\[\nf(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\]\n\ndef normal_pdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    return math.exp(-((x - mu) ** 2) / 2 / sigma**2) / (math.sqrt(2 * math.pi) * sigma)\n\n\nxs = [x / 10.0 for x in range(-50, 50)]\nfig, ax = plt.subplots(1, 1, tight_layout=True)\nax.plot(xs, [normal_pdf(x, sigma=1) for x in xs], \"-\", label=\"mu=0,sigma=1\")\nax.plot(xs, [normal_pdf(x, sigma=2) for x in xs], \"--\", label=\"mu=0,sigma=2\")\nax.plot(xs, [normal_pdf(x, sigma=0.5) for x in xs], \":\", label=\"mu=0,sigma=0.5\")\nax.plot(xs, [normal_pdf(x, mu=-1) for x in xs], \"-.\", label=\"mu=-1,sigma=1\")\nax.legend()\nplt.title(\"Various Normal pdfs\")\nplt.show()\n\n\n\n\nWhen \\(\\mu\\)=0 and \\(\\sigma\\)=1 it is called the standard normal distribution. If \\(Z\\) is a standard normal random variable, then it turns out that \\(X = \\sigma Z + \\mu\\) is also normal but with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Conversely, if \\(X\\) is a normal random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) then \\(Z = \\frac{X-\\mu}{\\sigma}\\) is a standard normal variable.\n\n\nCumulative distribution function\nThe CDF for the normal distribution cannot be written in an “elementary” manner, but we can write it using Python’s math.erf error function:\n\ndef normal_cdf(x: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n\n\nxs = [x / 10.0 for x in range(-50, 50)]\n\nfig, ax = plt.subplots(1, 1, tight_layout=True)\nax.plot(xs, [normal_cdf(x, sigma=1) for x in xs], \"-\", label=\"mu=0,sigma=1\")\nax.plot(xs, [normal_cdf(x, sigma=2) for x in xs], \"--\", label=\"mu=0,sigma=2\")\nax.plot(xs, [normal_cdf(x, sigma=0.5) for x in xs], \":\", label=\"mu=0,sigma=0.5\")\nax.plot(xs, [normal_cdf(x, mu=-1) for x in xs], \"-.\", label=\"mu=-1,sigma=1\")\nax.legend(loc=4)  # bottom right\nplt.title(\"Various Normal cdfs\")\nplt.show()\n\n\n\n\nSometimes we’ll need to invert the normal CDF to find the value corresponding to a specified probability. There’s no simple way to compute its inverse, but normal_cdf is continuous and strictly increasing, so we can use a binary search. The function repeatedly bisects intervals until it narrows in on a \\(Z\\) that’s close enough to the desired probability.\n\ndef inverse_normal_cdf(p: float, mu: float = 0, sigma: float = 1, tolerance: float = 0.00001) -&gt; float:\n    \"\"\"Find approximate inverse using binary search\"\"\"\n    # if not standard, compute standard and rescale\n    if mu != 0 or sigma != 1:\n        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n    low_z = -10.0  # normal_cdf(-10) is (very close to) 0\n    hi_z = 10.0  # normal_cdf(10)  is (very close to) 1\n    while hi_z - low_z &gt; tolerance:\n        mid_z = (low_z + hi_z) / 2  # Consider the midpoint\n        mid_p = normal_cdf(mid_z)  # and the CDF's value there\n        if mid_p &lt; p:\n            low_z = mid_z  # Midpoint too low, search above it\n        else:\n            hi_z = mid_z  # Midpoint too high, search below it\n    return mid_z\n\n\n\n\nThe Central Limit Theorem\nOne reason the normal distribution is so useful is the central limit theorem, which says (in essence) that a random variable defined as the average of a large number of independent and identically distributed random variables is itself approximately normally distributed.\nIn particular, if \\(x_1, ..., x_n\\) are random variables with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), and if \\(n\\) is large, then \\(\\frac{1}{n} (x_1 + ... + x_n)\\) is approximately normally distributed with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\). Equivalently (but often more usefully), \\(\\frac{(x_1 + ... + x_n) - \\mu n}{\\sigma \\sqrt{n}}\\) is approximately normally distributed with mean 0 and standard deviation 1.\nAn easy way to illustrate this is by looking at binomial random variables, which have two parameters \\(n\\) and \\(p\\). A \\(Binomial(n,p)\\) random variable is simply the sum of \\(n\\) independent \\(Bernoulli(p)\\) random variables, each of which equals 1 with probability \\(p\\) and 0 with probability \\(1 – p\\). The mean of a \\(Bernoulli(p)\\) variable is \\(p\\), and its standard deviation is \\(\\sqrt{p(1-p)}\\). The central limit theorem says that as \\(n\\) gets large, a \\(Binomial(n,p)\\) variable is approximately a normal random variable with mean \\(\\mu = np\\) and standard deviation \\(\\sigma = \\sqrt{np(1-p)}\\). If we plot both, you can easily see the resemblance:\n\ndef bernoulli_trial(p: float) -&gt; int:\n    \"\"\"Returns 1 with probability p and 0 with probability 1-p\"\"\"\n    return 1 if random.random() &lt; p else 0\n\n\ndef binomial(n: int, p: float) -&gt; int:\n    \"\"\"Returns the sum of n bernoulli(p) trials\"\"\"\n    return sum(bernoulli_trial(p) for _ in range(n))\n\n\ndef binomial_histogram(p: float, n: int, num_points: int) -&gt; None:\n    \"\"\"Picks points from a Binomial(n, p) and plots their histogram\"\"\"\n    fig, ax = plt.subplots(1, 1, tight_layout=True)\n\n    data = [binomial(n, p) for _ in range(num_points)]\n\n    # use a bar chart to show the actual binomial samples\n    histogram = Counter(data)\n    ax.bar([x - 0.4 for x in histogram.keys()], [v / num_points for v in histogram.values()], 0.8, color=\"0.75\")\n\n    mu = p * n\n    sigma = math.sqrt(n * p * (1 - p))\n\n    # use a line chart to show the normal approximation\n    xs = range(min(data), max(data) + 1)\n    ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma) for i in xs]\n    ax.plot(xs, ys)\n    plt.title(\"Binomial Distribution vs. Normal Approximation\")\n    plt.show()\n\n\nbinomial_histogram(0.75, 100, 10000)\n\n\n\n\nThe moral of this approximation is that if you want to know the probability that e.g. a fair coin turns up more than 60 heads in 100 flips, you can estimate it as the probability that a \\(Normal(50,5)\\) is greater than 60, which is easier than computing the \\(Binomial(100,0.5)\\) CDF. (Although in most applications you’d probably be using statistical software that would gladly compute whatever probabilities you want.)"
  },
  {
    "objectID": "00_Statistics/statistics.html#inferential-statistics",
    "href": "00_Statistics/statistics.html#inferential-statistics",
    "title": "Statistics",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\n\nHypothesis Testing\nUnder various assumptions, statistics can be thought of as observations of random variables from known distributions, which allows us to make statements about how likely those assumptions are to hold. In the classical setup, we have a null hypothesis, \\(H_0\\), that represents some default position, and some alternative hypothesis, \\(H_1\\), that we’d like to compare it with. We use statistics to decide whether we can reject \\(H_1\\) as false or not. This will probably make more sense with an example.\nImagine we have a coin and we want to test whether it’s fair. We’ll make the assumption that the coin has some probability \\(p\\) of landing heads, and so our null hypothesis is that the coin is fair - that \\(p = 0.5\\). We’ll test this against the alternative hypothesis \\(p \\ne 0.5\\). In particular, our test will involve flipping the coin \\(n\\) times and counting the number of heads \\(X\\). Each coin flip is a Bernoulli trial, which means that \\(X\\) is a \\(Binomial(n,p)\\) random variable, which we can approximate using the normal distribution. Whenever a random variable follows a normal distribution, we can use normal_cdf to figure out the probability that its realized value lies within or outside a particular interval.\n\ndef normal_approximation_to_binomial(n: int, p: float) -&gt; Tuple[float, float]:\n    \"\"\"Returns mu and sigma corresponding to a Binomial(n, p)\"\"\"\n    mu = p * n\n    sigma = math.sqrt(p * (1 - p) * n)\n    return mu, sigma\n\n\n# The normal cdf _is_ the probability the variable is below a threshold\nnormal_probability_below = normal_cdf\n\n\n# It's above the threshold if it's not below the threshold\ndef normal_probability_above(lo: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    \"\"\"The probability that an N(mu, sigma) is greater than lo.\"\"\"\n    return 1 - normal_cdf(lo, mu, sigma)\n\n\n# It's between if it's less than hi, but not less than lo\ndef normal_probability_between(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    \"\"\"The probability that an N(mu, sigma) is between lo and hi.\"\"\"\n    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n\n\n# It's outside if it's not between\ndef normal_probability_outside(lo: float, hi: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    \"\"\"The probability that an N(mu, sigma) is not between lo and hi.\"\"\"\n    return 1 - normal_probability_between(lo, hi, mu, sigma)\n\nWe can also do the reverse — find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood. For example, if we want to find an interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability (leaving 60%):\n\ndef normal_upper_bound(probability: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    \"\"\"Returns the z for which P(Z &lt;= z) = probability\"\"\"\n    return inverse_normal_cdf(probability, mu, sigma)\n\n\ndef normal_lower_bound(probability: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    \"\"\"Returns the z for which P(Z &gt;= z) = probability\"\"\"\n    return inverse_normal_cdf(1 - probability, mu, sigma)\n\n\ndef normal_two_sided_bounds(probability: float, mu: float = 0, sigma: float = 1) -&gt; Tuple[float, float]:\n    \"\"\"Returns the symmetric (about the mean) bounds that contain the specified probability\"\"\"\n    tail_probability = (1 - probability) / 2\n    # upper bound should have tail_probability above it\n    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n    # lower bound should have tail_probability below it\n    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n    return lower_bound, upper_bound\n\nLet’s say that we choose to flip the coin \\(n = 1000\\) times. If our hypothesis of fairness is true, \\(X\\) should be distributed approximately normally with mean 500 and standard deviation 15.8:\n\nmu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)\nmu_0, sigma_0\n\n(500.0, 15.811388300841896)\n\n\nWe need to make a decision about significance — how willing we are to make a type 1 error (“false positive”) in which we reject \\(H_0\\) even though it’s true. For reasons lost to the annals of history, this willingness is often set at 5% or 1%. Let’s choose 5%. Consider the test that rejects \\(H_0\\) if \\(X\\) falls outside the bounds given by:\n\nlower_bound, upper_bound = normal_two_sided_bounds(0.95, mu_0, sigma_0)\nlower_bound, upper_bound\n\n(469.01026640487555, 530.9897335951244)\n\n\nAssuming \\(p\\) really equals 0.5 (i.e. \\(H_0\\) is true) there is just a 5% chance we observe an \\(X\\) that lies outside this interval which is the exact significance we wanted. Said differently, if \\(H_0\\) is true, then, approximately 19 times out of 20, this test will give the correct result. We are also often interested in the power of a test, which is the probability of not making a type 2 error (“false negative”) in which we fail to reject \\(H_0\\) even though it’s false. In order to measure this we have to specify what exactly \\(H_0\\) being false means. (Knowing merely that \\(p \\ne 0.5\\) doesn’t give us a ton of information about the distribution of X). In particular, let’s check what happens if \\(p\\) is really 0.55, so that the coin is slightly biased toward heads. In that case, we can calculate the power of the test with:\n\n# 95% bounds based on assumption p is 0.5\nlo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\nprint(lo, hi)\n# actual mu and sigma based on p = 0.55\nmu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\nprint(mu_1, sigma_1)\n# a type 2 error means we fail to reject the null hypothesis which will happen when X is still in our original interval\ntype_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\npower = 1 - type_2_probability\nprint(power)\n\n469.01026640487555 530.9897335951244\n550.0 15.732132722552274\n0.8865480012953671\n\n\nImagine instead that our null hypothesis was that the coin is not biased toward heads or that \\(P \\le 0.5\\). In that case we want a one-sided test that rejects the null hypothesis when \\(X\\) is much larger than 500 but not when \\(X\\) is smaller than 500. So, a 5% significance test involves using normal_probability_below to find the cutoff below which 95% of the probability lies:\n\nhi = normal_upper_bound(0.95, mu_0, sigma_0)\nprint(hi)  # is 526 (&lt; 531, since we need more probability in the upper tail)\ntype_2_probability = normal_probability_below(hi, mu_1, sigma_1)\npower = 1 - type_2_probability  # 0.936\nprint(power)\n\n526.0073585242053\n0.9363794803307173\n\n\nThis is a more powerful test, since it no longer rejects \\(H_0\\) when \\(X\\) is below 469 (which is very unlikely to happen if \\(H_1\\) is true) and instead rejects \\(H_0\\) when \\(X\\) is between 526 and 531 (which is somewhat likely to happen if \\(H_1\\) is true).\n\nP-Values\nAn alternative way of thinking about the preceding test involves p-values. Instead of choosing bounds based on some probability cutoff, we compute the probability — assuming \\(H_0\\) is true — that we would see a value at least as extreme as the one we actually observed. For our two-sided test of whether the coin is fair, we compute:\n\ndef two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -&gt; float:\n    \"\"\"\n    How likely are we to see a value at least as extreme as x (in either\n    direction) if our values are from an N(mu, sigma)?\n    \"\"\"\n    if x &gt;= mu:\n        # x is greater than the mean, so the tail is everything greater than x\n        return 2 * normal_probability_above(x, mu, sigma)\n    else:\n        # x is less than the mean, so the tail is everything less than x\n        return 2 * normal_probability_below(x, mu, sigma)\n\nIf we were to see 530 heads, we would compute:\n\ntwo_sided_p_value(529.5, mu_0, sigma_0)\n\n0.06207721579598835\n\n\nSince the p-value is greater than our 5% significance, we don’t reject the null. If we instead saw 532 heads, the p-value would be:\n\ntwo_sided_p_value(531.5, mu_0, sigma_0)\n\n0.046345287837786575\n\n\nand we would reject the null.\nMake sure your data is roughly normally distributed before using normal_probability_above to compute p-values. The annals of bad data science are filled with examples of people opining that the chance of some observed event occurring at random is one in a million, when what they really mean is “the chance, assuming the data is distributed normally” which is fairly meaningless if the data isn’t. There are various statistical tests for normality but even plotting the data is a good start.\n\nContinuity Corrections\nWe use a value of 529.5 rather than 530 because normal_probability_between(529.5, 530.5, mu_0, sigma_0) is a better estimate of the probability of seeing 530 heads than normal_probability_between(530, 531, mu_0, sigma_0). Correspondingly, normal_probability_above(529.5, mu_0, sigma_0) is a better estimate of the probability of seeing at least 530 heads.\n\n\n\n\nT Tests/Distribution\n\n\nANOVA\nWhen looking at data, you might want to know if two groups are different. ANOVA (analysis of variance) calculates the between group variance and the within group variance\nThe ratio of the SS (between SS divided by within SS) results is known as the F-statistic\n\n\nVariance ratio (F) test"
  },
  {
    "objectID": "00_Statistics/statistics.html#regression",
    "href": "00_Statistics/statistics.html#regression",
    "title": "Statistics",
    "section": "Regression",
    "text": "Regression"
  },
  {
    "objectID": "00_Statistics/statistics.html#noise-reduction",
    "href": "00_Statistics/statistics.html#noise-reduction",
    "title": "Statistics",
    "section": "Noise reduction",
    "text": "Noise reduction"
  },
  {
    "objectID": "00_Statistics/statistics.html#baysian-data-analysis",
    "href": "00_Statistics/statistics.html#baysian-data-analysis",
    "title": "Statistics",
    "section": "Baysian Data Analysis",
    "text": "Baysian Data Analysis\nBayesian data analysis takes a question in the form of a model and uses logic to produce an answer in the form of probability distributions.\nIn Baysian data analysis:\n\nFor each possible explanation of the data, count all the ways the data can happen\nExplanations with more ways to produce the data are more plausible\n\nImagine a bag that contains 4 marbles. Some are blue and some are white. We don’t know in what proportion. A table of all the possible combinations looks like this. These possible combinations are our conjectures.\n\n\n\nB\nB\nB\nB\n\n\nB\nB\nB\nW\n\n\nB\nB\nW\nW\n\n\nB\nW\nW\nW\n\n\nW\nW\nW\nW\n\n\n\nNow suppose that we draw 3 marbles from the bag with replacement. The results from that will be our data.\n\\[\nBWB\n\\]\nHow could we think about what the 4th marble is likely to be?\nNotice that the number of ways to produce the data, for each conjecture, can be computed by first counting the number of paths in each “ring” of the garden and then by multiplying these counts together.\n\n\n\nConjecture\nNumber of ways to produce the \\[BWB\\]\nPlausibility\n\n\n\n\n\\[WWWW\\]\n\\[0 \\times 4 \\times 0 = 0\\]\n0\n\n\n\\[BWWW\\]\n\\[1 \\times 3 \\times 1 = 3\\]\n0.15\n\n\n\\[BBWW\\]\n\\[2 \\times 2 \\times 2 = 8\\]\n0.4\n\n\n\\[BBBW\\]\n\\[3 \\times 1 \\times 3  = 9\\]\n0.45\n\n\n\\[BBBB\\]\n\\[4 \\times 0 \\times 4  = 0\\]\n0\n\n\n\nThe rules for Baysian updating:\n\nState a causal model for how observations arise, given each possible explanation\nCount ways data could arise for each explanation\nRelative plausibility is relative value from (2)\n\nLet’s suppose we draw another \\[B\\] from the bag. We can update our previous (prior) counts and update them in light of the new observation becuase the new observation is logically independent of the previous observations. First count the numbers of ways each conjecture could produce the new observation. Then multiply each of these new counts by the prior numbers of ways for each conjecture. In table form:\n\n\n\n\n\n\n\n\n\n\nConjecture\nWays to produce \\[B\\]\nPrevious counts\nNew count\nNew plausibility\n\n\n\n\n\\[WWWW\\]\n0\n0\n\\[0 \\times 0 = 0\\]\n0\n\n\n\\[BWWW\\]\n1\n3\n\\[3 \\times 1 = 3\\]\n\\[0.07\\]\n\n\n\\[BBWW\\]\n2\n8\n\\[8 \\times 2 = 16\\]\n\\[0.35\\]\n\n\n\\[BBBW\\]\n3\n9\n\\[9 \\times 3 = 27\\]\n\\[0.57\\]\n\n\n\\[BBBB\\]\n4\n0\n\\[0 \\times 4 = 0\\]\n0\n\n\n\nObviously you want to normalise these values which is why we have a plausibilty colum. You can can converts the counts to probabilities with the following formula:\n\\[\nplausibility\\ of\\ p\\ after\\ D_{new} = \\frac {ways\\ p\\ can\\ produce\\ D_{new} \\times prior\\ plausibility\\ p} {sum\\ of\\ products}\n\\]\nEvery updated set of plausibilities becomes the initial plausibilities for the next observation. Every conclusion is the starting point for future inference. This updating process works backwards as well as forwards. Given a final set of plausibilities and knowing the final observation \\[W\\] it is possible to mathematically divide out the observation to infer the previous plausibility curve. So the data could be presented to your model in any order, or all at once even. In most cases you will present the data all at once for convenience. This represents an abbreviation of an iterated learning process.\nSome points about Baysian Inference:\n\nThere is no such thing as a minimum sample size. The curve is likely to be flat and wide when you have little data but that’s fine. It might even be enough information to be useful in some sense.\nThe shape of the curve embodies the sample size. As you get more data the curve gets narrower and taller.\nPoint estimates don’t have a big role in Baysian data analyis because the the estimate is the curve/distribution. Always use the entire posterior distribution because to do otherwise is to throw away uncertainty and generate overconfident answers. Summary should always be the last step.\nIntervals don’t have a strong role in Baysian inference. How wide to make an interval or even where to centre it is an arbitrary descision.\n\nWhat proportion of the Earth is covered by water? We aren’t going to visit literally every point on Earth. So how can we work this out? We can take a sample. Suppose we visit ten points of earth at random and record whether it is covered by land or water. Suppose these are our results :\n\\[\nW, W, W, L, W, W, W, W, W, L\n\\]\nThe counts of “water” \\[W\\] and “land’ \\[L\\] are distributed binomially, with probability p of “water” on each toss.\n\\[\nPr(W,L|p) = \\frac {(W + L)!} {W!L!} p^W(1 − p)^L\n\\]\nA short way of writing our model\nW ~ Binomial(N, p)\nWhere N = W + L\np ~ Uniform(0,1)"
  }
]