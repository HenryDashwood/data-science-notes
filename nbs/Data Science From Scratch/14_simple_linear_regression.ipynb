{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with a single independent variable\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation tells us the strength of a linear relationship between two variables. But what if we want to predict the value of one variable given the value of another? For example, suppose we want to predict the price of a house given its size. We can do this using linear regression.\n",
    "\n",
    "We begin by hypothesising the existence a linear model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta x_i + \\alpha\n",
    "$$\n",
    "\n",
    "where $y_i$ is the price of the house $i$, $x_i$ is the size of the house $i$, $\\beta$ is the slope of the line, $\\alpha$ is the intercept. Which parameters would result in the best fit line? We can use the least squares method to find the best fit line. The least squares method minimises the **_sum of the squared errors_** (or residuals). This is also known as our cost function, $S$:\n",
    "\n",
    "$$\n",
    "S = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "where $\\hat{y_i}$ is the predicted value of $y_i$.\n",
    "\n",
    "We can subsitute our model for the predicted value of $y_i$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S &= \\sum_{i=1}^n (y_i - (\\beta x_i + \\alpha))^2 \\\\\n",
    "&= \\sum_{i=1}^n (y_i - \\beta x_i - \\alpha)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To minimize our cost function, S, we must find where the first derivative of $S$ is equal to 0 with respect to $\\alpha$ and $\\beta$. The closer $\\alpha$ and $\\beta$ are to 0, the less the total error for each point is. Let’s start with the partial derivative of $\\alpha$ first.\n",
    "\n",
    "### Finding $\\alpha$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial \\alpha}[\\sum_{i=1}^n (y_i - \\beta x_i - \\alpha)^2]\n",
    "$$\n",
    "\n",
    "We can use the chain rule to find the partial derivative with respect to $\\alpha$.\n",
    "\n",
    "The outer term goes from $u^2$ to 2u (where $u =y_i - \\alpha - \\beta x_i$).\n",
    "\n",
    "Within the perenthesese we treat the non-$\\alpha$ terms as constants so we go from $y_i - \\alpha - \\beta x_i$ to $-1$.\n",
    "\n",
    "So in the end we have:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n -2(y_i - \\alpha - \\beta x_i)\n",
    "$$\n",
    "\n",
    "We can divide both sides by $-2$ to get:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)\n",
    "$$\n",
    "\n",
    "We can then break this summation in 3 parts and pull the constant $\\beta$ out:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\alpha - \\beta \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "So the summamation of $\\alpha$ to $n$ is\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\alpha = n\\alpha\n",
    "$$\n",
    "\n",
    "We can subsitute this back in to get\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n y_i - n\\alpha - \\beta \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "We are trying to solve for $\\alpha$ so we add $n\\alpha$ to both sides and divide by $n$.\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{\\sum_{i=1}^n y_i - \\beta \\sum_{i=1}^n x_i}{n}\n",
    "$$\n",
    "\n",
    "In the above equation we are calculating the sum of $y$ and $x& and then dividing by the number of of points. In other words we are using the mean of $y$ and $x$. So we can rewrite the equation as:\n",
    "\n",
    "$$\n",
    "\\alpha = \\bar{y} - \\beta \\bar{x} \\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\bar{y}$ is the mean of $y$ and $\\bar{x}$ is the mean of $x$.\n",
    "\n",
    "### Finding $\\beta$\n",
    "\n",
    "Having minimised the cost function of $S$ with respect to $\\alpha$. Let’s find the last part which is $S$ with respect to $\\beta$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial \\beta}[\\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2]\n",
    "$$\n",
    "\n",
    "We can use the chain rule to find the partial derivative with respect to $\\beta$.\n",
    "\n",
    "The outer term goes from $u^2$ to 2u (where $u =y_i - \\alpha - \\beta x_i$).\n",
    "\n",
    "Within the perenthesese we treat the non-$\\beta$ terms as constants so we go from $y_i - \\alpha - \\beta x_i$ to $-x_i$.\n",
    "\n",
    "So in the end we have:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n -2x_i(y_i - \\alpha - \\beta x_i)\n",
    "$$\n",
    "\n",
    "Again we can divide both sides by $-2$ to get:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n x_i(y_i - \\alpha - \\beta x_i)\n",
    "$$\n",
    "\n",
    "Multiplying each term by $x_i$ we get:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n (y_ix_i - \\alpha x_i - \\beta x_i^2)\n",
    "$$\n",
    "\n",
    "Let’s substitute $a$ (formula $(1)$) into the partial derivative of $S$ with respect to $\\beta$ so we have a function of $\\alpha$ and $\\beta$ in terms of only $x$ and $y$.\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n(x_iy_i - (\\bar{y} - \\beta\\bar{x})x_i - \\beta x_i^2)\n",
    "$$\n",
    "\n",
    "Multiplying out the brackets we get:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n(x_iy_i - \\bar{y}x_i + \\beta\\bar{x}x_i - \\beta x_i^2)\n",
    "$$\n",
    "\n",
    "We can split this into 2 sums\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n(x_iy_i - \\bar{y}x_i) + \\sum_{i=1}^n(\\beta\\bar{x}x_i - \\beta x_i^2))\n",
    "$$\n",
    "\n",
    "and factor out $-\\beta$ (note the minus!)\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^n(x_iy_i - \\bar{y}x_i) - \\beta \\sum_{i=1}^n(x_i^2 - \\bar{x}x_i))\n",
    "$$\n",
    "\n",
    "Adding $\\beta \\sum_{i=1}^n(x_i^2 - \\bar{x}x_i))$ to both sides and then dividing both sides by the same gives us:\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{\\sum_{i=1}^n(x_iy_i - \\bar{y}x_i)}{\\sum_{i=1}^n(x_i^2 - \\bar{x}x_i))} \\tag{2}\n",
    "$$\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "So the **tldr** is that if you have a dataset with 1 independent variable, you find the line of best fit by:\n",
    "\n",
    "1. Calculating $\\beta$ using equation $(2)$\n",
    "2. Substituting $\\beta$ into equation $(1)$ to find $\\alpha$\n",
    "3. Substituting $\\beta$ and $\\alpha$ into the equation for the line of best fit:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta x_i + \\alpha\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have determined $\\alpha$ and $\\beta$, we can make predictions like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha: float, beta: float, x_i: float) -> float:\n",
    "    return beta * x_i + alpha\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving $\\alpha$ and $\\beta$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any choice of $\\alpha$ and $\\beta$ gives us a predicted output for each input $x_i$. Since we know the actual output $y_i$, we can compute the error for each pair:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha: float, beta: float, x_i: float, y_i: float) -> float:\n",
    "    \"\"\"\n",
    "    The error from predicting beta * x_i + alpha when the actual value is y_i\n",
    "    \"\"\"\n",
    "    return predict(alpha, beta, x_i) - y_i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’d like to know the total error over the entire dataset. But we don’t want to just add the errors — if the prediction for $x_1$ is too high and the prediction for $x_2$ is too low, the errors may just cancel out. So instead we add up the squared errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_sqerrors(alpha: float, beta: float, x: List[float], y: List[float]) -> float:\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2 for x_i, y_i in zip(x, y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares solution is to choose the $\\alpha$ and $\\beta$ that make `sum_of_sqerrors` as small as possible. Using calculus (or tedious algebra), the error-minimizing alpha and beta are given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v: List[float], w: List[float]) -> float:\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "\n",
    "def sum_of_squares(v: List[float]) -> float:\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "\n",
    "def mean(xs: List[float]) -> float:\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "\n",
    "def de_mean(xs: List[float]) -> List[float]:\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(xs)\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "\n",
    "def covariance(xs: List[float], ys: List[float]) -> float:\n",
    "    assert len(xs) == len(ys), \"xs and ys must have same number of elements\"\n",
    "\n",
    "    return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1)\n",
    "\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (len(xs) - 1)\n",
    "\n",
    "\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    return math.sqrt(variance(xs))\n",
    "\n",
    "\n",
    "def correlation(xs: List[float], ys: List[float]) -> float:\n",
    "    \"\"\"Measures how much xs and ys vary in tandem about their means\"\"\"\n",
    "    stdev_x = standard_deviation(xs)\n",
    "    stdev_y = standard_deviation(ys)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(xs, ys) / stdev_x / stdev_y\n",
    "    else:\n",
    "        return 0  # if no variation, correlation is zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(x: List[float], y: List[float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Given two vectors x and y, find the least-squares values of alpha and beta\n",
    "    \"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])  # Independent variable (feature)\n",
    "y = np.array([2, 4, 5, 7, 9])  # Dependent variable (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3000000000000007, 1.6999999999999997)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X)\n",
    "y_mean = np.mean(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression the slope $\\beta$ is the correlation between $x$ and $y$ multiplied by the standard deviation of $y$ divided by the standard deviation of $x$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = np.sum((X - X_mean) * (y - y_mean))\n",
    "denominator = np.sum((X - X_mean) ** 2)\n",
    "m = numerator / denominator\n",
    "\n",
    "b = y_mean - m * X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = m * X + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (m): 1.7\n",
      "Y-intercept (b): 0.3000000000000007\n",
      "Predicted values (y_pred): [2.  3.7 5.4 7.1 8.8]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Slope (m): {m}\")\n",
    "print(f\"Y-intercept (b): {b}\")\n",
    "print(f\"Predicted values (y_pred): {y_pred}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple regression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a model with multiple independent variables. e.g. minutes spent on a social media site for data scientists:\n",
    "\n",
    "$$\n",
    "\\text{minutes} = \\beta_1 \\text{friends} + \\beta_2 \\text{work hours} + \\beta_3 \\text{has phd} + \\alpha\n",
    "$$\n",
    "\n",
    "Essentially we are expanding the model with one independent variable:\n",
    "\n",
    "$$\n",
    "y_i = \\beta x_i + \\alpha\n",
    "$$\n",
    "\n",
    "where $x_i$ is a single number to a model of the form:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_1 x_{i1} + ... + \\beta_k x_{ik} + \\alpha\n",
    "$$\n",
    "\n",
    "where $x_i$ is a vector of numbers $x_{i1}, ..., x_{ik}$. In multiple regression the vector of parameters is usually called $\\beta$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making the assumption that the columns of $x$ are _linearly independent_, that there is no way to write one as the weighted sum of the others.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
