{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation tells us the strength of a linear relationship between two variables. But what if we want to predict the value of one variable given the value of another? For example, suppose we want to predict the price of a house given its size. We can do this using linear regression.\n",
    "\n",
    "We begin by hypothesising the existence a linear model:\n",
    "\n",
    "$$\n",
    "y_i = \\beta x_i + \\alpha\n",
    "$$\n",
    "\n",
    "where $y_i$ is the price of the house $i$, $x_i$ is the size of the house $i$, $\\beta$ is the slope of the line, $\\alpha$ is the intercept. Which parameters would result in the best fit line? We can use the least squares method to find the best fit line. The least squares method minimises the **_sum of the squared errors_** (or residuals). This is also known as our cost function, $S$:\n",
    "\n",
    "$$\n",
    "S = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "where $\\hat{y_i}$ is the predicted value of $y_i$.\n",
    "\n",
    "We can subsitute our model for the predicted value of $y_i$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S &= \\sum_{i=1}^n (y_i - (\\beta x_i + \\alpha))^2 \\\\\n",
    "&= \\sum_{i=1}^n (y_i - \\beta x_i - \\alpha)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To minimize our cost function, S, we must find where the first derivative of S is equal to 0 with respect to $\\alpha$ and $\\beta$. The closer $\\alpha$ and $\\beta$ are to 0, the less the total error for each point is. Let’s start with the partial derivative of $\\alpha$ first.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial \\alpha}[\\sum_{i=1}^n (y_i - \\beta x_i - \\alpha)]\n",
    "$$\n",
    "\n",
    "We\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have determined $\\alpha$ and $\\beta$, we can make predictions like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha: float, beta: float, x_i: float) -> float:\n",
    "    return beta * x_i + alpha\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving $\\alpha$ and $\\beta$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any choice of $\\alpha$ and $\\beta$ gives us a predicted output for each input $x_i$. Since we know the actual output $y_i$, we can compute the error for each pair:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha: float, beta: float, x_i: float, y_i: float) -> float:\n",
    "    \"\"\"\n",
    "    The error from predicting beta * x_i + alpha when the actual value is y_i\n",
    "    \"\"\"\n",
    "    return predict(alpha, beta, x_i) - y_i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’d like to know the total error over the entire dataset. But we don’t want to just add the errors — if the prediction for $x_1$ is too high and the prediction for $x_2$ is too low, the errors may just cancel out. So instead we add up the squared errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_sqerrors(alpha: float, beta: float, x: List[float], y: List[float]) -> float:\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2 for x_i, y_i in zip(x, y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares solution is to choose the $\\alpha$ and $\\beta$ that make `sum_of_sqerrors` as small as possible. Using calculus (or tedious algebra), the error-minimizing alpha and beta are given by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v: List[float], w: List[float]) -> float:\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "\n",
    "def sum_of_squares(v: List[float]) -> float:\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "\n",
    "def mean(xs: List[float]) -> float:\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "\n",
    "def de_mean(xs: List[float]) -> List[float]:\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(xs)\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "\n",
    "def covariance(xs: List[float], ys: List[float]) -> float:\n",
    "    assert len(xs) == len(ys), \"xs and ys must have same number of elements\"\n",
    "\n",
    "    return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1)\n",
    "\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (len(xs) - 1)\n",
    "\n",
    "\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    return math.sqrt(variance(xs))\n",
    "\n",
    "\n",
    "def correlation(xs: List[float], ys: List[float]) -> float:\n",
    "    \"\"\"Measures how much xs and ys vary in tandem about their means\"\"\"\n",
    "    stdev_x = standard_deviation(xs)\n",
    "    stdev_y = standard_deviation(ys)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(xs, ys) / stdev_x / stdev_y\n",
    "    else:\n",
    "        return 0  # if no variation, correlation is zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(x: List[float], y: List[float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Given two vectors x and y, find the least-squares values of alpha and beta\n",
    "    \"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3, 4, 5])  # Independent variable (feature)\n",
    "y = np.array([2, 4, 5, 7, 9])  # Dependent variable (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3000000000000007, 1.6999999999999997)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares_fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X)\n",
    "y_mean = np.mean(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression the slope $\\beta$ is the correlation between $x$ and $y$ multiplied by the standard deviation of $y$ divided by the standard deviation of $x$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = np.sum((X - X_mean) * (y - y_mean))\n",
    "denominator = np.sum((X - X_mean) ** 2)\n",
    "m = numerator / denominator\n",
    "\n",
    "b = y_mean - m * X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = m * X + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (m): 1.7\n",
      "Y-intercept (b): 0.3000000000000007\n",
      "Predicted values (y_pred): [2.  3.7 5.4 7.1 8.8]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Slope (m): {m}\")\n",
    "print(f\"Y-intercept (b): {b}\")\n",
    "print(f\"Predicted values (y_pred): {y_pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
