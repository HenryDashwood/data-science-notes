{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "## Descriptive Statistics\n",
    "\n",
    "### Mean\n",
    "\n",
    "$$\n",
    "\\bar x = \\frac 1 n \\sum_ {i=1}^n  x_i\n",
    "$$\n",
    "\n",
    "### Median\n",
    "\n",
    "The median is the middle value in an ordered set of data. If there are an even number of values it is the average of the middle 2 values in the set.\n",
    "\n",
    "### Mode\n",
    "\n",
    "The mode is the most common value (or values) in a set of data.\n",
    "\n",
    "### Dispersion\n",
    "\n",
    "Dispersion refers to measures of how spread out our data is. The simplest measure of dispersion is the range which is the difference between the largest and smallest elements in a set of data.\n",
    "\n",
    "### Variance\n",
    "\n",
    "A more sophisticated measure of dispersion is the variance. You can think of this as the average squared deviation from the mean. However we modify the sum by dividing by $n-1$ because when dealing with a sample from a larger population, $\\bar{x}$ is only an estimate of the actual mean. On average this means $x_i - \\bar{x}$ is an underestimate of the true deviation which is why we divide by a smaller denominator.\n",
    "\n",
    "[QUESTION: Why only subtract 1?]\n",
    "\n",
    "[QUESTION: What if you actually do have the entire population?]\n",
    "\n",
    "$$\n",
    "S^2 = \\frac {\\sum {(x_i - \\bar{x})}^2} {n - 1}\n",
    "$$\n",
    "\n",
    "$S^2$ = sample variance\\\n",
    "$x_i$ = value of the one observation\\\n",
    "$\\bar{x}$ = mean value of all observations\\\n",
    "$n$ = number of observations\n",
    "\n",
    "### Standard Deviation\n",
    "\n",
    "The standard deviation is the square root of the variance which means it is in the same units as the range of the dataset.\n",
    "\n",
    "$$\n",
    "S = \\sqrt {\\frac {\\sum {(x_i - \\bar{x})}^2} {n - 1}}\n",
    "$$\n",
    "\n",
    "$S^2$ = sample variance\\\n",
    "$x_i$ = value of the one observation\\\n",
    "$\\bar{x}$ = mean value of all observations\\\n",
    "$n$ = number of observations\n",
    "\n",
    "### Interquartile Range\n",
    "\n",
    "The interquartile range is the difference between the 75th and 25th percentile value. This is a simple measure but useful as it is not affected by outliers like the range and standard deviation are.\n",
    "\n",
    "### Covariance\n",
    "\n",
    "Whereas variance measures how a single variable deviates from its mean, covariance measures how two variables vary in tandem from their means.\n",
    "\n",
    "$$\n",
    "cov_{x,y} = \\frac {\\sum {(x_i - \\bar{x})(y_i - \\bar{y})}} {N - 1}\n",
    "$$\n",
    "\n",
    "$cov_{x,y}$ = covariance between variable x and y\\\n",
    "$x_i$ = data value of x\\\n",
    "$y_i$ = data value of y\\\n",
    "$\\bar{x}$ = mean of x\\\n",
    "$\\bar{y}$ = mean of y\\\n",
    "$N$ = number of data values\n",
    "\n",
    "A \"large\" positive covariance means that $x$ tends to be large when $y$ is large and small when y is small. A “large” negative covariance means the opposite — that $x$ tends to be small when $y$ is large and vice versa. A covariance close to zero means that no such relationship exists.\n",
    "\n",
    "Covariance can be hard to interpret because:\n",
    "\n",
    "- Its units are the product of the inputs' units which can be hard to make sense of.\n",
    "- If each every value of $x$ was doubled (with $y$ staying the same) the covariance would be twice as large. But the variables would be just as interrelated. So it’s hard to say what counts as a \"large\" covariance.\n",
    "\n",
    "### Correlation\n",
    "\n",
    "Pearson's correlation coefficient is the covariance of the two variables divided by the product of their standard deviations:\n",
    "\n",
    "$$\n",
    "\\rho(X,Y) = \\frac {cov(X,Y)}{\\sigma(X)\\sigma(Y)}\n",
    "$$\n",
    "\n",
    "$cov$ = the covariance\\\n",
    "$\\sigma(X)$ = the standard deviation of X\\\n",
    "$\\sigma(Y)$ = the standard deviation of Y\\\n",
    "\n",
    "Correlation shows us relationships in which knowing how $x_i$ compares to the mean of $x$ gives us information about how $y_i$ compares to the mean of $y$. Other types of relationship may not show up. It also doesn't tell us how large the relationship is. It is also the case that if $E$ and $F$ are independent if the probability of them both happening is equal to the product of the probability of each one happening.\n",
    "\n",
    "#### Simpson's Paradox\n",
    "\n",
    "## Probability\n",
    "\n",
    "Notationally, $P(E)$ means the probability of the event $E$.\n",
    "\n",
    "Events $E$ and $F$ are `dependent` if inforamation about whether $E$ happens gives us information about whether $F$ happens or vice versa. Otherwise the two events are independent of each other.\n",
    "\n",
    "$$ P(E,F) = P(E)P(F) $$\n",
    "\n",
    "If the two events are not necessarily independent (and the probability of $F$ is not 0) we define probability of $E$ given $F$ as:\n",
    "\n",
    "$$ P(E|F) = \\frac {P(E,F)}{P(F)} $$\n",
    "\n",
    "This is often written as:\n",
    "\n",
    "$$ P(E,F) = P(E|F)P(F) $$\n",
    "\n",
    "When $E$ and $F$ are independent this means:\n",
    "\n",
    "$$ P(E|F) = P(E) $$\n",
    "\n",
    "Let's ask _what is the probability \"both children are girls\" conditional on the event \"the older child is a girl\"?_\n",
    "\n",
    "We use the definition of conditional probability and the fact that the event $Both$ and $Girl$ (\"both children are girls and the older child is a girl\") is the same as the event $Both$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(Both|Girl) &= \\frac {P(Both, Girl)}{P(Girl)} \\notag \\\\\n",
    "&= \\frac {P(Both)}{P(Girl)} \\notag \\\\\n",
    "&= \\frac {1}{2} \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can also ask about the probability of the event \"both children are girls\" conditional on the event \"at least one of the children is a girl\" ($At\\ Least\\ One$). Surprisingly, the answer is different from before. If all you know is that at least one of the children is a girl, then it is twice as likely that the family has one boy and one girl than that it has both girls.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(Both | At\\ Least\\ One) &= \\frac {P(Both, At\\ Least\\ One)} {P(At\\ Least\\ One)} \\notag \\\\\n",
    "&= \\frac {P(Both)} {P(At\\ Least\\ One)} \\notag \\\\\n",
    "&= \\frac {1}{3} \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "Bayes’s theorem is a way of “reversing” conditional probabilities. If we need to know the probability of some event $E$ conditional on some other event $F$ occurring but we only have information about the probability of $F$ conditional on $E$ occurring, using the definition of conditional probability twice tells us that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(E | F) &= \\frac {P(E, F)} {P(F)} \\notag \\\\\n",
    "&= \\frac {P(F | E)P(E)} {P(F)} \\notag \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The event $F$ can be split into the two mutually exclusive events \"$F$ and $E$\" and \"$F$ and not $E$\". We write $\\neg$ for \"not\".\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(F) &= P(F, E) + P(F, \\neg E) \\notag \\\\\n",
    "P(E | F) &= \\frac {P(F | E)P(E)} {P(F, E)P(E) + P(F, \\neg E)P(\\neg E)} \\notag \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If a disease has a 1 in 10000 chance of occurring ($P(Disease) = 0.0001$) and a test has a 99% chance of giving a positive result when someone has the disease ($P(Positive|Disease) = 0.99$) we would apply Bayes Theorum like so:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(Disease | Positive) &= \\frac {P(Positive | Disease)P(Disease)} {P(Positive | Disease)P(Disease) + P(Positive | \\neg Disease)P(\\neg Disease)} \\notag \\\\\n",
    "&= \\frac {0.99 \\times 0.0001}{0.99 \\times 0.0001 + 0.01 \\times 0.9999} \\notag \\\\\n",
    "&\\approx 0.0098 \\notag \\\\\n",
    "&= 0.98\\% \\notag\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that this assumes people take the test roughly at random. If only people with certain symptoms take the test, we would have to condition on the event \"positive result and symptoms\" and the number would likely be a lot higher.\n",
    "\n",
    "We can also view this problem using natural frequencies. In a population of 1 million people you’d expect 100 of them to have the disease, and 99 of those 100 to test positive. On the other hand, you’d expect 999,900 of them not to have the disease, and 9,999 of those to test positive. That means you’d expect only 99 out of 10098 (99 + 9999) positive testers to actually have the disease which is roughly 0.98%.\n",
    "\n",
    "### Random Variables\n",
    "\n",
    "A random variable is a variable whose possible values have an associated probability distribution. The expected value of a random variable is the average of its values weighted by their probabilities.\n",
    "\n",
    "A very simple random variable equals 1 if a coin flip turns up heads and 0 if the flip turns up tails. The variable equals 0 with probability 0.5 and 1 with probability 0.5. It has an expected value of $\\frac{1}{2} = (0 * \\frac{1}{2} + 1 * \\frac{1}{2})$\n",
    "\n",
    "A more complicated random variable might measure the number of heads you observe when flipping a coin 10 times or a value picked from `range(10)` where each number is equally likely. The associated distribution gives the probabilities that the variable realizes each of its possible values. In this case the variable has a distribution that assigns probability 0.1 to each of the numbers from 0 to 9. Flipping a coin 10 times has an expected value of 5.5. The range(10) variable has an expected value of 4.5.\n",
    "\n",
    "$$\n",
    "E(X) = \\sum {X_iP(X_i)}\n",
    "$$\n",
    "\n",
    "$X_i$ = The values that X can take\\\n",
    "$P(X_i)$ = The probability that X takes the value $X_i$\n",
    "\n",
    "Random variables can be conditioned on events just as other events can. Going back to the two-child example, if $X$ is the random variable representing the number of girls, $X$ equals 0 with probability $1 \\over 2$ $\\frac{1}{4}$, 1 with probability $\\frac{1}{2}$, and 2 with probability $\\frac{1}{4}$. We can define a new random variable $Y$ that gives the number of girls conditional on at least one of the children being a girl. Then $Y$ equals 1 with probability $\\frac{2}{3}$ and 2 with probability $\\frac{1}{3}$. And a variable $Z$ that's the number of girls conditional on the older child being a girl equals 1 with probability $\\frac{1}{2}$ and 2 with probability $\\frac{1}{2}$.\n",
    "\n",
    "### Continuous Distributions\n",
    "\n",
    "A coin flip corresponds to a discrete distribution — one that associates positive probability with discrete outcomes. Often we’ll want to model distributions across a continuum of outcomes. Because there are infinitely many numbers between 0 and 1, this means that the weight it assigns to individual points is be zero (for our purposes these outcomes will always be real numbers although that’s not always the case in real life). We represent a continuous distribution with a `probability density function` (PDF) such that the probability of seeing a value in a certain interval equals the integral of the density function over the interval. The `cumulative distribution function` (CDF) gives the probability that a random variable is less than or equal to a certain value.\n",
    "\n",
    "#### Uniform Distribution\n",
    "\n",
    "The uniform distribution puts equal weight on all the numbers between 0 and 1.\n",
    "\n",
    "$$\n",
    "\\text {Probability\\ density\\ function} \\\\\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "    \\frac{1}{b-a} & a \\leq x \\leq b \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<div class=\"blog-image\">\n",
    "![](https://henry-dashwood-public-assets.s3.eu-west-2.amazonaws.com/post-imgs/2022-02-05-sr-bayeian-stats/uniform-pdf.svg)\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\text {Cumulative\\ distribution\\ function} \\\\\n",
    "F(x) =\n",
    "\\begin{cases}\n",
    "    0 & x < a \\\\\n",
    "    \\frac{x-a}{b-a} & a \\leq x \\leq b \\\\\n",
    "    1 & x > b\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<div class=\"blog-image\">\n",
    "![](https://henry-dashwood-public-assets.s3.eu-west-2.amazonaws.com/post-imgs/2022-02-05-sr-bayeian-stats/uniform-cdf.svg)\n",
    "</div>\n",
    "\n",
    "#### Normal Distribution\n",
    "\n",
    "The normal distribution is a continuous distribution that is symmetric about its mean. It is defined by two parameters: the mean $\\mu$ and the standard deviation $\\sigma$. The normal distribution is the most important continuous distribution in statistics. It is often used to model random variables that are the sum of many independent random variables. It is also the basis for the t-distribution, which is used to model the distribution of sample means.\n",
    "\n",
    "$$\n",
    "\\text {Probability\\ density\\ function} \\\\\n",
    "f(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n",
    "$$\n",
    "\n",
    "When $\\mu$=0 and $\\sigma$=1 it is called the standard normal distribution. If $Z$ is a standard normal random variable, then it turns out that $X = \\sigma Z + \\mu$ is also normal but with mean $\\mu$ and standard deviation $\\sigma$. Conversely, if $X$ is a normal random variable with mean $\\mu$ and standard deviation $\\sigma$ then $Z = \\frac{X-\\mu}{\\sigma}$ is a standard normal variable.\n",
    "\n",
    "Sometimes we’ll need to invert the normal CDF to find the value corresponding to a specified probability.\n",
    "\n",
    "## Inferential Statistics\n",
    "\n",
    "### Hypothosis Testing\n",
    "\n",
    "### T Tests/Distribution\n",
    "\n",
    "### ANOVA\n",
    "\n",
    "When looking at data, you might want to know if two groups are different. ANOVA (analysis of variance) calculates the **between group variance** and the **within group variance**\n",
    "\n",
    "The ratio of the SS (between SS divided by within SS) results is known as the F-statistic\n",
    "\n",
    "### Variance ratio (F) test\n",
    "\n",
    "## Regression\n",
    "\n",
    "## Noise reduction\n",
    "\n",
    "## Baysian Data Analysis\n",
    "\n",
    "Bayesian data analysis takes a question in the form of a model and uses logic to produce an answer in the form of probability distributions.\n",
    "\n",
    "In Baysian data analysis:\n",
    "\n",
    "- For each possible explanation of the data, count all the ways the data can happen\n",
    "- Explanations with more ways to produce the data are more plausible\n",
    "\n",
    "Imagine a bag that contains 4 marbles. Some are blue and some are white. We don't know in what proportion. A table of all the possible combinations looks like this. These possible combinations are our **conjectures**.\n",
    "\n",
    "|     |     |     |     |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "|  B  |  B  |  B  |  B  |\n",
    "|  B  |  B  |  B  |  W  |\n",
    "|  B  |  B  |  W  |  W  |\n",
    "|  B  |  W  |  W  |  W  |\n",
    "|  W  |  W  |  W  |  W  |\n",
    "\n",
    "Now suppose that we draw 3 marbles from the bag with replacement. The results from that will be our **data**.\n",
    "\n",
    "$$\n",
    "BWB\n",
    "$$\n",
    "\n",
    "How could we think about what the 4th marble is likely to be?\n",
    "\n",
    "Notice that the number of ways to produce the data, for each conjecture, can be computed by first counting the number of paths in each “ring” of the garden and then by multiplying these counts together.\n",
    "\n",
    "| Conjecture | Number of ways to produce the $$BWB$$ | Plausibility |\n",
    "| :--------: | :-----------------------------------: | :----------: |\n",
    "|  $$WWWW$$  |      $$0 \\times 4 \\times 0 = 0$$      |      0       |\n",
    "|  $$BWWW$$  |      $$1 \\times 3 \\times 1 = 3$$      |     0.15     |\n",
    "|  $$BBWW$$  |      $$2 \\times 2 \\times 2 = 8$$      |     0.4      |\n",
    "|  $$BBBW$$  |     $$3 \\times 1 \\times 3  = 9$$      |     0.45     |\n",
    "|  $$BBBB$$  |     $$4 \\times 0 \\times 4  = 0$$      |      0       |\n",
    "\n",
    "The rules for Baysian updating:\n",
    "\n",
    "1. State a causal model for how observations arise, given each possible explanation\n",
    "2. Count ways data could arise for each explanation\n",
    "3. Relative plausibility is relative value from (2)\n",
    "\n",
    "Let's suppose we draw another $$B$$ from the bag. We can update our previous (**prior**) counts and update them in light of the new observation becuase the new observation is logically independent of the previous observations. First count the numbers of ways each conjecture could produce the new observation. Then multiply each of these new counts by the prior numbers of ways for each conjecture. In table form:\n",
    "\n",
    "| Conjecture | Ways to produce $$B$$ | Previous counts |      New count      | New plausibility |\n",
    "| :--------: | :-------------------: | :-------------: | :-----------------: | :--------------: |\n",
    "|  $$WWWW$$  |           0           |        0        | $$0 \\times 0 = 0$$  |        0         |\n",
    "|  $$BWWW$$  |           1           |        3        | $$3 \\times 1 = 3$$  |     $$0.07$$     |\n",
    "|  $$BBWW$$  |           2           |        8        | $$8 \\times 2 = 16$$ |     $$0.35$$     |\n",
    "|  $$BBBW$$  |           3           |        9        | $$9 \\times 3 = 27$$ |     $$0.57$$     |\n",
    "|  $$BBBB$$  |           4           |        0        | $$0 \\times 4 = 0$$  |        0         |\n",
    "\n",
    "Obviously you want to normalise these values which is why we have a plausibilty colum. You can can converts the counts to probabilities with the following formula:\n",
    "\n",
    "$$\n",
    "plausibility\\ of\\ p\\ after\\ D_{new} = \\frac {ways\\ p\\ can\\ produce\\ D_{new} \\times prior\\ plausibility\\ p} {sum\\ of\\ products}\n",
    "$$\n",
    "\n",
    "<div class=\"blog-image\">\n",
    "![](https://henry-dashwood-public-assets.s3.eu-west-2.amazonaws.com/post-imgs/2022-02-05-sr-bayeian-stats/prob_dense_plot.gif)\n",
    "</div>\n",
    "Every updated set of plausibilities becomes the initial plausibilities for the  next observation. Every conclusion is the starting point for future inference. This updating process works backwards as well as forwards. Given a final set of plausibilities and knowing the final observation $$W$$ it is possible to mathematically divide out the observation to infer the previous plausibility curve. So the data could be presented to your model in any order, or all at once even. In most cases you will present the data all at once for convenience. This represents an abbreviation of an iterated learning process.\n",
    "\n",
    "Some points about Baysian Inference:\n",
    "\n",
    "1. There is no such thing as a minimum sample size. The curve is likely to be flat and wide when you have little data but that's fine. It might even be enough information to be useful in some sense.\n",
    "2. The shape of the curve embodies the sample size. As you get more data the curve gets narrower and taller.\n",
    "3. Point estimates don't have a big role in Baysian data analyis because the the estimate is the curve/distribution. Always use the entire posterior distribution because to do otherwise is to throw away uncertainty and generate overconfident answers. Summary should always be the last step.\n",
    "4. Intervals don't have a strong role in Baysian inference. How wide to make an interval or even where to centre it is an arbitrary descision.\n",
    "\n",
    "What proportion of the Earth is covered by water? We aren't going to visit literally every point on Earth. So how can we work this out? We can take a sample. Suppose we visit ten points of earth at random and record whether it is covered by land or water. Suppose these are our results :\n",
    "\n",
    "$$\n",
    "W, W, W, L, W, W, W, W, W, L\n",
    "$$\n",
    "\n",
    "The counts of “water” $$W$$ and “land’ $$L$$ are distributed binomially, with probability p of “water” on each toss.\n",
    "\n",
    "$$\n",
    "Pr(W,L|p) = \\frac {(W + L)!} {W!L!} p^W(1 − p)^L\n",
    "$$\n",
    "\n",
    "A short way of writing our model\n",
    "\n",
    "W ~ Binomial(N, p)\n",
    "\n",
    "Where N = W + L\n",
    "\n",
    "p ~ Uniform(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3111",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
